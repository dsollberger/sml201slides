[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SML 201 (Spring 2026)",
    "section": "",
    "text": "10: Logistic Regression\n\n\n\n\n\n\n\n\nFeb 26, 2026\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n9: Modeling Categorical Variables\n\n\n\n\n\n\n\n\nFeb 24, 2026\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n8: Multiple Linear Regression\n\n\n\n\n\n\n\n\nFeb 19, 2026\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n7: Linear Regression\n\n\n\n\n\n\n\n\nFeb 17, 2026\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n6: Correlation and Paradoxes\n\n\n\n\n\n\n\n\nFeb 12, 2026\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n5: Geospatial\n\n\n\n\n\n\n\n\nFeb 10, 2026\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n4: Categories\n\n\n\n\n\n\n\n\nFeb 5, 2026\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n3: Variance\n\n\n\n\n\n\n\n\nFeb 3, 2026\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n2: Centrality\n\n\n\n\n\n\n\n\nJan 29, 2026\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n1: Introduction\n\n\n\n\n\n\n\n\nJan 27, 2026\n\n\nDerek Sollberger\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html",
    "href": "posts/01_introduction/01_introduction.html",
    "title": "1: Introductions",
    "section": "",
    "text": "Goal: Introduce data science\nObjective: Explore a data set with bar graphs and facets\n\n\n\n\n\nImage credit: Steven Geringer"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#start",
    "href": "posts/01_introduction/01_introduction.html#start",
    "title": "1: Introductions",
    "section": "",
    "text": "Goal: Introduce data science\nObjective: Explore a data set with bar graphs and facets\n\n\n\n\n\nImage credit: Steven Geringer"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#sml-201-introduction-to-data-science",
    "href": "posts/01_introduction/01_introduction.html#sml-201-introduction-to-data-science",
    "title": "1: Introductions",
    "section": "",
    "text": "Fall 2024\nTuesdays and Thursdays\n\n11 AM to 1220 PM: Lewis Library 120\n130 PM to 250 PM: Lewis Library 120\n\nLecturer: Derek\n\nI go by “Derek” or “teacher”\n\n\n\n\n\n\n\n\nCourse Description\n\n\n\n\n\nIntroduction to Data Science provides a practical introduction to the burgeoning field of data science. The course introduces students to the essential tools for conducting data-driven research, including the fundamentals of programming techniques and the essentials of statistics. Students will work with real-world datasets from various domains; write computer code to manipulate, explore, and analyze data; use basic techniques from statistics and machine learning to analyze data; learn to draw conclusions using sound statistical reasoning; and produce scientific reports. No prior knowledge of programming or statistics is required."
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#lecturer",
    "href": "posts/01_introduction/01_introduction.html#lecturer",
    "title": "1: Introductions",
    "section": "Lecturer",
    "text": "Lecturer"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#current-research-in-pedagogy",
    "href": "posts/01_introduction/01_introduction.html#current-research-in-pedagogy",
    "title": "1: Introductions",
    "section": "Current Research in Pedagogy",
    "text": "Current Research in Pedagogy\n\n\n\n\n\nactive learning\ncomputer programming\nflipped classrooms"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#identity-statement",
    "href": "posts/01_introduction/01_introduction.html#identity-statement",
    "title": "1: Introductions",
    "section": "Identity Statement",
    "text": "Identity Statement\n\n\n\nOriginally from Los Angeles\nMath: easier to understand through graphs\nComputer Programming: years of experience with R, Python, MATLAB, PHP, HTML, etc.\nLearning: drawn to puzzles and manageable tasks\nPersonality: shy, introvert"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#textbooks",
    "href": "posts/01_introduction/01_introduction.html#textbooks",
    "title": "1: Introductions",
    "section": "Textbooks",
    "text": "Textbooks\n\n\n\n\n\nR for Data Science\n\n\n\nThis course will loosely follow\n\nR for Data Science by Hadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund (online textbook)\nStatistical Inference via Data Science by Chester Ismay and Albert Y Kim (online textbook)\n\n\n\n\n\nModern Dive"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#additional-reading",
    "href": "posts/01_introduction/01_introduction.html#additional-reading",
    "title": "1: Introductions",
    "section": "Additional Reading",
    "text": "Additional Reading\nThe following list of books is optional for student studies, but the instructor may use some materials to add depth and interest to the course.\n\n\n\n\n\n\nAdditional Reading\n\n\n\n\n\n\nThe Seven Pillars of Statistical Wisdom by Stephen M Stigler provides a wonderful overview of the history of statistics and the field’s major developments.\nStatistical Rethinking by Richard McElreath is the premier body of work in the field of Bayesian analysis. This resource is great for people who want to build a strong foundation in philosophy and theory in this branch of mathematics.\nTeaching Statistics by Andrew Gelman and Deborah Nolan features a variety of classroom activities that engage audiences at prestigious universities into learning statistical concepts.\nBernoulli’s Fallacy by Aubrey Clayton is a scathing review of the history of statistics and posits that the foundations of the field are flawed."
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#cooperative-classroom",
    "href": "posts/01_introduction/01_introduction.html#cooperative-classroom",
    "title": "1: Introductions",
    "section": "Cooperative Classroom",
    "text": "Cooperative Classroom\nLearning in a cooperative environment should be stimulating, demanding, and fair. Because this approach to learning is different from the competitive classroom structure that many other courses used to be based on, it is important for us to be clear about mutual expectations. Below are my expectations for students in this class. This set of expectations is intended to maximize debate and exchange of ideas in an atmosphere of mutual respect while preserving individual ownership of ideas and written words. If you feel you do not understand or cannot agree to these expectations, you should discuss this with your instructor and classmates.\n\nStudents are expected to work cooperatively with other members of the class and show respect for the ideas and contributions of other people.\nWhen working as part of a group, students should strive to be good contributors to the group, listen to others, not dominate, and recognize the contributions of others. Students should try to ensure that everyone in the group is welcome to contribute and recognize that everyone contributes in different ways to a group process.\nStudents should explore data, make observations, and develop inferences as part of a group. If you use material from published sources, you must provide appropriate attribution.\n\n\n\n(Students will be asked to acknowledge this document in an online form.)\nThis document has been adapted from Scientific Teaching by Jo Handelsman, Sarah Miller, and Christine Pfund\n\n\n\n\nScientific Teaching"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#pep-talk",
    "href": "posts/01_introduction/01_introduction.html#pep-talk",
    "title": "1: Introductions",
    "section": "Pep Talk",
    "text": "Pep Talk\nLearning R can be difficult at first—it is like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you will be using like ggplot2—made this wise observation:\n\n\n\n\n\n\nWisdom from Hadley Wickham\n\n\n\n\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\n\n\nIf you are finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, ask questions … e-mail [Derek], etc. I promise you can do this.\n—Andrew Heiss, Georgia State University"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#inclusion-statement",
    "href": "posts/01_introduction/01_introduction.html#inclusion-statement",
    "title": "1: Introductions",
    "section": "Inclusion Statement",
    "text": "Inclusion Statement\nI value all students regardless of their background, country of origin, race, religion, ethnicity, gender, sexual orientation, disability status, etc. and am committed to providing a climate of excellence and inclusiveness within all aspects of the course. If there are aspects of your culture or identity that you would like to share with me as they relate to your success in this class, I am happy to meet to discuss. Likewise, if you have any concerns in this area or facing any special issues or challenges, you are encouraged to discuss the matter with me (set up a meeting by e-mail) with an assurance of full confidentiality (only exception being mandatory reporting of academic integrity code violations or sexual harassment)."
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#concern-for-bee-populations",
    "href": "posts/01_introduction/01_introduction.html#concern-for-bee-populations",
    "title": "1: Introductions",
    "section": "Concern for Bee Populations",
    "text": "Concern for Bee Populations\n\n\n\n\n\nTime\n\n\n\n\n\n\n\n\nABC"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#finding-data",
    "href": "posts/01_introduction/01_introduction.html#finding-data",
    "title": "1: Introductions",
    "section": "Finding Data",
    "text": "Finding Data\n\nUS Dept of Agriculture\nNational Agricultural Statistics Service\n\n\nLoading DataR Code\n\n\n\n\n# A tibble: 4 × 7\n  Program  Year   Value `Geo Level` State    Commodity `Data Item`              \n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;                    \n1 SURVEY   2015 2849500 NATIONAL    US TOTAL HONEY     HONEY, BEE COLONIES - IN…\n2 SURVEY   2016 2801470 NATIONAL    US TOTAL HONEY     HONEY, BEE COLONIES - IN…\n3 SURVEY   2017 2694150 NATIONAL    US TOTAL HONEY     HONEY, BEE COLONIES - IN…\n4 SURVEY   2018 2665880 NATIONAL    US TOTAL HONEY     HONEY, BEE COLONIES - IN…\n\n\n\n\n\nbee_df &lt;- readr::read_csv(\"bee_population.csv\")\nhead(bee_df)"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#presenting-narrative",
    "href": "posts/01_introduction/01_introduction.html#presenting-narrative",
    "title": "1: Introductions",
    "section": "Presenting Narrative",
    "text": "Presenting Narrative"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#why-r",
    "href": "posts/01_introduction/01_introduction.html#why-r",
    "title": "1: Introductions",
    "section": "Why R?",
    "text": "Why R?\n\n\n\n\n\nR\n\n\n\n\n\n\nmatches data science concepts well\nlanguage made for statistics and probability calculations\nsoftware compatibility\neasier to learn\neasier to teach\ngaining popularity in areas such as consulting, finance, epidemiology, genomics, pharmaceuticals, etc."
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#r-vs-python",
    "href": "posts/01_introduction/01_introduction.html#r-vs-python",
    "title": "1: Introductions",
    "section": "R vs Python",
    "text": "R vs Python\n\ntablegt code\n\n\n\n\n\n\n\n\n\n\nData Science Programming Languages\n\n\nWhich one is better?\n\n\nR\nPython\n\n\n\n\nData Science\nMachine Learning\n\n\nDashboards\nSoftware Development\n\n\nInteractvity\nObject-Oriented Programming\n\n\nVisualization\nBig Data\n\n\nDebugging\nFaster\n\n\n\nsource: Derek's opinion\n\n\n\n\n\n\n\n\n\n\n\nlanguages_df &lt;- data.frame(\n  R = c(\"Data Science\", \"Dashboards\", \"Interactvity\", \"Visualization\", \"Debugging\"),\n  Python = c(\"Machine Learning\", \"Software Development\", \"Object-Oriented Programming\", \"Big Data\", \"Faster\")\n)\n\nlanguages_df |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"source: Derek's opinion\") |&gt;\n  tab_header(\n    title = \"Data Science Programming Languages\",\n    subtitle = \"Which one is better?\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#ffc100\")\n    ),\n    locations = cells_body(columns = R)\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#d0cef3\")\n    ),\n    locations = cells_body(columns = Python)\n  )"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#critique",
    "href": "posts/01_introduction/01_introduction.html#critique",
    "title": "1: Introductions",
    "section": "Critique",
    "text": "Critique\n\n\nCritique this graph. What comments do you have about it?"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#continuing-the-narrative",
    "href": "posts/01_introduction/01_introduction.html#continuing-the-narrative",
    "title": "1: Introductions",
    "section": "Continuing the Narrative",
    "text": "Continuing the Narrative\n\nMore DataCodeFacetsCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbee_totals_df |&gt;\n  mutate(num_colonies = Value / 1e6) |&gt;\n  ggplot(aes(x = factor(Year), y = num_colonies)) +\n  geom_bar(color = \"#121212\", fill = \"#E77500\",\n           stat = \"identity\") +\n  labs(title = \"US Bee Population\",\n       subtitle = \"Survey of bee colonies\",\n       caption = \"source: USDA NASS\",\n       x = \"year\",\n       y = \"bee colonies (in millions)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbee_states_df |&gt;\n  # filter(Year &gt;= 2015 & Year &lt;= 2018) |&gt;\n  filter(State %in% c(\"NEW JERSEY\", \"CALIFORNIA\")) |&gt;\n  ggplot(aes(x = factor(Year), y = Value)) +\n  facet_wrap(vars(State), ncol = 1, scales = \"free_y\") +\n  geom_bar(color = \"#121212\", fill = \"#E77500\",\n           stat = \"identity\") +\n  labs(title = \"US Bee Population\",\n       subtitle = \"Selection of States\",\n       caption = \"source: USDA NASS\",\n       x = \"year\",\n       y = \"bee colonies\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#epilogue",
    "href": "posts/01_introduction/01_introduction.html#epilogue",
    "title": "1: Introductions",
    "section": "Epilogue",
    "text": "Epilogue\n * inspiration: Wait, does America suddenly have a record number of bees? by Andrew Van Dam"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html",
    "href": "posts/02_centrality/02_Centrality.html",
    "title": "2: Centrality",
    "section": "",
    "text": "Goal: Summarize data by centrality\nObjective: Compute means and medians\n\n\n\n\n\n\n\nThe limit does not exist!\n\n\n\n\n\n\n\n\n\n\nTipAdvice\n\n\n\n\ncreate a folder on your computer desktop called “SML 201”\n\nlater: place all code scripts and data sets in this folder\n\nTo create a new Quarto document, open RStudio and then\n\nFile –&gt; New File –&gt; Quarto Document ...\nsave the file into your SML 201 folder\n\nTo run a line of code, the keyboard short cut is\n\nWindows: CTRL + ENTER\nMac: CMD + ENTER"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#start",
    "href": "posts/02_centrality/02_Centrality.html#start",
    "title": "2: Centrality",
    "section": "",
    "text": "Goal: Summarize data by centrality\nObjective: Compute means and medians\n\n\n\n\n\n\n\nThe limit does not exist!\n\n\n\n\n\n\n\n\n\n\nTipAdvice\n\n\n\n\ncreate a folder on your computer desktop called “SML 201”\n\nlater: place all code scripts and data sets in this folder\n\nTo create a new Quarto document, open RStudio and then\n\nFile –&gt; New File –&gt; Quarto Document ...\nsave the file into your SML 201 folder\n\nTo run a line of code, the keyboard short cut is\n\nWindows: CTRL + ENTER\nMac: CMD + ENTER"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#advice",
    "href": "posts/02_centrality/02_Centrality.html#advice",
    "title": "2: Centrality",
    "section": "",
    "text": "create a folder on your computer desktop called “SML 201”\n\nlater: place all code scripts and data sets in this folder\n\nopen RStudio and create a new Quarto document\n\nFile –&gt; New File –&gt; Quarto Document ...\nsave the file into your SML 201 folder\n\nTo run a line of code, the keyboard short cut is\n\nWindows: CTRL + ENTER\nMac: CMD + ENTER"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#definition",
    "href": "posts/02_centrality/02_Centrality.html#definition",
    "title": "2: Centrality",
    "section": "Definition",
    "text": "Definition\nFor a list of data\n\\[\\{a_{1}, a_{2}, ..., a_{n}\\}\\]\nthe mean or average of the data is defined as\n\\[\\bar{x} = \\displaystyle\\frac{1}{n}\\sum_{i = 1}^{n} a_{i}\\] where “x bar” denotes a sample mean"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#in-r",
    "href": "posts/02_centrality/02_Centrality.html#in-r",
    "title": "2: Centrality",
    "section": "In R",
    "text": "In R\nRun each of these lines of code, and describe the code\n\nsome_data &lt;- c(32, 45, 16, 78, 39)\nsum(some_data)\nlength(some_data)\nsum(some_data) / length(some_data)\nmean(some_data)"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#missing-data",
    "href": "posts/02_centrality/02_Centrality.html#missing-data",
    "title": "2: Centrality",
    "section": "Missing Data",
    "text": "Missing Data\nRun each of these lines of code, and describe the code\n\nsome_data &lt;- c(32, 45, 16, 78, NA, 39)\nsum(some_data)\nlength(some_data)\nsum(some_data) / length(some_data)\nmean(some_data)\nmean(some_data, na.rm = TRUE)\n\n\n\n\n\n\n\nWarningDCP1"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#loading-the-data",
    "href": "posts/02_centrality/02_Centrality.html#loading-the-data",
    "title": "2: Centrality",
    "section": "Loading the Data",
    "text": "Loading the Data\nI have supplied a couple of data sets to a GitHub repository to ease the loading of data for classroom work.\n\nolympic_df1 &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/02_centrality/olympic_data.csv\")\nolympic_df2 &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/02_centrality/olympic_data2.csv\")"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#summary-statistics",
    "href": "posts/02_centrality/02_Centrality.html#summary-statistics",
    "title": "2: Centrality",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nRun each of these lines of code, and describe the code\n\nmean(olympic_df1$weight)\nmean(olympic_df1$weight, na.rm = TRUE)\n\n\n\n\n\n\n\nTipThe fix\n\n\n\n\n\n\nolympic_df1$weight[olympic_df1$weight &lt;= 0] &lt;- NA\nolympic_df2$weight[olympic_df2$weight &lt;= 0] &lt;- NA"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#filter",
    "href": "posts/02_centrality/02_Centrality.html#filter",
    "title": "2: Centrality",
    "section": "Filter",
    "text": "Filter\nFor this demonstration, let us focus on the athletes from Turkey.\n\nTurkey_df1 &lt;- olympic_df1 |&gt;\n  filter(country_code == \"TUR\")"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#dotplot",
    "href": "posts/02_centrality/02_Centrality.html#dotplot",
    "title": "2: Centrality",
    "section": "Dotplot",
    "text": "Dotplot\nEarly in an introductory statistics course, a dotplot is useful for visualizing integer data.\n\nmean_1 &lt;- mean(Turkey_df1$age, na.rm = TRUE)\n\nTurkey_df1 |&gt;\n  ggplot(aes(x = age)) +\n  geom_dotplot() +\n  geom_vline(xintercept = mean_1, color = \"blue\", linewidth = 3) +\n  labs(title = \"Ages of Turkish Athletics\",\n       subtitle = \"mean in blue\",\n       caption = \"SML 201\")"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#the-outlier",
    "href": "posts/02_centrality/02_Centrality.html#the-outlier",
    "title": "2: Centrality",
    "section": "The Outlier",
    "text": "The Outlier\n\n\n\n\n\nYusuf Dikec\n\n\n\nimage source: News 18\n\n\n\n\n\nYusuf Dikec\nTurkish sharpshooter\n\nsilver medalist (2024 Olympics)\n10m air pistol mixed team\n\nAge: 51"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#filtered-again",
    "href": "posts/02_centrality/02_Centrality.html#filtered-again",
    "title": "2: Centrality",
    "section": "Filtered Again",
    "text": "Filtered Again\n\nTurkey_df2 &lt;- olympic_df2 |&gt;\n  filter(country_code == \"TUR\")"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#dotplot-revisited",
    "href": "posts/02_centrality/02_Centrality.html#dotplot-revisited",
    "title": "2: Centrality",
    "section": "Dotplot Revisited",
    "text": "Dotplot Revisited\nEarly in an introductory statistics course, a dotplot is useful for visualizing integer data.\n\nmean_1 &lt;- mean(Turkey_df1$age, na.rm = TRUE)\nmean_2 &lt;- mean(Turkey_df2$age, na.rm = TRUE)\n\nTurkey_df2 |&gt;\n  ggplot(aes(x = age)) +\n  geom_dotplot() +\n  geom_vline(xintercept = mean_1, color = \"blue\", linewidth = 3) +\n  geom_vline(xintercept = mean_2, color = \"blue\", linewidth = 3) +\n  labs(title = \"Ages of Turkish Athletics\",\n       subtitle = \"mean in blue\",\n       caption = \"SML 201\")"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#medians",
    "href": "posts/02_centrality/02_Centrality.html#medians",
    "title": "2: Centrality",
    "section": "Medians",
    "text": "Medians\n\nmedian_1 &lt;- median(Turkey_df1$age, na.rm = TRUE)\nmedian_2 &lt;- median(Turkey_df2$age, na.rm = TRUE)\n\nTurkey_df2 |&gt;\n  ggplot(aes(x = age)) +\n  geom_dotplot() +\n  geom_vline(xintercept = median_1, color = \"red\", linewidth = 3) +\n  geom_vline(xintercept = median_2, color = \"red\", linewidth = 3) +\n  labs(title = \"Ages of Turkish Athletics\",\n       subtitle = \"median in red\",\n       caption = \"SML 201\")"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#difference-in-means",
    "href": "posts/02_centrality/02_Centrality.html#difference-in-means",
    "title": "2: Centrality",
    "section": "Difference in Means",
    "text": "Difference in Means\n\nmean_1 - mean_2\n\n[1] -1.92449\n\nabs(mean_1 - mean_2)\n\n[1] 1.92449"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#difference-in-medians",
    "href": "posts/02_centrality/02_Centrality.html#difference-in-medians",
    "title": "2: Centrality",
    "section": "Difference in Medians",
    "text": "Difference in Medians\n\nmedian_1 - median_2\n\n[1] 0\n\nabs(median_1 - median_2)\n\n[1] 0"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#punchline",
    "href": "posts/02_centrality/02_Centrality.html#punchline",
    "title": "2: Centrality",
    "section": "Punchline",
    "text": "Punchline\n\n\n\nInvincible\n\n\n\nThe median is robust against outliers!\nimage source: Know Your Meme"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#why-the-mean",
    "href": "posts/02_centrality/02_Centrality.html#why-the-mean",
    "title": "2: Centrality",
    "section": "Why the mean?",
    "text": "Why the mean?\nLater, we use the mean for:\n\nnormal distributions (“bell curves”)\nlinear regression goes through center of mass\nestimators and other statistical theory"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#which-do-we-use",
    "href": "posts/02_centrality/02_Centrality.html#which-do-we-use",
    "title": "2: Centrality",
    "section": "Which do we use?",
    "text": "Which do we use?\n\n\n\nWhen feasible, compute and report both the mean and median.\n\n\n\n\n\n\n\nWhy not both?\n\n\n\nimage source: Know Your Meme"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#time-series",
    "href": "posts/02_centrality/02_Centrality.html#time-series",
    "title": "2: Centrality",
    "section": "Time Series",
    "text": "Time Series\n\nVizCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntickets_df |&gt;\n  ggplot(aes(x = date, y = n)) +\n  geom_line() +\n  labs(title = \"Parking Tickets in Philadelphia\",\n       subtitle = \"Street Sweeping Violations (2017)\",\n       caption = \"Source: Open Data Philly\",\n       y = \"number of tickets\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#moving-average",
    "href": "posts/02_centrality/02_Centrality.html#moving-average",
    "title": "2: Centrality",
    "section": "Moving Average",
    "text": "Moving Average\n\nConcept357911Code\n\n\nA rolling mean or moving average compues the mean across a group of \\(L\\) (lag) consecutive data points in a time series and slides the “window”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntickets_df |&gt;\n  mutate(roll_mean = zoo::rollapply(\n    n, 3, mean, align = 'left', fill = NA\n  )) |&gt;\n    ggplot() +\n    geom_point(aes(x = date, y = n),\n               color = \"black\") +\n  geom_line(aes(x = date, y = roll_mean),\n            color = \"blue\") +\n    labs(title = \"Parking Tickets in Philadelphia\",\n         subtitle = \"Rolling mean in blue (n = 3 day window)\",\n         caption = \"Source: Open Data Philly\",\n         y = \"number of tickets\") +\n    theme_minimal()"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#rolling-median",
    "href": "posts/02_centrality/02_Centrality.html#rolling-median",
    "title": "2: Centrality",
    "section": "Rolling Median",
    "text": "Rolling Median\n\n357911Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntickets_df |&gt;\n  mutate(roll_median = zoo::rollapply(\n    n, 3, median, align = 'left', fill = NA\n  )) |&gt;\n    ggplot() +\n    geom_point(aes(x = date, y = n),\n               color = \"black\") +\n  geom_line(aes(x = date, y = roll_median),\n            color = \"red\") +\n    labs(title = \"Parking Tickets in Philadelphia\",\n         subtitle = \"Rolling median in red (n = 3 day window)\",\n         caption = \"Source: Open Data Philly\",\n         y = \"number of tickets\") +\n    theme_minimal()"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html",
    "href": "posts/03_variance/03_Variance.html",
    "title": "3: Variance",
    "section": "",
    "text": "Goal: Introduce the concept of variance\nObjective: Compute range, variance, and standard deviation\n\n\n\n\n\n\n\nSpread!"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#start",
    "href": "posts/03_variance/03_Variance.html#start",
    "title": "3: Variance",
    "section": "",
    "text": "Goal: Introduce the concept of variance\nObjective: Compute range, variance, and standard deviation\n\n\n\n\n\n\n\nSpread!"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#centrality",
    "href": "posts/03_variance/03_Variance.html#centrality",
    "title": "3: Variance",
    "section": "Centrality",
    "text": "Centrality\nRecall that we can compute means and medians.\n\nmean(A)\n\n[1] 0\n\nmean(B)\n\n[1] 0\n\nmedian(A)\n\n[1] 0\n\nmedian(B)\n\n[1] 0"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#visualization",
    "href": "posts/03_variance/03_Variance.html#visualization",
    "title": "3: Variance",
    "section": "Visualization",
    "text": "Visualization\n\nVizCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsimple_df &lt;- data.frame(A,B)\ntitle_string &lt;- \"Compare and Contrast: &lt;span style='color:blue'&gt;Set A&lt;/span&gt; versus &lt;span style='color:red'&gt;Set B&lt;/span&gt;\"\n\nsimple_df |&gt;\n  ggplot() +\n  geom_point(aes(x = A, y = 1), color = \"blue\", size = 10) +\n  geom_point(aes(x = B, y = 2), color = \"red\", size = 10) +\n  labs(title = title_string,\n       subtitle = \"What is alike?  What is different?\",\n       caption = \"SML 201\",\n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        plot.title = element_markdown(face = \"bold\", hjust = 0.5,size = 20),\n        plot.subtitle = element_markdown(hjust = 0.5,size = 15)) +\n  ylim(0,3)"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#aside-range",
    "href": "posts/03_variance/03_Variance.html#aside-range",
    "title": "3: Variance",
    "section": "Aside: Range",
    "text": "Aside: Range\nTo describe variance, an early draft was the range\n\\[\\text{range}(x) = \\text{max}(x) - \\text{min}(x)\\]\n\nhighly affected by outliers\nuses only two data points\n\n\n# range in R computes min and max values\nrange(A)\n\n[1] -3  3\n\nrange(B)\n\n[1] -9  9\n\n# range in statistics\ndiff(range(A))\n\n[1] 6\n\ndiff(range(B))\n\n[1] 18"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#sample-mean",
    "href": "posts/03_variance/03_Variance.html#sample-mean",
    "title": "3: Variance",
    "section": "Sample Mean",
    "text": "Sample Mean\nRecall that we compute the sample mean of the data as\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}\\]\n\nH &lt;- c(75,76,63,62,58)\nxbar &lt;- mean(H)"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#deviations",
    "href": "posts/03_variance/03_Variance.html#deviations",
    "title": "3: Variance",
    "section": "Deviations",
    "text": "Deviations\nNext, we can compute deviations from the mean\n\ndeviations &lt;- H - xbar\ndeviations\n\n[1]  8.2  9.2 -3.8 -4.8 -8.8"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#squared-deviations",
    "href": "posts/03_variance/03_Variance.html#squared-deviations",
    "title": "3: Variance",
    "section": "Squared Deviations",
    "text": "Squared Deviations\nWe don’t need negative signs in this calculations. One way around this is to square the deviations.\n\nsq_deviations &lt;- deviations^2\nsq_deviations\n\n[1] 67.24 84.64 14.44 23.04 77.44"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#tabulation",
    "href": "posts/03_variance/03_Variance.html#tabulation",
    "title": "3: Variance",
    "section": "Tabulation",
    "text": "Tabulation\nSo far we have\n\nTableCode\n\n\n\n\n\n\n\n\n\n\nNathan's Hot Dog Eating Contest\n\n\nRecent winning amounts\n\n\nhot_dogs\nxbar\ndeviations\nsq_deviations\n\n\n\n\n75\n66.8\n8.2\n67.24\n\n\n76\n66.8\n9.2\n84.64\n\n\n63\n66.8\n-3.8\n14.44\n\n\n62\n66.8\n-4.8\n23.04\n\n\n58\n66.8\n-8.8\n77.44\n\n\n\nMen's competition\n\n\n\n\n\n\n\n\n\n\n\nhot_dog_data &lt;- data.frame(hot_dogs = c(75,76,63,62,58))\nhot_dog_data |&gt;\n  mutate(xbar = mean(hot_dogs, na.rm = TRUE),\n         deviations = hot_dogs - xbar,\n         sq_deviations = deviations^2) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Men's competition\") |&gt;\n  tab_header(\n    title = \"Nathan's Hot Dog Eating Contest\",\n    subtitle = \"Recent winning amounts\") |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  )"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#summarizing",
    "href": "posts/03_variance/03_Variance.html#summarizing",
    "title": "3: Variance",
    "section": "Summarizing",
    "text": "Summarizing\nLike before, we want to summarize a list of numbers.\n\\[s^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2} = \\frac{266.8}{4} = 66.7\\]\nAt this point, the calculation has produced a sample variance\n\nWhy “n-1”? See later session about “Estimators”\nBut what are the units?"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#dimensional-analysis",
    "href": "posts/03_variance/03_Variance.html#dimensional-analysis",
    "title": "3: Variance",
    "section": "Dimensional Analysis",
    "text": "Dimensional Analysis\n\nTableCodeWhat?\n\n\n\n\n\n\n\n\n\n\nNathan's Hot Dog Eating Contest\n\n\nRecent winning amounts\n\n\nhot_dogs\nxbar\ndeviations\nsq_deviations\n\n\n\n\n75\n66.8\n8.2\n67.24 (hot dogs)^2\n\n\n76\n66.8\n9.2\n84.64 (hot dogs)^2\n\n\n63\n66.8\n-3.8\n14.44 (hot dogs)^2\n\n\n62\n66.8\n-4.8\n23.04 (hot dogs)^2\n\n\n58\n66.8\n-8.8\n77.44 (hot dogs)^2\n\n\n\nMen's competition\n\n\n\n\n\n\n\n\n\n\n\nhot_dog_data &lt;- data.frame(hot_dogs = c(75,76,63,62,58))\nhot_dog_data |&gt;\n  mutate(xbar = mean(hot_dogs, na.rm = TRUE),\n         deviations = hot_dogs - xbar,\n         sq_deviations = paste(round(deviations^2,2), \n                               \"(hot dogs)^2\")) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Men's competition\") |&gt;\n  tab_header(\n    title = \"Nathan's Hot Dog Eating Contest\",\n    subtitle = \"Recent winning amounts\") |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = cell_text(color = \"red\"),\n    locations = cells_body(columns = sq_deviations)\n  )\n\n\n\nThe sample variance is\n\\[s^{2} = 66.7 \\,(\\text{hot dogs})^{2}\\]\n\n\n\nsquare hot dogs?\n\n\n\nimage created with Canva AI"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#rectify",
    "href": "posts/03_variance/03_Variance.html#rectify",
    "title": "3: Variance",
    "section": "Rectify",
    "text": "Rectify\nIf we need to use these results in subsequent calculations, we can fix the units by taking the square root of the sample variance. This yields the sample standard deviation\n\\[s = \\sqrt{66.7} \\approx 8.1670 \\text{ hot dogs}\\]"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#sse",
    "href": "posts/03_variance/03_Variance.html#sse",
    "title": "3: Variance",
    "section": "SSE",
    "text": "SSE\nFor the sum of squared errors, what value of \\(c\\) will minimize the error?\n\\[\\text{SSE} = \\sum_{i=1}^{n} (x_{i} - c)^{2}, \\quad H = \\{75,76,63,62,58\\}\\]\n\ncvals &lt;- seq(58, 76)\nSSE &lt;- rep(NA, length(cvals))\nfor(i in 1:length(cvals)){\n  SSE[i] &lt;- sum((H - cvals[i])^{2})\n}\nmin(SSE)\n\n[1] 267\n\ncvals[which.min(SSE)]\n\n[1] 67\n\n\n\ncvals &lt;- seq(58, 76, by = 0.1)\nSSE &lt;- rep(NA, length(cvals))\nfor(i in 1:length(cvals)){\n  SSE[i] &lt;- sum((H - cvals[i])^{2})\n}\nmin(SSE)\n\n[1] 266.8\n\ncvals[which.min(SSE)]\n\n[1] 66.8\n\nmean(H)\n\n[1] 66.8\n\n\nClaim: The sample mean minimizes the sum of squared errors.\n\n\n\n\n\n\n(optional) Calculus proof\n\n\n\n\n\nFor a non-constant data set \\(\\{x_{i}\\}_{i=1}^{n}\\), and for the sum of squared errors\n\\[S(c) = \\sum_{i=1}^{n} (x_{i} - c)^{2}\\]\nwe can set the derivative equal to zero\n\\[\\begin{array}{rcl}\n  0 & = & \\frac{dS}{dc} \\\\\n  0 & = & \\frac{d}{dc} \\sum_{i=1}^{n} (x_{i} - c)^{2} \\\\\n  0 & = & \\sum_{i=1}^{n} \\frac{d}{dc} (x_{i} - c)^{2} \\\\\n  0 & = & \\sum_{i=1}^{n} 2(x_{i} - c) \\\\\n  0 & = &  2\\sum_{i=1}^{n} x_{i} - 2nc \\\\\n  0 & = &  \\sum_{i=1}^{n} x_{i} - nc \\\\\n  nc & = &  \\sum_{i=1}^{n} x_{i} \\\\\n  c & = & \\frac{1}{n}\\sum_{i=1}^{n} x_{i} \\\\\n\\end{array}\\]\nWe recognize that the right-hand side is the sample mean. Since the function was a concave up parabola, we know that this critical point is a global minimum."
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#absolute-value",
    "href": "posts/03_variance/03_Variance.html#absolute-value",
    "title": "3: Variance",
    "section": "Absolute Value",
    "text": "Absolute Value\nWhat if we had used the absolute value instead? We can use a similar argument on the sum of absolute errors.\n\ncvals &lt;- seq(58, 76, by = 0.1)\nSE &lt;- rep(NA, length(cvals))\nfor(i in 1:length(cvals)){\n  SE[i] &lt;- sum(abs(H - cvals[i]))\n}\nmin(SE)\n\n[1] 31\n\ncvals[which.min(SE)]\n\n[1] 63\n\nmedian(H)\n\n[1] 63\n\n\nClaim: The sample median minimizes the sum of absolute errors.\n\\[SE(c) = \\sum_{i=1}^{n} |x_{i} - c|\\]\n\n\n\n\n\n\n(optional) Outline of proof\n\n\n\n\n\n\nargue that the summation is smallest when one of terms is zero\ninterpolate for the case when the number of observations is even"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#data-dow-jones-industrial-average",
    "href": "posts/03_variance/03_Variance.html#data-dow-jones-industrial-average",
    "title": "3: Variance",
    "section": "Data: Dow Jones Industrial Average",
    "text": "Data: Dow Jones Industrial Average\n\n\n\n30 popular stocks\nYear to date\n\nJan 2, 2024\nSept 6, 2024\n\nsource: Yahoo Finance\n\n\n\n\n\ndow_df &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/03_variance/DOW30.csv\")"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#exploration",
    "href": "posts/03_variance/03_Variance.html#exploration",
    "title": "3: Variance",
    "section": "Exploration",
    "text": "Exploration\n\nhead(dow_df)\n\n# A tibble: 6 × 11\n  ticker ref_date   price_open price_high price_low price_close   volume\n  &lt;chr&gt;  &lt;date&gt;          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 AAPL   2024-01-02       187.       188.      184.        186. 82488700\n2 AAPL   2024-01-03       184.       186.      183.        184. 58414500\n3 AAPL   2024-01-04       182.       183.      181.        182. 71983600\n4 AAPL   2024-01-05       182.       183.      180.        181. 62303300\n5 AAPL   2024-01-08       182.       186.      182.        186. 59144500\n6 AAPL   2024-01-09       184.       185.      183.        185. 42841800\n# ℹ 4 more variables: price_adjusted &lt;dbl&gt;, ret_adjusted_prices &lt;dbl&gt;,\n#   ret_closing_prices &lt;dbl&gt;, cumret_adjusted_prices &lt;dbl&gt;\n\n\n\nstr(dow_df, give.attr = FALSE)\n\nspc_tbl_ [5,160 × 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ ticker                : chr [1:5160] \"AAPL\" \"AAPL\" \"AAPL\" \"AAPL\" ...\n $ ref_date              : Date[1:5160], format: \"2024-01-02\" \"2024-01-03\" ...\n $ price_open            : num [1:5160] 187 184 182 182 182 ...\n $ price_high            : num [1:5160] 188 186 183 183 186 ...\n $ price_low             : num [1:5160] 184 183 181 180 182 ...\n $ price_close           : num [1:5160] 186 184 182 181 186 ...\n $ volume                : num [1:5160] 82488700 58414500 71983600 62303300 59144500 ...\n $ price_adjusted        : num [1:5160] 185 184 181 180 185 ...\n $ ret_adjusted_prices   : num [1:5160] NA -0.00749 -0.0127 -0.00401 0.02417 ...\n $ ret_closing_prices    : num [1:5160] NA -0.00749 -0.0127 -0.00401 0.02417 ...\n $ cumret_adjusted_prices: num [1:5160] 1 0.993 0.98 0.976 1 ...\n\n\n\ncolnames(dow_df)\n\n [1] \"ticker\"                 \"ref_date\"               \"price_open\"            \n [4] \"price_high\"             \"price_low\"              \"price_close\"           \n [7] \"volume\"                 \"price_adjusted\"         \"ret_adjusted_prices\"   \n[10] \"ret_closing_prices\"     \"cumret_adjusted_prices\""
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#which-stocks",
    "href": "posts/03_variance/03_Variance.html#which-stocks",
    "title": "3: Variance",
    "section": "Which Stocks?",
    "text": "Which Stocks?\nThe table command tallys the observations in a categorical variable.\n\ntable(dow_df$ticker)\n\n\nAAPL AMGN AMZN  AXP   BA  CAT  CRM CSCO  CVX  DIS  DOW   GS   HD  HON  IBM INTC \n 172  172  172  172  172  172  172  172  172  172  172  172  172  172  172  172 \n JNJ  JPM   KO  MCD  MMM  MRK MSFT  NKE   PG  TRV  UNH    V   VZ  WMT \n 172  172  172  172  172  172  172  172  172  172  172  172  172  172"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#histograms",
    "href": "posts/03_variance/03_Variance.html#histograms",
    "title": "3: Variance",
    "section": "Histograms",
    "text": "Histograms\n\ndow_df |&gt;\n  filter(ticker == \"VZ\") |&gt;\n  ggplot(aes(x = price_close)) +\n  geom_histogram() +\n  labs(title = \"Verizon stock\",\n       subtitle = \"2024 YTD\",\n       caption = \"SML 201\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\ndow_df |&gt;\n  filter(ticker == \"GS\") |&gt;\n  ggplot(aes(x = price_close)) +\n  geom_histogram() +\n  labs(title = \"Goldman Sachs stock\",\n       subtitle = \"2024 YTD\",\n       caption = \"SML 201\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#application-1",
    "href": "posts/03_variance/03_Variance.html#application-1",
    "title": "3: Variance",
    "section": "Application 1",
    "text": "Application 1\nWhich stocks have had the highest average price_close this year?\n\ndow_df |&gt;\n  group_by(ticker) |&gt;\n  mutate(avg_price = mean(price_close, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(ticker, avg_price) |&gt;\n  distinct() |&gt;\n  arrange(desc(avg_price))\n\n# A tibble: 30 × 2\n   ticker avg_price\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 UNH         516.\n 2 GS          435.\n 3 MSFT        417.\n 4 HD          354.\n 5 CAT         335.\n 6 AMGN        302.\n 7 MCD         275.\n 8 V           273.\n 9 CRM         271.\n10 AXP         227.\n# ℹ 20 more rows\n\n\n\nUnited Health\nGoldman Sachs\nMicrosoft\nHome Depot\nCaterpillar"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#volatility",
    "href": "posts/03_variance/03_Variance.html#volatility",
    "title": "3: Variance",
    "section": "Volatility",
    "text": "Volatility\nWhich stocks have been the most volatile this year?\n\ndow_df |&gt;\n  group_by(ticker) |&gt;\n  mutate(volatility = sd(price_close, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(ticker, volatility) |&gt;\n  distinct() |&gt;\n  arrange(desc(volatility))\n\n# A tibble: 30 × 2\n   ticker volatility\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 GS           43.5\n 2 UNH          37.5\n 3 CRM          22.2\n 4 CAT          21.9\n 5 AMGN         21.3\n 6 AAPL         20.8\n 7 MSFT         19.5\n 8 AXP          18.9\n 9 BA           17.5\n10 MMM          16.2\n# ℹ 20 more rows\n\n\n\nGoldman Sachs\nUnited Health\nSalesforce\nCaterpillar\nAmgen\n\nWhich stocks have been the most volatile this year?\n\ndow_df |&gt;\n  group_by(ticker) |&gt;\n  mutate(volatility = sd(price_close, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(ticker, volatility) |&gt;\n  distinct() |&gt;\n  arrange(volatility)\n\n# A tibble: 30 × 2\n   ticker volatility\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 VZ          0.973\n 2 CSCO        1.73 \n 3 DOW         2.29 \n 4 KO          3.58 \n 5 MRK         5.56 \n 6 PG          6.23 \n 7 JNJ         6.23 \n 8 CVX         6.24 \n 9 WMT         6.47 \n10 HON         6.56 \n# ℹ 20 more rows\n\n\n\nVerizon\nCisco\nDow Inc\nCoca-Cola\nMerck"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#coefficient-of-variation",
    "href": "posts/03_variance/03_Variance.html#coefficient-of-variation",
    "title": "3: Variance",
    "section": "Coefficient of Variation",
    "text": "Coefficient of Variation\nWouldn’t the most expensive stocks naturally vary more?\n\\[\\text{CoV} = \\frac{s}{\\bar{x}}\\]\n\n\\(\\bar{x}\\): sample mean\n\\(s\\): sample standard deviation\n\nWhich stocks have the highest coefficients of variation this year?\n\ndow_df |&gt;\n  group_by(ticker) |&gt;\n  mutate(avg_price = mean(price_close, na.rm = TRUE)) |&gt;\n  mutate(volatility = sd(price_close, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(ticker, avg_price, volatility) |&gt;\n  distinct() |&gt;\n  mutate(coef_var = volatility / avg_price) |&gt;\n  arrange(desc(coef_var))\n\n# A tibble: 30 × 4\n   ticker avg_price volatility coef_var\n   &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 INTC        35.1       8.58   0.244 \n 2 MMM         98.2      16.2    0.165 \n 3 NKE         91.6      10.6    0.115 \n 4 AAPL       195.       20.8    0.106 \n 5 WMT         63.3       6.47   0.102 \n 6 GS         435.       43.5    0.100 \n 7 DIS        102.        9.92   0.0971\n 8 BA         187.       17.5    0.0934\n 9 AXP        227.       18.9    0.0833\n10 CRM        271.       22.2    0.0819\n# ℹ 20 more rows\n\n\n\nIntel\n3M\nNike\nApple\nWalmart"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#z-scores-1",
    "href": "posts/03_variance/03_Variance.html#z-scores-1",
    "title": "3: Variance",
    "section": "Z-scores",
    "text": "Z-scores\nSometimes, we want to rescale numerical columns to be able to compare them together.\n\nsummary(dow_df$price_close)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.89   96.86  181.35  198.37  271.92  604.18 \n\n\n\ndow_df &lt;- dow_df |&gt;\n  mutate(price_scaled = scale(price_close)) #z-scores\n\n\nsummary(dow_df$price_scaled)\n\n       V1         \n Min.   :-1.4525  \n 1st Qu.:-0.8215  \n Median :-0.1377  \n Mean   : 0.0000  \n 3rd Qu.: 0.5953  \n Max.   : 3.2842"
  },
  {
    "objectID": "posts/03_variance/03_Variance.html#line-plots",
    "href": "posts/03_variance/03_Variance.html#line-plots",
    "title": "3: Variance",
    "section": "Line Plots",
    "text": "Line Plots\n\nVizCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitle_string &lt;- \"&lt;span style='color:blue'&gt;Verizon&lt;/span&gt; and &lt;span style='color:red'&gt;Goldman Sachs&lt;/span&gt;\"\nsubtitle_string &lt;- \"2024 stock prices\"\n\ndow_df |&gt;\n  ggplot() +\n  geom_line(aes(x = ref_date, y = price_close),\n            color = \"blue\", linewidth = 2,\n            data = dow_df |&gt; filter(ticker == \"VZ\")) +\n  geom_line(aes(x = ref_date, y = price_close),\n            color = \"red\", linewidth = 3,\n            data = dow_df |&gt; filter(ticker == \"GS\")) +\n  labs(title = title_string,\n       subtitle = subtitle_string,\n       caption = \"SML 201\",\n       x = \"date\", y = \"closing price\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", hjust = 0.5,size = 25),\n        plot.subtitle = element_markdown(hjust = 0.5,size = 20))"
  },
  {
    "objectID": "posts/04_categories/04_categories.html",
    "href": "posts/04_categories/04_categories.html",
    "title": "4: Categories",
    "section": "",
    "text": "Goal: Explore data wrangling with categorical variables\nObjective: Compute counts, make bar graphs, and discuss data\n\n\n\n\n\n\n\ndemographics survey"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#start",
    "href": "posts/04_categories/04_categories.html#start",
    "title": "4: Categories",
    "section": "",
    "text": "Goal: Explore data wrangling with categorical variables\nObjective: Compute counts, make bar graphs, and discuss data\n\n\n\n\n\n\n\ndemographics survey"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#data",
    "href": "posts/04_categories/04_categories.html#data",
    "title": "4: Categories",
    "section": "Data",
    "text": "Data\n\ndemo_df &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/04_categories/sml201survey.csv\")"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#queries",
    "href": "posts/04_categories/04_categories.html#queries",
    "title": "4: Categories",
    "section": "Queries",
    "text": "Queries\n\n\nFor the Demographics Survey, here are the variable names that represent the responses to the survey questions.\n\n\n\n\n\n\ndemographics survey"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#example-ocean-or-snow",
    "href": "posts/04_categories/04_categories.html#example-ocean-or-snow",
    "title": "4: Categories",
    "section": "Example: Ocean or Snow?",
    "text": "Example: Ocean or Snow?\n\ndemo_df |&gt;\n  filter(!is.na(oceanSnow)) |&gt;\n  ggplot(aes(x = oceanSnow)) +\n  geom_bar(aes(fill = oceanSnow),\n           color = \"black\",\n           stat = \"count\")"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#example-campus-safety",
    "href": "posts/04_categories/04_categories.html#example-campus-safety",
    "title": "4: Categories",
    "section": "Example: Campus Safety",
    "text": "Example: Campus Safety\nOn a scale from 0 to 100—with 0 = very anxious and 100 = comfortable—how safe do you feel on campus?\n\nsummary(demo_df$safety)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   2.00   80.00   90.00   87.16   97.25  100.00       5 \n\n\n\ndemo_df |&gt;\n  ggplot(aes(x = safety)) +\n  geom_histogram(binwidth = 5)\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\ndemo_df |&gt;\n  ggplot(aes(x = safety, group = gender)) +\n  geom_density(aes(fill = gender),\n               alpha = 0.5)\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\ndemo_df |&gt;\n  ggplot(aes(x = gender, y = safety)) +\n  geom_boxplot()\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_boxplot()`)."
  },
  {
    "objectID": "posts/04_categories/04_categories.html#example-flossing",
    "href": "posts/04_categories/04_categories.html#example-flossing",
    "title": "4: Categories",
    "section": "Example: Flossing",
    "text": "Example: Flossing\n\nsummary(demo_df$flossing)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  0.000   2.000   5.000   5.224   7.000  14.000      10"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#example-football",
    "href": "posts/04_categories/04_categories.html#example-football",
    "title": "4: Categories",
    "section": "Example: Football",
    "text": "Example: Football\n\ndemo_df |&gt;\n  select(football) |&gt;\n  separate_longer_delim(football, delim = \",\") |&gt;\n  count(football) |&gt;\n  arrange(desc(n))\n\n# A tibble: 29 × 2\n   football                 n\n   &lt;chr&gt;                &lt;int&gt;\n 1 &lt;NA&gt;                    40\n 2 New York Giants         14\n 3 Philadelphia Eagles     13\n 4 San Francisco 49ers     11\n 5 Dallas Cowboys           7\n 6 Buffalo Bills            6\n 7 Houston Texas            6\n 8 New England Patriots     6\n 9 Arizona Cardinals        5\n10 New York Jets            5\n# ℹ 19 more rows\n\n\n\ndemo_df |&gt;\n  select(baseball) |&gt;\n  separate_longer_delim(baseball, delim = \",\") |&gt;\n  count(baseball) |&gt;\n  arrange(desc(n))\n\n# A tibble: 28 × 2\n   baseball                  n\n   &lt;chr&gt;                 &lt;int&gt;\n 1 &lt;NA&gt;                     49\n 2 New York Yankees         16\n 3 Boston Red Sox           11\n 4 New York Mets            11\n 5 Philadelphia Phillies    11\n 6 Los Angeles Dodgers       8\n 7 Arizona Diamondbacks      4\n 8 Houston Astros            4\n 9 Miami Marlins             4\n10 San Franciso Giants       4\n# ℹ 18 more rows"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#example-sleep",
    "href": "posts/04_categories/04_categories.html#example-sleep",
    "title": "4: Categories",
    "section": "Example: Sleep",
    "text": "Example: Sleep"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#example-residential-colleges",
    "href": "posts/04_categories/04_categories.html#example-residential-colleges",
    "title": "4: Categories",
    "section": "Example: Residential Colleges",
    "text": "Example: Residential Colleges"
  },
  {
    "objectID": "posts/05_networks/05_networks.html",
    "href": "posts/05_networks/05_networks.html",
    "title": "5: Networks",
    "section": "",
    "text": "Goal: Visualize network data\nObjective: Arrange paired data into nodes and edges\n\n\n\n\n\n\n\nnetwork terminology\n\n\n\nimage source: MRI Questions\n\n\n\n\n\n\nMoving forward, those who want to type along with the lecture sessio will probably want to use template files\n\nGo to our Canvas Page –&gt; Files –&gt; lecture_notes\nDownload today’s template file\nMove that .qmd file into your SML 201 folder\nOpen that template file"
  },
  {
    "objectID": "posts/05_networks/05_networks.html#start",
    "href": "posts/05_networks/05_networks.html#start",
    "title": "5: Networks",
    "section": "",
    "text": "Goal: Visualize network data\nObjective: Arrange paired data into nodes and edges\n\n\n\n\n\n\n\nnetwork terminology\n\n\n\nimage source: MRI Questions"
  },
  {
    "objectID": "posts/05_networks/05_networks.html#rbind",
    "href": "posts/05_networks/05_networks.html#rbind",
    "title": "5: Networks",
    "section": "rbind",
    "text": "rbind\n\ndf1 &lt;- music_df |&gt;\n  select(artist1, song1) |&gt;\n  set_names(c(\"artist\", \"song\"))\ndf2 &lt;- music_df |&gt;\n  select(artist2, song2) |&gt;\n  set_names(c(\"artist\", \"song\"))\n\nmusic_long &lt;- df1 |&gt;\n  rbind(df2) |&gt;\n  separate_longer_delim(artist, delim = \", \") |&gt;\n  filter(!is.na(artist)) |&gt;\n  filter(!is.na(song))"
  },
  {
    "objectID": "posts/05_networks/05_networks.html#top_n",
    "href": "posts/05_networks/05_networks.html#top_n",
    "title": "5: Networks",
    "section": "top_n",
    "text": "top_n\nWho are the top 5 most frequent artists in the data set?\n\nmusic_long |&gt;\n  count(artist) |&gt;\n  arrange(desc(n)) |&gt;\n  top_n(5, wt = n)\n\n# A tibble: 6 × 2\n  artist                n\n  &lt;chr&gt;             &lt;int&gt;\n1 Taylor Swift         10\n2 Sabrina Carpenter     9\n3 SZA                   7\n4 Daya                  6\n5 Bruno Mars            4\n6 Lauv                  4\n\n\n\ntop_artists &lt;- music_long |&gt;\n  count(artist) |&gt;\n  arrange(desc(n)) |&gt;\n  top_n(5, wt = n) |&gt;\n  pull(artist)"
  },
  {
    "objectID": "posts/05_networks/05_networks.html#node-tibble",
    "href": "posts/05_networks/05_networks.html#node-tibble",
    "title": "5: Networks",
    "section": "Node Tibble",
    "text": "Node Tibble\n\nnode_tbl &lt;- tibble(id = music_long$artist,\n                  label = music_long$artist,\n                  color = \"green\",\n                  shape = \"circle\")\nnode_tbl &lt;- node_tbl |&gt;\n  bind_rows(\n    tibble(id = music_long$song,\n           label = music_long$song,\n           color = \"yellow\",\n           shape = \"circle\")\n  ) |&gt;\n  distinct()"
  },
  {
    "objectID": "posts/05_networks/05_networks.html#edge-tibble",
    "href": "posts/05_networks/05_networks.html#edge-tibble",
    "title": "5: Networks",
    "section": "Edge Tibble",
    "text": "Edge Tibble\n\nedge_tbl &lt;- tibble(\n  from = music_long$artist,\n  to = music_long$song,\n  arrow = \"to\") #directed network"
  },
  {
    "objectID": "posts/05_networks/05_networks.html#viznetwork-plot",
    "href": "posts/05_networks/05_networks.html#viznetwork-plot",
    "title": "5: Networks",
    "section": "vizNetwork Plot",
    "text": "vizNetwork Plot\n\nvisNetwork(nodes = node_tbl, \n           edges = edge_tbl,\n           main = \"Song Playlist Network\")"
  },
  {
    "objectID": "posts/05_networks/05_networks.html#filtered-plot",
    "href": "posts/05_networks/05_networks.html#filtered-plot",
    "title": "5: Networks",
    "section": "Filtered Plot",
    "text": "Filtered Plot\n\nmusic_top &lt;- music_long |&gt;\n  filter(artist %in% top_artists)\n\nnode_tbl &lt;- tibble(id = music_top$artist,\n                  label = music_top$artist,\n                  color = \"green\",\n                  shape = \"circle\")\nnode_tbl &lt;- node_tbl |&gt;\n  bind_rows(\n    tibble(id = music_top$song,\n           label = music_top$song,\n           color = \"yellow\",\n           shape = \"circle\")\n  ) |&gt;\n  distinct()\n\nedge_tbl &lt;- tibble(\n  from = music_top$artist,\n  to = music_top$song)\n\nvisNetwork(nodes = node_tbl, \n           edges = edge_tbl,\n           main = \"Song Playlist Network\")"
  },
  {
    "objectID": "posts/05_networks/05_networks.html#other-music",
    "href": "posts/05_networks/05_networks.html#other-music",
    "title": "5: Networks",
    "section": "Other Music",
    "text": "Other Music\nWe can negate the %in% to create a complement set of lessor known songs. We can also use the sample function to display a small number of those songs.\n\nmusic_long |&gt;\n  filter(!(artist %in% top_artists)) |&gt;\n  sample_n(10)\n\n# A tibble: 10 × 2\n   artist            song                         \n   &lt;chr&gt;             &lt;chr&gt;                        \n 1 Drake             Stars Align                  \n 2 Miley Cyrus       The Climb                    \n 3 Cage the Elephant Social Cues                  \n 4 UMI               Remember Me                  \n 5 Harry Styles      Sign of the Times            \n 6 Barry Can't Swim  El Layali                    \n 7 The Cranberries   Linger                       \n 8 Lady Gaga         Die With A Smile             \n 9 Tom Lehrer        Poisoning Pigeons in the Park\n10 Dom Dolla         Saving Up"
  },
  {
    "objectID": "posts/05_networks/05_networks.html#node-tibble-1",
    "href": "posts/05_networks/05_networks.html#node-tibble-1",
    "title": "5: Networks",
    "section": "Node Tibble",
    "text": "Node Tibble\n\nnode_tbl &lt;- tibble(id = majors_df$major,\n                  label = majors_df$major,\n                  color = \"#E77500\",\n                  shape = \"circle\") |&gt;\n  bind_rows(\n    tibble(id = majors_df$minor1,\n           label = majors_df$minor1,\n           color = \"#121212\",\n           opacity = 0.5,\n           shape = \"circle\")\n  ) |&gt; \n  bind_rows(\n    tibble(id = majors_df$minor2,\n           label = majors_df$minor2,\n           color = \"#121212\",\n           opacity = 0.5,\n           shape = \"circle\")\n  ) |&gt; \n  distinct(id, .keep_all = TRUE)"
  },
  {
    "objectID": "posts/05_networks/05_networks.html#edge-tibble-1",
    "href": "posts/05_networks/05_networks.html#edge-tibble-1",
    "title": "5: Networks",
    "section": "Edge Tibble",
    "text": "Edge Tibble\n\nedge_tbl &lt;- tibble(\n  from = majors_df$major,\n  to = majors_df$minor1,\n  color = \"#333333\") |&gt;\n  bind_rows(\n    tibble(from = majors_df$major,\n           to = majors_df$minor2,\n           color = \"#333333\")\n  ) |&gt;\n  filter(!is.na(to))"
  },
  {
    "objectID": "posts/05_networks/05_networks.html#viznetwork-plot-1",
    "href": "posts/05_networks/05_networks.html#viznetwork-plot-1",
    "title": "5: Networks",
    "section": "vizNetwork Plot",
    "text": "vizNetwork Plot\n\nvisNetwork(nodes = node_tbl, \n           edges = edge_tbl,\n           main = \"Majors and Minors at Princeton\\nAmong SML 201 Students\")"
  },
  {
    "objectID": "posts/05_networks/05_networks.html#node-tibble-2",
    "href": "posts/05_networks/05_networks.html#node-tibble-2",
    "title": "5: Networks",
    "section": "Node Tibble",
    "text": "Node Tibble\n\nnode_tbl &lt;- tibble(\n  id = classes_df$class1,\n  label = classes_df$class1) |&gt;\n  bind_rows(tibble(\n    id = classes_df$class2,\n    label = classes_df$class2\n  )) |&gt;\n  bind_rows(tibble(\n    id = classes_df$class3,\n    label = classes_df$class3\n  )) |&gt;\n  bind_rows(tibble(\n    id = classes_df$class4,\n    label = classes_df$class4\n  )) |&gt;\n  bind_rows(tibble(\n    id = classes_df$class5,\n    label = classes_df$class5\n  )) |&gt;\n  bind_rows(tibble(\n    id = classes_df$class6,\n    label = classes_df$class6\n  )) |&gt;\n  distinct() |&gt;\n  filter(!is.na(id)) |&gt;\n  filter(id != \"SML 201\") |&gt;\n  separate(id, into = c(\"dept\", \"num\"), \n           sep = \" \", remove = FALSE)"
  },
  {
    "objectID": "posts/05_networks/05_networks.html#edge-tibble-2",
    "href": "posts/05_networks/05_networks.html#edge-tibble-2",
    "title": "5: Networks",
    "section": "Edge Tibble",
    "text": "Edge Tibble\n\nedge_tbl &lt;- tibble(\n  from   = classes_df$class1,\n  to     = classes_df$class2\n) |&gt;\n  bind_rows(tibble(\n    from = classes_df$class1,\n    to   = classes_df$class3\n  )) |&gt;\n  bind_rows(tibble(\n    from = classes_df$class1,\n    to   = classes_df$class4\n  )) |&gt;\n  bind_rows(tibble(\n    from = classes_df$class1,\n    to   = classes_df$class5\n  )) |&gt;\n  bind_rows(tibble(\n    from = classes_df$class1,\n    to   = classes_df$class6\n  )) |&gt;\n  bind_rows(tibble(\n    from = classes_df$class2,\n    to   = classes_df$class3\n  )) |&gt;\n  bind_rows(tibble(\n    from = classes_df$class2,\n    to   = classes_df$class4\n  )) |&gt;\n  bind_rows(tibble(\n    from = classes_df$class2,\n    to   = classes_df$class5\n  )) |&gt;\n  bind_rows(tibble(\n    from = classes_df$class2,\n    to   = classes_df$class6\n  )) |&gt;\n  bind_rows(tibble(\n    from = classes_df$class3,\n    to   = classes_df$class4\n  )) |&gt;\n  bind_rows(tibble(\n    from = classes_df$class3,\n    to   = classes_df$class5\n  )) |&gt;\n  bind_rows(tibble(\n    from = classes_df$class3,\n    to   = classes_df$class6\n  )) |&gt;\n  bind_rows(tibble(\n    from = classes_df$class4,\n    to   = classes_df$class5\n  )) |&gt;\n  bind_rows(tibble(\n    from = classes_df$class4,\n    to   = classes_df$class6\n  )) |&gt;\n  bind_rows(tibble(\n    from = classes_df$class5,\n    to   = classes_df$class6\n  )) |&gt;\n  distinct() |&gt;\n  filter(!is.na(to)) |&gt;\n  filter(from != \"SML 201\") |&gt;\n  separate(from, into = c(\"dept_from\", \"num_from\"), \n           sep = \" \", remove = FALSE) |&gt;\n  separate(to, into = c(\"dept_to\", \"num_to\"), \n           sep = \" \", remove = FALSE)"
  },
  {
    "objectID": "posts/05_networks/05_networks.html#viznetwork-plot-2",
    "href": "posts/05_networks/05_networks.html#viznetwork-plot-2",
    "title": "5: Networks",
    "section": "vizNetwork Plot",
    "text": "vizNetwork Plot\n\nvisNetwork(nodes = node_tbl, \n           edges = edge_tbl,\n           main = \"Current Classes\",\n           submain = \"Among SML 201 Students\") |&gt;\n  visPhysics(maxVelocity = 5,\n             stabilization = list(iterations = 5))"
  },
  {
    "objectID": "posts/05_networks/05_networks.html#highlighted-plot",
    "href": "posts/05_networks/05_networks.html#highlighted-plot",
    "title": "5: Networks",
    "section": "Highlighted Plot",
    "text": "Highlighted Plot\n\ndept_abbrev = \"MOL\"\n\nnode_tbl &lt;- node_tbl |&gt;\n  mutate(color = ifelse(dept == dept_abbrev,\n                        \"#E77500\", \"#DDDDDD\"),\n         opacity = ifelse(dept == dept_abbrev,\n                        1.0, 0.5))\nedge_tbl &lt;- edge_tbl |&gt;\n  mutate(color = ifelse(dept_from == dept_abbrev | dept_to == dept_abbrev,\n                        \"#E77500\", \"#DDDDDD\"))\n\n\nvisNetwork(nodes = node_tbl, \n           edges = edge_tbl,\n           main = \"Classes Connected to MOL BIO\",\n           submain = \"Among SML 201 Students\") |&gt;\n  visPhysics(maxVelocity = 5,\n             stabilization = list(iterations = 5))"
  },
  {
    "objectID": "posts/05_networks/05_networks.html#template-files",
    "href": "posts/05_networks/05_networks.html#template-files",
    "title": "5: Networks",
    "section": "",
    "text": "Moving forward, those who want to type along with the lecture sessio will probably want to use template files\n\nGo to our Canvas Page –&gt; Files –&gt; lecture_notes\nDownload today’s template file\nMove that .qmd file into your SML 201 folder\nOpen that template file"
  },
  {
    "objectID": "lecture_code/session4notes_L02.html",
    "href": "lecture_code/session4notes_L02.html",
    "title": "session 4 notes",
    "section": "",
    "text": "library(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndemo_df &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/04_categories/sml201survey.csv\")\n\nRows: 133 Columns: 84\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (38): timestamp, currentCourse, statsBefore, classStanding, major, resid...\ndbl (46): numCourses, GPA, hoursStudying, age, height, shoeSize, weight, cal...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "lecture_code/session4notes_L02.html#filtering",
    "href": "lecture_code/session4notes_L02.html#filtering",
    "title": "session 4 notes",
    "section": "Filtering",
    "text": "Filtering\n\ndf2 &lt;- select(demo_df, residentialCollege, languages)\n\ndf3 &lt;- filter(df2, residentialCollege == \"Mathey\")\n\n# sample means\nxbar &lt;- mean(df3$languages, na.rm = TRUE)\nxbar\n\n[1] 2.092308\n\ndf4 &lt;- filter(df2, residentialCollege == \"Forbes\")\nxbar &lt;- mean(df4$languages, na.rm = TRUE)\nxbar\n\n[1] 2.552941\n\n\n\nkey observation: the sample mean can change based on the sampling\n\n\ndf_grouped &lt;- group_by(df2, residentialCollege)\nsummarize(df_grouped,\n          xbar = mean(languages, na.rm = TRUE))\n\n# A tibble: 8 × 2\n  residentialCollege   xbar\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Butler               2.05\n2 Forbes               2.55\n3 Mathey               2.09\n4 New College West     2.53\n5 Rockefeller          2.37\n6 Whitman              2.32\n7 Yeh College          2.29\n8 &lt;NA&gt;               NaN   \n\n\n\nall_means &lt;- summarize(df_grouped,\n          xbar = mean(languages, na.rm = TRUE))\n\n# variables go inside aesthetics\nggplot(all_means, aes(x = residentialCollege, y = xbar)) +\n  \n  # when we want to plot the numbers directly, use \"identity\"\n  geom_bar(color = \"red\", fill = \"green\",stat = \"identity\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`)."
  },
  {
    "objectID": "lecture_code/session4notes_L02.html#more-examples",
    "href": "lecture_code/session4notes_L02.html#more-examples",
    "title": "session 4 notes",
    "section": "More Examples",
    "text": "More Examples\nWhat time do you go to sleep?\n\ntable(demo_df$sleepTime)\n\n\n       1 AM       10 PM       11 PM 12 midnight        2 AM        3 AM \n         33           6          19          52          15           6 \n\n\n\nsummary(demo_df$flossing)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  0.000   2.000   5.000   5.224   7.000  14.000      10 \n\n\n\nsummary(demo_df$attractiveness)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00   50.00   65.00   63.18   79.00  100.00      16 \n\n\n\nsummary(demo_df$intelligence)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00   60.00   70.00   67.52   83.00  100.00      12 \n\n\n\nsummary(demo_df$showers)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   2.00    7.00    7.00    7.55    8.00   16.00       4 \n\n\n\ntable(demo_df$drugUse)\n\n\n No Yes \n 99  32 \n\n\n\ntable(demo_df$pancakesWaffles)\n\n\nPancakes  Waffles \n      59       69 \n\n\n\nsummary(demo_df$SAT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      0    1500    1540    1617    1560   15560      24 \n\ndemo_df$SAT[demo_df$SAT &gt; 1600] &lt;- NA\ndemo_df$SAT[demo_df$SAT &lt; 400] &lt;- NA\n\nsummary(demo_df$SAT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1100    1500    1540    1516    1560    1600      27"
  },
  {
    "objectID": "lecture_code/session3notes_L02.html",
    "href": "lecture_code/session3notes_L02.html",
    "title": "session 3 notes",
    "section": "",
    "text": "A &lt;- seq(-3,3)\nB &lt;- seq(-9, 9, by = 3)\nA\n\n[1] -3 -2 -1  0  1  2  3\n\nB\n\n[1] -9 -6 -3  0  3  6  9\n\n\n\n\n\nmean(A)\n\n[1] 0\n\nmean(B)\n\n[1] 0\n\nmedian(A)\n\n[1] 0\n\nmedian(B)\n\n[1] 0\n\n\n\n\n\n\nvar(A)\n\n[1] 4.666667\n\nvar(B)\n\n[1] 42\n\n\nThe variance of set B is larger than the variance of set A.\n\n\n\n\nsd(A)\n\n[1] 2.160247\n\nsd(B)\n\n[1] 6.480741\n\n\nThe variance of set B is larger than the variance of set A."
  },
  {
    "objectID": "lecture_code/session3notes_L02.html#centrality",
    "href": "lecture_code/session3notes_L02.html#centrality",
    "title": "session 3 notes",
    "section": "",
    "text": "mean(A)\n\n[1] 0\n\nmean(B)\n\n[1] 0\n\nmedian(A)\n\n[1] 0\n\nmedian(B)\n\n[1] 0"
  },
  {
    "objectID": "lecture_code/session3notes_L02.html#variance",
    "href": "lecture_code/session3notes_L02.html#variance",
    "title": "session 3 notes",
    "section": "",
    "text": "var(A)\n\n[1] 4.666667\n\nvar(B)\n\n[1] 42\n\n\nThe variance of set B is larger than the variance of set A."
  },
  {
    "objectID": "lecture_code/session3notes_L02.html#standard-deviation",
    "href": "lecture_code/session3notes_L02.html#standard-deviation",
    "title": "session 3 notes",
    "section": "",
    "text": "sd(A)\n\n[1] 2.160247\n\nsd(B)\n\n[1] 6.480741\n\n\nThe variance of set B is larger than the variance of set A."
  },
  {
    "objectID": "lecture_code/session3notes_L02.html#exploration",
    "href": "lecture_code/session3notes_L02.html#exploration",
    "title": "session 3 notes",
    "section": "exploration",
    "text": "exploration\n\nhead(dow_df)\n\n# A tibble: 6 × 11\n  ticker ref_date   price_open price_high price_low price_close   volume\n  &lt;chr&gt;  &lt;date&gt;          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 AAPL   2024-01-02       187.       188.      184.        186. 82488700\n2 AAPL   2024-01-03       184.       186.      183.        184. 58414500\n3 AAPL   2024-01-04       182.       183.      181.        182. 71983600\n4 AAPL   2024-01-05       182.       183.      180.        181. 62303300\n5 AAPL   2024-01-08       182.       186.      182.        186. 59144500\n6 AAPL   2024-01-09       184.       185.      183.        185. 42841800\n# ℹ 4 more variables: price_adjusted &lt;dbl&gt;, ret_adjusted_prices &lt;dbl&gt;,\n#   ret_closing_prices &lt;dbl&gt;, cumret_adjusted_prices &lt;dbl&gt;\n\n\n\ntail(dow_df)\n\n# A tibble: 6 × 11\n  ticker ref_date   price_open price_high price_low price_close   volume\n  &lt;chr&gt;  &lt;date&gt;          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 WMT    2024-08-29       76.0       76.5      75.7        76.4 11856800\n2 WMT    2024-08-30       76.4       77.5      76.2        77.2 23230000\n3 WMT    2024-09-03       77.3       77.8      76.8        77.2 22666100\n4 WMT    2024-09-04       77.3       77.5      76.7        77.2 18442800\n5 WMT    2024-09-05       77.2       77.4      76.4        77.0 13082900\n6 WMT    2024-09-06       76.9       77.3      76.3        76.6 14545900\n# ℹ 4 more variables: price_adjusted &lt;dbl&gt;, ret_adjusted_prices &lt;dbl&gt;,\n#   ret_closing_prices &lt;dbl&gt;, cumret_adjusted_prices &lt;dbl&gt;\n\n\n\nstr(dow_df, give.attr = FALSE)\n\nspc_tbl_ [5,160 × 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ ticker                : chr [1:5160] \"AAPL\" \"AAPL\" \"AAPL\" \"AAPL\" ...\n $ ref_date              : Date[1:5160], format: \"2024-01-02\" \"2024-01-03\" ...\n $ price_open            : num [1:5160] 187 184 182 182 182 ...\n $ price_high            : num [1:5160] 188 186 183 183 186 ...\n $ price_low             : num [1:5160] 184 183 181 180 182 ...\n $ price_close           : num [1:5160] 186 184 182 181 186 ...\n $ volume                : num [1:5160] 82488700 58414500 71983600 62303300 59144500 ...\n $ price_adjusted        : num [1:5160] 185 184 181 180 185 ...\n $ ret_adjusted_prices   : num [1:5160] NA -0.00749 -0.0127 -0.00401 0.02417 ...\n $ ret_closing_prices    : num [1:5160] NA -0.00749 -0.0127 -0.00401 0.02417 ...\n $ cumret_adjusted_prices: num [1:5160] 1 0.993 0.98 0.976 1 ...\n\n\n\ncolnames(dow_df)\n\n [1] \"ticker\"                 \"ref_date\"               \"price_open\"            \n [4] \"price_high\"             \"price_low\"              \"price_close\"           \n [7] \"volume\"                 \"price_adjusted\"         \"ret_adjusted_prices\"   \n[10] \"ret_closing_prices\"     \"cumret_adjusted_prices\""
  },
  {
    "objectID": "lecture_code/session3notes_L02.html#which-stocks",
    "href": "lecture_code/session3notes_L02.html#which-stocks",
    "title": "session 3 notes",
    "section": "Which stocks?",
    "text": "Which stocks?\ntable tallies the amounts in a categorical variable\n\ntable(dow_df$ticker)\n\n\nAAPL AMGN AMZN  AXP   BA  CAT  CRM CSCO  CVX  DIS  DOW   GS   HD  HON  IBM INTC \n 172  172  172  172  172  172  172  172  172  172  172  172  172  172  172  172 \n JNJ  JPM   KO  MCD  MMM  MRK MSFT  NKE   PG  TRV  UNH    V   VZ  WMT \n 172  172  172  172  172  172  172  172  172  172  172  172  172  172"
  },
  {
    "objectID": "lecture_code/session3notes_L02.html#histograms",
    "href": "lecture_code/session3notes_L02.html#histograms",
    "title": "session 3 notes",
    "section": "Histograms",
    "text": "Histograms\nA histogram shows us the distribution of a numerical variable.\n\ndow_df |&gt;\n  filter(ticker == \"VZ\") |&gt;\n  ggplot(aes(x = price_close)) +\n  geom_histogram() +\n  labs(title = \"Verison Stock\",\n       subtitle = \"2024 prices\",\n       caption = \"SML 201\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\ndow_df |&gt;\n  filter(ticker == \"GS\") |&gt;\n  ggplot(aes(x = price_close)) +\n  geom_histogram() +\n  labs(title = \"Goldman Sachs Stock\",\n       subtitle = \"2024 prices\",\n       caption = \"SML 201\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lecture_code/session3notes_L02.html#z-scores-1",
    "href": "lecture_code/session3notes_L02.html#z-scores-1",
    "title": "session 3 notes",
    "section": "Z-scores",
    "text": "Z-scores\nSometimes we want to rescale numerical columns to be able to compare them together.\n\nsummary(dow_df$price_close)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.89   96.86  181.35  198.37  271.92  604.18 \n\n\n\ndow_df &lt;- dow_df |&gt;\n  #mutate attachs a new column\n  mutate(price_scaled = scale(price_close))\n\n\nsummary(dow_df$price_scaled)\n\n       V1         \n Min.   :-1.4525  \n 1st Qu.:-0.8215  \n Median :-0.1377  \n Mean   : 0.0000  \n 3rd Qu.: 0.5953  \n Max.   : 3.2842"
  },
  {
    "objectID": "lecture_code/session2notes_L01.html",
    "href": "lecture_code/session2notes_L01.html",
    "title": "session 2 notes",
    "section": "",
    "text": "add up the values\ndivide by amount of values\n\n“c” means “concatenate” (or “combine”)\nto run one line of code: CTRL + ENTER\n\n# create some data\nsome_data &lt;- c(32, 45, 16, 78, 39)\n\nsum(some_data)\n\n[1] 210\n\nlength(some_data)\n\n[1] 5\n\nsum(some_data) / length(some_data)\n\n[1] 42\n\nmean(some_data)\n\n[1] 42\n\n\n\n\nNA for non-applicable, or “missing data”\nBy default, R stops upon missing\n\nwant to avoid the missing data\n\n\nsome_data &lt;- c(32, 45, 16, 78, NA, 39)\n\nmean(some_data)\n\n[1] NA\n\nmean(some_data, na.rm = TRUE)\n\n[1] 42"
  },
  {
    "objectID": "lecture_code/session2notes_L01.html#missing-data",
    "href": "lecture_code/session2notes_L01.html#missing-data",
    "title": "session 2 notes",
    "section": "",
    "text": "NA for non-applicable, or “missing data”\nBy default, R stops upon missing\n\nwant to avoid the missing data\n\n\nsome_data &lt;- c(32, 45, 16, 78, NA, 39)\n\nmean(some_data)\n\n[1] NA\n\nmean(some_data, na.rm = TRUE)\n\n[1] 42"
  },
  {
    "objectID": "lecture_code/session2notes_L01.html#explore-the-data",
    "href": "lecture_code/session2notes_L01.html#explore-the-data",
    "title": "session 2 notes",
    "section": "explore the data",
    "text": "explore the data\n\nhead(olympic_df1)\n\n# A tibble: 6 × 36\n     code name         name_short name_tv gender `function` country_code country\n    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;  \n1 1532873 AMOYAN Malk… AMOYAN M   Malkha… Male   Athlete    ARM          Armenia\n2 1532874 GALSTYAN Sl… GALSTYAN S Slavik… Male   Athlete    ARM          Armenia\n3 1532944 HARUTYUNYAN… HARUTYUNY… Arsen … Male   Athlete    ARM          Armenia\n4 1532945 TEVANYAN Va… TEVANYAN V Vazgen… Male   Athlete    ARM          Armenia\n5 1533136 BASS BITTAY… BASS BITT… Gina M… Female Athlete    GAM          Gambia \n6 1533176 CAMARA Ebra… CAMARA E   Ebrahi… Male   Athlete    GAM          Gambia \n# ℹ 28 more variables: country_full &lt;chr&gt;, nationality &lt;chr&gt;,\n#   nationality_full &lt;chr&gt;, nationality_code &lt;chr&gt;, height &lt;dbl&gt;, weight &lt;dbl&gt;,\n#   disciplines &lt;chr&gt;, events &lt;chr&gt;, birth_date &lt;date&gt;, birth_place &lt;chr&gt;,\n#   birth_country &lt;chr&gt;, residence_place &lt;chr&gt;, residence_country &lt;chr&gt;,\n#   nickname &lt;chr&gt;, hobbies &lt;chr&gt;, occupation &lt;chr&gt;, education &lt;chr&gt;,\n#   family &lt;chr&gt;, lang &lt;chr&gt;, coach &lt;chr&gt;, reason &lt;chr&gt;, hero &lt;chr&gt;,\n#   influence &lt;chr&gt;, philosophy &lt;chr&gt;, sporting_relatives &lt;chr&gt;, …\n\nstr(olympic_df1) #structure\n\nspc_tbl_ [8,687 × 36] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ code              : num [1:8687] 1532873 1532874 1532944 1532945 1533136 ...\n $ name              : chr [1:8687] \"AMOYAN Malkhas\" \"GALSTYAN Slavik\" \"HARUTYUNYAN Arsen\" \"TEVANYAN Vazgen\" ...\n $ name_short        : chr [1:8687] \"AMOYAN M\" \"GALSTYAN S\" \"HARUTYUNYAN A\" \"TEVANYAN V\" ...\n $ name_tv           : chr [1:8687] \"Malkhas AMOYAN\" \"Slavik GALSTYAN\" \"Arsen HARUTYUNYAN\" \"Vazgen TEVANYAN\" ...\n $ gender            : chr [1:8687] \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ function          : chr [1:8687] \"Athlete\" \"Athlete\" \"Athlete\" \"Athlete\" ...\n $ country_code      : chr [1:8687] \"ARM\" \"ARM\" \"ARM\" \"ARM\" ...\n $ country           : chr [1:8687] \"Armenia\" \"Armenia\" \"Armenia\" \"Armenia\" ...\n $ country_full      : chr [1:8687] \"Armenia\" \"Armenia\" \"Armenia\" \"Armenia\" ...\n $ nationality       : chr [1:8687] \"Armenia\" \"Armenia\" \"Armenia\" \"Armenia\" ...\n $ nationality_full  : chr [1:8687] \"Armenia\" \"Armenia\" \"Armenia\" \"Armenia\" ...\n $ nationality_code  : chr [1:8687] \"ARM\" \"ARM\" \"ARM\" \"ARM\" ...\n $ height            : num [1:8687] 0 0 0 0 161 178 0 0 183 0 ...\n $ weight            : num [1:8687] -99 -99 -99 -99 -99 -99 -99 -99 -99 -99 ...\n $ disciplines       : chr [1:8687] \"['Wrestling']\" \"['Wrestling']\" \"['Wrestling']\" \"['Wrestling']\" ...\n $ events            : chr [1:8687] \"[\\\"Men's Greco-Roman 77kg\\\"]\" \"[\\\"Men's Greco-Roman 67kg\\\"]\" \"[\\\"Men's Freestyle 57kg\\\"]\" \"[\\\"Men's Freestyle 65kg\\\"]\" ...\n $ birth_date        : Date[1:8687], format: \"1999-01-22\" \"1996-12-21\" ...\n $ birth_place       : chr [1:8687] \"YEREVAN\" NA \"MASIS\" \"POKR VEDI\" ...\n $ birth_country     : chr [1:8687] \"Armenia\" NA \"Armenia\" \"Armenia\" ...\n $ residence_place   : chr [1:8687] \"YEREVAN\" \"YEREVAN\" \"YEREVAN\" NA ...\n $ residence_country : chr [1:8687] \"Armenia\" \"Armenia\" \"Armenia\" \"Armenia\" ...\n $ nickname          : chr [1:8687] NA NA NA NA ...\n $ hobbies           : chr [1:8687] NA NA NA NA ...\n $ occupation        : chr [1:8687] NA NA \"Athlete\" \"Athlete\" ...\n $ education         : chr [1:8687] NA NA \"Graduated with a Master's degree from the Armenian State Institute of Physical Culture and Sport (2023)\" \"Studied at the Armenian State Institute of Physical Culture and Sport (Yerevan, ARM)\" ...\n $ family            : chr [1:8687] NA NA \"Wife, Diana (married October 2022). Daughter, Marias (born 2023)\" \"Wife, Sona (married November 2023)\" ...\n $ lang              : chr [1:8687] \"Armenian\" \"Armenian\" \"Armenian\" \"Armenian, Russian\" ...\n $ coach             : chr [1:8687] NA \"Personal: Martin Alekhanyan (ARM).&lt;br&gt;National: Armen Babalaryan (ARM)\" \"National: Habetnak Kurghinyan\" \"National: Habetnak Kurghinyan (ARM)\" ...\n $ reason            : chr [1:8687] NA NA \"While doing karate he noticed wrestlers training and decided to give it a try. He also tried judo but his fathe\"| __truncated__ \"“My family did not like wrestling very much. At first I wanted to do boxing but my older friends advised me to \"| __truncated__ ...\n $ hero              : chr [1:8687] NA NA \"Wrestler Armen Nazaryan (ARM, BUL), two-time Olympic champion (1996, 2000) and 2004 bronze medallist. Eight-tim\"| __truncated__ NA ...\n $ influence         : chr [1:8687] NA NA NA NA ...\n $ philosophy        : chr [1:8687] \"\\\"To become a good athlete, you first have to be a good person.\\\" (ankakh.com, 6 Oct 2018)\" NA \"“Nothing is impossible, set goals in front of you, fight and achieve it.” (Instagram, 13 May 2023)\" NA ...\n $ sporting_relatives: chr [1:8687] \"Uncle, Roman Amoyan (wrestling), 2008 Olympic bronze medallist and two-time European champion in Greco-Roman\" NA NA NA ...\n $ ritual            : chr [1:8687] NA NA NA NA ...\n $ other_sports      : chr [1:8687] NA NA NA NA ...\n $ age               : num [1:8687] 25 28 25 25 29 28 30 27 27 17 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   code = col_double(),\n  ..   name = col_character(),\n  ..   name_short = col_character(),\n  ..   name_tv = col_character(),\n  ..   gender = col_character(),\n  ..   `function` = col_character(),\n  ..   country_code = col_character(),\n  ..   country = col_character(),\n  ..   country_full = col_character(),\n  ..   nationality = col_character(),\n  ..   nationality_full = col_character(),\n  ..   nationality_code = col_character(),\n  ..   height = col_double(),\n  ..   weight = col_double(),\n  ..   disciplines = col_character(),\n  ..   events = col_character(),\n  ..   birth_date = col_date(format = \"\"),\n  ..   birth_place = col_character(),\n  ..   birth_country = col_character(),\n  ..   residence_place = col_character(),\n  ..   residence_country = col_character(),\n  ..   nickname = col_character(),\n  ..   hobbies = col_character(),\n  ..   occupation = col_character(),\n  ..   education = col_character(),\n  ..   family = col_character(),\n  ..   lang = col_character(),\n  ..   coach = col_character(),\n  ..   reason = col_character(),\n  ..   hero = col_character(),\n  ..   influence = col_character(),\n  ..   philosophy = col_character(),\n  ..   sporting_relatives = col_character(),\n  ..   ritual = col_character(),\n  ..   other_sports = col_character(),\n  ..   age = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\ncolnames(olympic_df1)\n\n [1] \"code\"               \"name\"               \"name_short\"        \n [4] \"name_tv\"            \"gender\"             \"function\"          \n [7] \"country_code\"       \"country\"            \"country_full\"      \n[10] \"nationality\"        \"nationality_full\"   \"nationality_code\"  \n[13] \"height\"             \"weight\"             \"disciplines\"       \n[16] \"events\"             \"birth_date\"         \"birth_place\"       \n[19] \"birth_country\"      \"residence_place\"    \"residence_country\" \n[22] \"nickname\"           \"hobbies\"            \"occupation\"        \n[25] \"education\"          \"family\"             \"lang\"              \n[28] \"coach\"              \"reason\"             \"hero\"              \n[31] \"influence\"          \"philosophy\"         \"sporting_relatives\"\n[34] \"ritual\"             \"other_sports\"       \"age\"               \n\n\n\nmean(olympic_df1$weight)\n\n[1] NA\n\nmean(olympic_df1$weight, na.rm = TRUE)\n\n[1] -93.50138\n\nsummary(olympic_df1$weight)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  -99.0   -99.0   -99.0   -93.5   -99.0   113.0      11 \n\n\n\nolympic_df1$weight[olympic_df1$weight &lt;= 0] &lt;- NA\nolympic_df2$weight[olympic_df2$weight &lt;= 0] &lt;- NA\n\nmean(olympic_df1$weight, na.rm = TRUE)\n\n[1] 77.68889\n\nsummary(olympic_df1$weight)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  51.00   67.00   76.50   77.69   88.00  113.00    8417"
  },
  {
    "objectID": "lecture_code/session2notes_L01.html#filter",
    "href": "lecture_code/session2notes_L01.html#filter",
    "title": "session 2 notes",
    "section": "Filter",
    "text": "Filter\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nTurkey_df1 &lt;- olympic_df1 |&gt;\n  filter(country_code == \"TUR\")"
  },
  {
    "objectID": "lecture_code/session2notes_L02.html",
    "href": "lecture_code/session2notes_L02.html",
    "title": "session 2 notes",
    "section": "",
    "text": "add up the numbers\ndivide by the amount of numbers\n\nto run a line of code: CTRL + ENTER\n“c” for concatenate (“combine”)\n\nsome_data &lt;- c(32, 45, 16, 78, 39)\nsum(some_data)\n\n[1] 210\n\nlength(some_data)\n\n[1] 5\n\nsum(some_data) / length(some_data)\n\n[1] 42\n\nmean(some_data)\n\n[1] 42\n\n\n\n\nNA for not applicable (“missing data”)\nR stops calculations upon missing data\n* want to avoid the missing data\n\nsome_data2 &lt;- c(32, 45, 16, 78, NA, 39) #another list\nmean(some_data2)\n\n[1] NA\n\nmean(some_data2, na.rm = TRUE)\n\n[1] 42"
  },
  {
    "objectID": "lecture_code/session2notes_L02.html#missing-data",
    "href": "lecture_code/session2notes_L02.html#missing-data",
    "title": "session 2 notes",
    "section": "",
    "text": "NA for not applicable (“missing data”)\nR stops calculations upon missing data\n* want to avoid the missing data\n\nsome_data2 &lt;- c(32, 45, 16, 78, NA, 39) #another list\nmean(some_data2)\n\n[1] NA\n\nmean(some_data2, na.rm = TRUE)\n\n[1] 42"
  },
  {
    "objectID": "lecture_code/session2notes_L02.html#load-data",
    "href": "lecture_code/session2notes_L02.html#load-data",
    "title": "session 2 notes",
    "section": "Load Data",
    "text": "Load Data\n\nolympic_df1 &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/02_centrality/olympic_data.csv\")\n\nRows: 8687 Columns: 36\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (31): name, name_short, name_tv, gender, function, country_code, countr...\ndbl   (4): code, height, weight, age\ndate  (1): birth_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nolympic_df2 &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/02_centrality/olympic_data2.csv\")\n\nRows: 11110 Columns: 36\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (31): name, name_short, name_tv, gender, function, country_code, countr...\ndbl   (4): code, height, weight, age\ndate  (1): birth_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "lecture_code/session2notes_L02.html#explore-data",
    "href": "lecture_code/session2notes_L02.html#explore-data",
    "title": "session 2 notes",
    "section": "Explore Data",
    "text": "Explore Data\n\nhead(olympic_df1)\n\n# A tibble: 6 × 36\n     code name         name_short name_tv gender `function` country_code country\n    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;  \n1 1532873 AMOYAN Malk… AMOYAN M   Malkha… Male   Athlete    ARM          Armenia\n2 1532874 GALSTYAN Sl… GALSTYAN S Slavik… Male   Athlete    ARM          Armenia\n3 1532944 HARUTYUNYAN… HARUTYUNY… Arsen … Male   Athlete    ARM          Armenia\n4 1532945 TEVANYAN Va… TEVANYAN V Vazgen… Male   Athlete    ARM          Armenia\n5 1533136 BASS BITTAY… BASS BITT… Gina M… Female Athlete    GAM          Gambia \n6 1533176 CAMARA Ebra… CAMARA E   Ebrahi… Male   Athlete    GAM          Gambia \n# ℹ 28 more variables: country_full &lt;chr&gt;, nationality &lt;chr&gt;,\n#   nationality_full &lt;chr&gt;, nationality_code &lt;chr&gt;, height &lt;dbl&gt;, weight &lt;dbl&gt;,\n#   disciplines &lt;chr&gt;, events &lt;chr&gt;, birth_date &lt;date&gt;, birth_place &lt;chr&gt;,\n#   birth_country &lt;chr&gt;, residence_place &lt;chr&gt;, residence_country &lt;chr&gt;,\n#   nickname &lt;chr&gt;, hobbies &lt;chr&gt;, occupation &lt;chr&gt;, education &lt;chr&gt;,\n#   family &lt;chr&gt;, lang &lt;chr&gt;, coach &lt;chr&gt;, reason &lt;chr&gt;, hero &lt;chr&gt;,\n#   influence &lt;chr&gt;, philosophy &lt;chr&gt;, sporting_relatives &lt;chr&gt;, …\n\nstr(olympic_df1) #structure\n\nspc_tbl_ [8,687 × 36] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ code              : num [1:8687] 1532873 1532874 1532944 1532945 1533136 ...\n $ name              : chr [1:8687] \"AMOYAN Malkhas\" \"GALSTYAN Slavik\" \"HARUTYUNYAN Arsen\" \"TEVANYAN Vazgen\" ...\n $ name_short        : chr [1:8687] \"AMOYAN M\" \"GALSTYAN S\" \"HARUTYUNYAN A\" \"TEVANYAN V\" ...\n $ name_tv           : chr [1:8687] \"Malkhas AMOYAN\" \"Slavik GALSTYAN\" \"Arsen HARUTYUNYAN\" \"Vazgen TEVANYAN\" ...\n $ gender            : chr [1:8687] \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ function          : chr [1:8687] \"Athlete\" \"Athlete\" \"Athlete\" \"Athlete\" ...\n $ country_code      : chr [1:8687] \"ARM\" \"ARM\" \"ARM\" \"ARM\" ...\n $ country           : chr [1:8687] \"Armenia\" \"Armenia\" \"Armenia\" \"Armenia\" ...\n $ country_full      : chr [1:8687] \"Armenia\" \"Armenia\" \"Armenia\" \"Armenia\" ...\n $ nationality       : chr [1:8687] \"Armenia\" \"Armenia\" \"Armenia\" \"Armenia\" ...\n $ nationality_full  : chr [1:8687] \"Armenia\" \"Armenia\" \"Armenia\" \"Armenia\" ...\n $ nationality_code  : chr [1:8687] \"ARM\" \"ARM\" \"ARM\" \"ARM\" ...\n $ height            : num [1:8687] 0 0 0 0 161 178 0 0 183 0 ...\n $ weight            : num [1:8687] -99 -99 -99 -99 -99 -99 -99 -99 -99 -99 ...\n $ disciplines       : chr [1:8687] \"['Wrestling']\" \"['Wrestling']\" \"['Wrestling']\" \"['Wrestling']\" ...\n $ events            : chr [1:8687] \"[\\\"Men's Greco-Roman 77kg\\\"]\" \"[\\\"Men's Greco-Roman 67kg\\\"]\" \"[\\\"Men's Freestyle 57kg\\\"]\" \"[\\\"Men's Freestyle 65kg\\\"]\" ...\n $ birth_date        : Date[1:8687], format: \"1999-01-22\" \"1996-12-21\" ...\n $ birth_place       : chr [1:8687] \"YEREVAN\" NA \"MASIS\" \"POKR VEDI\" ...\n $ birth_country     : chr [1:8687] \"Armenia\" NA \"Armenia\" \"Armenia\" ...\n $ residence_place   : chr [1:8687] \"YEREVAN\" \"YEREVAN\" \"YEREVAN\" NA ...\n $ residence_country : chr [1:8687] \"Armenia\" \"Armenia\" \"Armenia\" \"Armenia\" ...\n $ nickname          : chr [1:8687] NA NA NA NA ...\n $ hobbies           : chr [1:8687] NA NA NA NA ...\n $ occupation        : chr [1:8687] NA NA \"Athlete\" \"Athlete\" ...\n $ education         : chr [1:8687] NA NA \"Graduated with a Master's degree from the Armenian State Institute of Physical Culture and Sport (2023)\" \"Studied at the Armenian State Institute of Physical Culture and Sport (Yerevan, ARM)\" ...\n $ family            : chr [1:8687] NA NA \"Wife, Diana (married October 2022). Daughter, Marias (born 2023)\" \"Wife, Sona (married November 2023)\" ...\n $ lang              : chr [1:8687] \"Armenian\" \"Armenian\" \"Armenian\" \"Armenian, Russian\" ...\n $ coach             : chr [1:8687] NA \"Personal: Martin Alekhanyan (ARM).&lt;br&gt;National: Armen Babalaryan (ARM)\" \"National: Habetnak Kurghinyan\" \"National: Habetnak Kurghinyan (ARM)\" ...\n $ reason            : chr [1:8687] NA NA \"While doing karate he noticed wrestlers training and decided to give it a try. He also tried judo but his fathe\"| __truncated__ \"“My family did not like wrestling very much. At first I wanted to do boxing but my older friends advised me to \"| __truncated__ ...\n $ hero              : chr [1:8687] NA NA \"Wrestler Armen Nazaryan (ARM, BUL), two-time Olympic champion (1996, 2000) and 2004 bronze medallist. Eight-tim\"| __truncated__ NA ...\n $ influence         : chr [1:8687] NA NA NA NA ...\n $ philosophy        : chr [1:8687] \"\\\"To become a good athlete, you first have to be a good person.\\\" (ankakh.com, 6 Oct 2018)\" NA \"“Nothing is impossible, set goals in front of you, fight and achieve it.” (Instagram, 13 May 2023)\" NA ...\n $ sporting_relatives: chr [1:8687] \"Uncle, Roman Amoyan (wrestling), 2008 Olympic bronze medallist and two-time European champion in Greco-Roman\" NA NA NA ...\n $ ritual            : chr [1:8687] NA NA NA NA ...\n $ other_sports      : chr [1:8687] NA NA NA NA ...\n $ age               : num [1:8687] 25 28 25 25 29 28 30 27 27 17 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   code = col_double(),\n  ..   name = col_character(),\n  ..   name_short = col_character(),\n  ..   name_tv = col_character(),\n  ..   gender = col_character(),\n  ..   `function` = col_character(),\n  ..   country_code = col_character(),\n  ..   country = col_character(),\n  ..   country_full = col_character(),\n  ..   nationality = col_character(),\n  ..   nationality_full = col_character(),\n  ..   nationality_code = col_character(),\n  ..   height = col_double(),\n  ..   weight = col_double(),\n  ..   disciplines = col_character(),\n  ..   events = col_character(),\n  ..   birth_date = col_date(format = \"\"),\n  ..   birth_place = col_character(),\n  ..   birth_country = col_character(),\n  ..   residence_place = col_character(),\n  ..   residence_country = col_character(),\n  ..   nickname = col_character(),\n  ..   hobbies = col_character(),\n  ..   occupation = col_character(),\n  ..   education = col_character(),\n  ..   family = col_character(),\n  ..   lang = col_character(),\n  ..   coach = col_character(),\n  ..   reason = col_character(),\n  ..   hero = col_character(),\n  ..   influence = col_character(),\n  ..   philosophy = col_character(),\n  ..   sporting_relatives = col_character(),\n  ..   ritual = col_character(),\n  ..   other_sports = col_character(),\n  ..   age = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\ncolnames(olympic_df1) #column names\n\n [1] \"code\"               \"name\"               \"name_short\"        \n [4] \"name_tv\"            \"gender\"             \"function\"          \n [7] \"country_code\"       \"country\"            \"country_full\"      \n[10] \"nationality\"        \"nationality_full\"   \"nationality_code\"  \n[13] \"height\"             \"weight\"             \"disciplines\"       \n[16] \"events\"             \"birth_date\"         \"birth_place\"       \n[19] \"birth_country\"      \"residence_place\"    \"residence_country\" \n[22] \"nickname\"           \"hobbies\"            \"occupation\"        \n[25] \"education\"          \"family\"             \"lang\"              \n[28] \"coach\"              \"reason\"             \"hero\"              \n[31] \"influence\"          \"philosophy\"         \"sporting_relatives\"\n[34] \"ritual\"             \"other_sports\"       \"age\"               \n\n\n\nmean(olympic_df1$weight)\n\n[1] NA\n\nmean(olympic_df1$weight, na.rm = TRUE)\n\n[1] -93.50138\n\nsummary(olympic_df1$weight)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  -99.0   -99.0   -99.0   -93.5   -99.0   113.0      11 \n\n\n\nolympic_df1$weight[olympic_df1$weight &lt;= 0] &lt;- NA\nolympic_df2$weight[olympic_df2$weight &lt;= 0] &lt;- NA\n\n\nmean(olympic_df1$weight, na.rm = TRUE)\n\n[1] 77.68889\n\nsummary(olympic_df1$weight)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  51.00   67.00   76.50   77.69   88.00  113.00    8417"
  },
  {
    "objectID": "lecture_code/session2notes_L02.html#filter",
    "href": "lecture_code/session2notes_L02.html#filter",
    "title": "session 2 notes",
    "section": "Filter",
    "text": "Filter\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nTurkey_df1 &lt;- olympic_df1 |&gt;\n  filter(country_code == \"TUR\")"
  },
  {
    "objectID": "lecture_code/session4notes_L01.html",
    "href": "lecture_code/session4notes_L01.html",
    "title": "session 4 notes",
    "section": "",
    "text": "Loading a CSV file with the read_csv command.\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndemo_df &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/04_categories/sml201survey.csv\")\n\nRows: 133 Columns: 84\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (38): timestamp, currentCourse, statsBefore, classStanding, major, resid...\ndbl (46): numCourses, GPA, hoursStudying, age, height, shoeSize, weight, cal...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "lecture_code/session4notes_L01.html#visualization",
    "href": "lecture_code/session4notes_L01.html#visualization",
    "title": "session 4 notes",
    "section": "Visualization",
    "text": "Visualization\n\ndemo_df |&gt;\n  ggplot(aes(x = fct_rev(fct_infreq(major)), fill = major)) +\n  coord_flip() +\n  geom_bar(stat = \"count\") +\n  labs(title = \"SML 201 students by major\",\n       subtitle = \"Fall 2024\",\n       y = \"number of students\",\n       x = \"\") +\n  theme_minimal() + #removes gray background\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/06_geospatial/06_geospatial.html",
    "href": "posts/06_geospatial/06_geospatial.html",
    "title": "6: Geospatial",
    "section": "",
    "text": "Goal: Visualize geospatial data\nObjective: Merge shapefiles and data files\n\n\n\n\n\n\n\nNJ at a glance\n\n\n\nimage source: Totally Bamboo\n\n\n\n\n\n\nMoving forward, those who want to type along with the lecture sessio will probably want to use template files\n\nGo to our Canvas Page –&gt; Files –&gt; lecture_notes\nDownload today’s template file\nMove that .qmd file into your SML 201 folder\nOpen that template file\n\n\n\n\n\nnj_health &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/06_geospatial/nj_health.csv\")\nnj_pop    &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/06_geospatial/nj_pop.csv\")\nnj_shp    &lt;- readr::read_rds(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/06_geospatial/nj_shp.rds\")"
  },
  {
    "objectID": "posts/06_geospatial/06_geospatial.html#start",
    "href": "posts/06_geospatial/06_geospatial.html#start",
    "title": "6: Geospatial",
    "section": "",
    "text": "Goal: Visualize geospatial data\nObjective: Merge shapefiles and data files\n\n\n\n\n\n\n\nNJ at a glance\n\n\n\nimage source: Totally Bamboo"
  },
  {
    "objectID": "posts/06_geospatial/06_geospatial.html#template-files",
    "href": "posts/06_geospatial/06_geospatial.html#template-files",
    "title": "6: Geospatial",
    "section": "",
    "text": "Moving forward, those who want to type along with the lecture sessio will probably want to use template files\n\nGo to our Canvas Page –&gt; Files –&gt; lecture_notes\nDownload today’s template file\nMove that .qmd file into your SML 201 folder\nOpen that template file"
  },
  {
    "objectID": "posts/06_geospatial/06_geospatial.html#load-data",
    "href": "posts/06_geospatial/06_geospatial.html#load-data",
    "title": "6: Geospatial",
    "section": "",
    "text": "nj_health &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/06_geospatial/nj_health.csv\")\nnj_pop    &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/06_geospatial/nj_pop.csv\")\nnj_shp    &lt;- readr::read_rds(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/06_geospatial/nj_shp.rds\")"
  },
  {
    "objectID": "posts/06_geospatial/06_geospatial.html#centroids",
    "href": "posts/06_geospatial/06_geospatial.html#centroids",
    "title": "6: Geospatial",
    "section": "Centroids",
    "text": "Centroids\n\n# Calculate the centroid of each hexagon to add the label\n# https://stackoverflow.com/questions/49343958/do-the-values-returned-by-rgeosgcentroid-and-sfst-centroid-differ\ncenters &lt;- data.frame(\n  st_coordinates(st_centroid(nj_shp$geometry)),\n  id=nj_shp$COUNTY)\n\nnj_counties &lt;- nj_shp |&gt;\n  left_join(centers, by = c(\"COUNTY\" = \"id\"))"
  },
  {
    "objectID": "posts/06_geospatial/06_geospatial.html#text-for-labels",
    "href": "posts/06_geospatial/06_geospatial.html#text-for-labels",
    "title": "6: Geospatial",
    "section": "Text (for labels)",
    "text": "Text (for labels)\n\nnj_counties |&gt;\n  ggplot() +\n  geom_sf(aes(fill = COUNTY)) +\n  geom_text(aes(x = X, y = Y, label = COUNTY)) +\n  labs(title = \"Counties of New Jersey\",\n       subtitle = \"categorical data\",\n       caption = \"SML 201\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/06_geospatial/06_geospatial.html#labels",
    "href": "posts/06_geospatial/06_geospatial.html#labels",
    "title": "6: Geospatial",
    "section": "Labels",
    "text": "Labels\n\nnj_counties |&gt;\n  ggplot() +\n  geom_sf(aes(fill = COUNTY)) +\n  geom_label(aes(x = X, y = Y, label = COUNTY)) +\n  labs(title = \"Counties of New Jersey\",\n       subtitle = \"categorical data\",\n       caption = \"SML 201\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/06_geospatial/06_geospatial.html#subset",
    "href": "posts/06_geospatial/06_geospatial.html#subset",
    "title": "6: Geospatial",
    "section": "Subset",
    "text": "Subset\n\nnj_label_subset &lt;- nj_counties |&gt;\n  filter(COUNTY %in% c(\"MERCER\", \"SUSSEX\", \"SALEM\"))\n\nnj_counties |&gt;\n  ggplot() +\n  geom_sf(aes(fill = COUNTY)) +\n  geom_label(aes(x = X, y = Y, label = COUNTY),\n             data = nj_label_subset,\n             size = 2) +\n  labs(title = \"Counties of New Jersey\",\n       subtitle = \"categorical data\",\n       caption = \"SML 201\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/06_geospatial/06_geospatial.html#per-capita",
    "href": "posts/06_geospatial/06_geospatial.html#per-capita",
    "title": "6: Geospatial",
    "section": "Per Capita",
    "text": "Per Capita\n\nnj_df &lt;- nj_df |&gt;\n  left_join(nj_pop, by = c(\"COUNTY\" = \"county\"))\n\n\nnj_df &lt;- nj_df |&gt;\n  mutate(unemployed_per_cap = number_unemployed / population)\n\n\nnj_df |&gt;\n  ggplot() +\n  geom_sf(aes(fill = unemployed_per_cap)) +\n  labs(title = \"New Jersey Unemployment\",\n       subtitle = \"per capita\",\n       caption = \"SML 201\") +\n  scale_fill_distiller(palette = \"OrRd\",\n                       direction = 1) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/07_correlation/07_correlation.html",
    "href": "posts/07_correlation/07_correlation.html",
    "title": "7: Correlation",
    "section": "",
    "text": "library(\"corrplot\")\nlibrary(\"gt\")\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\ncorrelatedValues = function(x, r = 0.9){\n  r2 = r**2\n  ve = 1-r2\n  SD = sqrt(ve)\n  e  = rnorm(length(x), mean=0, sd=SD)\n  y  = r*x + e\n  return(y)\n}\njudging_categories &lt;- c(\"aroma\", \"flavor\", \"aftertaste\", \"acidity\", \"body\", \"balance\", \"uniformity\", \"clean_cup\", \"sweetness\")\n\nx1 = rnorm(100, mean = -6, sd = 1)\ny1 = correlatedValues(x1, r = 0.75) + 6\nx2 = rnorm(100, mean = -3, sd = 1)\ny2 = correlatedValues(x2, r = 0.75) + 3\nx3 = rnorm(100, mean = 0, sd = 1)\ny3 = correlatedValues(x3, r = 0.75)\nx4 = rnorm(100, mean = 3, sd = 1)\ny4 = correlatedValues(x4, r = 0.75) - 3\nx5 = rnorm(100, mean = 6, sd = 1)\ny5 = correlatedValues(x5, r = 0.75) - 6\n\ndf1 &lt;- data.frame(x1, y1, \"group 1\")\ndf2 &lt;- data.frame(x2, y2, \"group 2\")\ndf3 &lt;- data.frame(x3, y3, \"group 3\")\ndf4 &lt;- data.frame(x4, y4, \"group 4\")\ndf5 &lt;- data.frame(x5, y5, \"group 5\")\nnames(df1) &lt;- c(\"xdata\", \"ydata\", \"group\")\nnames(df2) &lt;- c(\"xdata\", \"ydata\", \"group\")\nnames(df3) &lt;- c(\"xdata\", \"ydata\", \"group\")\nnames(df4) &lt;- c(\"xdata\", \"ydata\", \"group\")\nnames(df5) &lt;- c(\"xdata\", \"ydata\", \"group\")\ndemo_df &lt;- rbind(df1, df2, df3, df4, df5)"
  },
  {
    "objectID": "posts/07_correlation/07_correlation.html#start",
    "href": "posts/07_correlation/07_correlation.html#start",
    "title": "7: Correlation",
    "section": "Start",
    "text": "Start\n\n\n\nGoal: Explore covariance\nObjective: Compute interquartile ranges and correlations\n\n\n\n\n\n\n\ncorrelation\n\n\n\nimage source: XKCD"
  },
  {
    "objectID": "posts/07_correlation/07_correlation.html#data",
    "href": "posts/07_correlation/07_correlation.html#data",
    "title": "7: Correlation",
    "section": "Data",
    "text": "Data\n\n\n\nCoffee Ratings\nsource: Coffee Quality Database\nhost: TidyTuesday — July 7, 2020\n\n\n\n\n\n# coffee_df &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-07/coffee_ratings.csv')\ncoffee_df &lt;- readr::read_csv(\"coffee_ratings.csv\")"
  },
  {
    "objectID": "posts/07_correlation/07_correlation.html#numerical-variables",
    "href": "posts/07_correlation/07_correlation.html#numerical-variables",
    "title": "7: Correlation",
    "section": "Numerical Variables",
    "text": "Numerical Variables\n\nstr(coffee_df, give.attr = FALSE)\n\nspc_tbl_ [1,339 × 43] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ total_cup_points     : num [1:1339] 90.6 89.9 89.8 89 88.8 ...\n $ species              : chr [1:1339] \"Arabica\" \"Arabica\" \"Arabica\" \"Arabica\" ...\n $ owner                : chr [1:1339] \"metad plc\" \"metad plc\" \"grounds for health admin\" \"yidnekachew dabessa\" ...\n $ country_of_origin    : chr [1:1339] \"Ethiopia\" \"Ethiopia\" \"Guatemala\" \"Ethiopia\" ...\n $ farm_name            : chr [1:1339] \"metad plc\" \"metad plc\" \"san marcos barrancas \\\"san cristobal cuch\" \"yidnekachew dabessa coffee plantation\" ...\n $ lot_number           : chr [1:1339] NA NA NA NA ...\n $ mill                 : chr [1:1339] \"metad plc\" \"metad plc\" NA \"wolensu\" ...\n $ ico_number           : chr [1:1339] \"2014/2015\" \"2014/2015\" NA NA ...\n $ company              : chr [1:1339] \"metad agricultural developmet plc\" \"metad agricultural developmet plc\" NA \"yidnekachew debessa coffee plantation\" ...\n $ altitude             : chr [1:1339] \"1950-2200\" \"1950-2200\" \"1600 - 1800 m\" \"1800-2200\" ...\n $ region               : chr [1:1339] \"guji-hambela\" \"guji-hambela\" NA \"oromia\" ...\n $ producer             : chr [1:1339] \"METAD PLC\" \"METAD PLC\" NA \"Yidnekachew Dabessa Coffee Plantation\" ...\n $ number_of_bags       : num [1:1339] 300 300 5 320 300 100 100 300 300 50 ...\n $ bag_weight           : chr [1:1339] \"60 kg\" \"60 kg\" \"1\" \"60 kg\" ...\n $ in_country_partner   : chr [1:1339] \"METAD Agricultural Development plc\" \"METAD Agricultural Development plc\" \"Specialty Coffee Association\" \"METAD Agricultural Development plc\" ...\n $ harvest_year         : chr [1:1339] \"2014\" \"2014\" NA \"2014\" ...\n $ grading_date         : chr [1:1339] \"April 4th, 2015\" \"April 4th, 2015\" \"May 31st, 2010\" \"March 26th, 2015\" ...\n $ owner_1              : chr [1:1339] \"metad plc\" \"metad plc\" \"Grounds for Health Admin\" \"Yidnekachew Dabessa\" ...\n $ variety              : chr [1:1339] NA \"Other\" \"Bourbon\" NA ...\n $ processing_method    : chr [1:1339] \"Washed / Wet\" \"Washed / Wet\" NA \"Natural / Dry\" ...\n $ aroma                : num [1:1339] 8.67 8.75 8.42 8.17 8.25 8.58 8.42 8.25 8.67 8.08 ...\n $ flavor               : num [1:1339] 8.83 8.67 8.5 8.58 8.5 8.42 8.5 8.33 8.67 8.58 ...\n $ aftertaste           : num [1:1339] 8.67 8.5 8.42 8.42 8.25 8.42 8.33 8.5 8.58 8.5 ...\n $ acidity              : num [1:1339] 8.75 8.58 8.42 8.42 8.5 8.5 8.5 8.42 8.42 8.5 ...\n $ body                 : num [1:1339] 8.5 8.42 8.33 8.5 8.42 8.25 8.25 8.33 8.33 7.67 ...\n $ balance              : num [1:1339] 8.42 8.42 8.42 8.25 8.33 8.33 8.25 8.5 8.42 8.42 ...\n $ uniformity           : num [1:1339] 10 10 10 10 10 10 10 10 9.33 10 ...\n $ clean_cup            : num [1:1339] 10 10 10 10 10 10 10 10 10 10 ...\n $ sweetness            : num [1:1339] 10 10 10 10 10 10 10 9.33 9.33 10 ...\n $ cupper_points        : num [1:1339] 8.75 8.58 9.25 8.67 8.58 8.33 8.5 9 8.67 8.5 ...\n $ moisture             : num [1:1339] 0.12 0.12 0 0.11 0.12 0.11 0.11 0.03 0.03 0.1 ...\n $ category_one_defects : num [1:1339] 0 0 0 0 0 0 0 0 0 0 ...\n $ quakers              : num [1:1339] 0 0 0 0 0 0 0 0 0 0 ...\n $ color                : chr [1:1339] \"Green\" \"Green\" NA \"Green\" ...\n $ category_two_defects : num [1:1339] 0 1 0 2 2 1 0 0 0 4 ...\n $ expiration           : chr [1:1339] \"April 3rd, 2016\" \"April 3rd, 2016\" \"May 31st, 2011\" \"March 25th, 2016\" ...\n $ certification_body   : chr [1:1339] \"METAD Agricultural Development plc\" \"METAD Agricultural Development plc\" \"Specialty Coffee Association\" \"METAD Agricultural Development plc\" ...\n $ certification_address: chr [1:1339] \"309fcf77415a3661ae83e027f7e5f05dad786e44\" \"309fcf77415a3661ae83e027f7e5f05dad786e44\" \"36d0d00a3724338ba7937c52a378d085f2172daa\" \"309fcf77415a3661ae83e027f7e5f05dad786e44\" ...\n $ certification_contact: chr [1:1339] \"19fef5a731de2db57d16da10287413f5f99bc2dd\" \"19fef5a731de2db57d16da10287413f5f99bc2dd\" \"0878a7d4b9d35ddbf0fe2ce69a2062cceb45a660\" \"19fef5a731de2db57d16da10287413f5f99bc2dd\" ...\n $ unit_of_measurement  : chr [1:1339] \"m\" \"m\" \"m\" \"m\" ...\n $ altitude_low_meters  : num [1:1339] 1950 1950 1600 1800 1950 ...\n $ altitude_high_meters : num [1:1339] 2200 2200 1800 2200 2200 NA NA 1700 1700 1850 ...\n $ altitude_mean_meters : num [1:1339] 2075 2075 1700 2000 2075 ...\n\n\nRecall that we can use summary on a numerical variable.\n\nsummary(coffee_df$total_cup_points)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   81.08   82.50   82.09   83.67   90.58 \n\n\nThe dplyr way to compute quantiles includes\n\ncoffee_df |&gt;\n  summarize(min = min(total_cup_points, na.rm = TRUE),\n            q25 = quantile(total_cup_points, 0.25, na.rm = TRUE),\n            q50 = quantile(total_cup_points, 0.50, na.rm = TRUE),\n            q75 = quantile(total_cup_points, 0.75, na.rm = TRUE),\n            max = max(total_cup_points, na.rm = TRUE))\n\n# A tibble: 1 × 5\n    min   q25   q50   q75   max\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0  81.1  82.5  83.7  90.6\n\n\nWe can verify that about 50% of the data are below the median value\n\nmean(coffee_df$total_cup_points &lt; \n       median(coffee_df$total_cup_points))\n\n[1] 0.4899178\n\n\nWe can verify that about 75% of the data are indeed below that value for the 0.75 quantile (i.e. 75th percentile.)\n\nmean(coffee_df$total_cup_points &lt; 83.67)\n\n[1] 0.7490665\n\n\nThe interquartile range is the 75th percentile minus the 25th percentile.\n\nsummary(coffee_df$total_cup_points)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   81.08   82.50   82.09   83.67   90.58 \n\n\n\nIQR(coffee_df$total_cup_points, na.rm = TRUE)\n\n[1] 2.59"
  },
  {
    "objectID": "posts/07_correlation/07_correlation.html#categorical-group",
    "href": "posts/07_correlation/07_correlation.html#categorical-group",
    "title": "7: Correlation",
    "section": "Categorical Group",
    "text": "Categorical Group\nThe dplyr code is easily adaptable to grouped data.\n\ncoffee_df |&gt;\n  group_by(species) |&gt;\n  summarize(min = min(total_cup_points, na.rm = TRUE),\n            q25 = quantile(total_cup_points, 0.25, na.rm = TRUE),\n            q50 = quantile(total_cup_points, 0.50, na.rm = TRUE),\n            q75 = quantile(total_cup_points, 0.75, na.rm = TRUE),\n            max = max(total_cup_points, na.rm = TRUE))\n\n# A tibble: 2 × 6\n  species   min   q25   q50   q75   max\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Arabica   0    81.2  82.5  83.7  90.6\n2 Robusta  73.8  80.2  81.5  82.5  83.8"
  },
  {
    "objectID": "posts/07_correlation/07_correlation.html#another-example",
    "href": "posts/07_correlation/07_correlation.html#another-example",
    "title": "7: Correlation",
    "section": "Another Example",
    "text": "Another Example\n\n12Boxplot\n\n\n\ncoffee_df |&gt;\n  filter(!is.na(processing_method)) |&gt;\n  group_by(processing_method) |&gt;\n  summarize(min = min(total_cup_points, na.rm = TRUE),\n            q25 = quantile(total_cup_points, 0.25, na.rm = TRUE),\n            q50 = quantile(total_cup_points, 0.50, na.rm = TRUE),\n            q75 = quantile(total_cup_points, 0.75, na.rm = TRUE),\n            max = max(total_cup_points, na.rm = TRUE))\n\n# A tibble: 5 × 6\n  processing_method           min   q25   q50   q75   max\n  &lt;chr&gt;                     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Natural / Dry              67.9  81.2  82.8  83.8  89  \n2 Other                      63.1  80.8  81.8  83.0  84.7\n3 Pulped natural / honey     80.1  82.0  82.7  83.2  86.6\n4 Semi-washed / Semi-pulped  78.8  81.5  82.5  83.6  86.1\n5 Washed / Wet               59.8  81    82.4  83.5  90.6\n\n\n\n\n\ncoffee_df |&gt;\n  filter(!is.na(processing_method)) |&gt;\n  group_by(processing_method) |&gt;\n  mutate(min_val = min(total_cup_points, na.rm = TRUE),\n         q25 = quantile(total_cup_points, 0.25, na.rm = TRUE),\n         q50 = quantile(total_cup_points, 0.50, na.rm = TRUE),\n         q75 = quantile(total_cup_points, 0.75, na.rm = TRUE),\n         max_val = max(total_cup_points, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(processing_method, min_val, q25, q50, q75, max_val) |&gt;\n  distinct() |&gt;\n  rev() |&gt; #reverse order of columns\n  t() |&gt;   #transpose (switch rows and columns)\n  data.frame() |&gt;\n  slice(1:5) |&gt;\n  set_names(c(\"washed\", \"natural\", \"pulped\", \"semi\", \"other\"))\n\n         washed natural  pulped    semi   other\nmax_val   90.58   89.00   86.58   86.08   84.67\nq75     83.5000 83.8300 83.2075 83.6425 82.9575\nq50      82.420  82.750  82.665  82.500  81.830\nq25     81.0000 81.2500 81.9550 81.5000 80.7925\nmin_val   59.83   67.92   80.08   78.75   63.08\n\n\n\n\n\ncoffee_df |&gt;\n  filter(!is.na(processing_method)) |&gt;\n  ggplot() +\n  geom_boxplot(aes(x = processing_method, y = total_cup_points,\n                   color = processing_method)) +\n  labs(title = \"Coffee Ratings\",\n       subtitle = \"Are these quantities different?\",\n       caption = \"Source: Coffee Quality Database\",\n       x = \"\", y = \"total points\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45),\n        legend.position = \"none\")"
  },
  {
    "objectID": "posts/07_correlation/07_correlation.html#covariance",
    "href": "posts/07_correlation/07_correlation.html#covariance",
    "title": "7: Correlation",
    "section": "Covariance",
    "text": "Covariance\n\nFormulaIntuitionCommentary\n\n\nFor data \\((X,Y)\\) listed as \\(n\\) data points \\((x_{i}, y_{i})\\), the covariance is defined as\n\\[\\begin{array}{rcl}\n  \\text{Cov}(X,Y) & = & \\frac{1}{2n^{2}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}(x_{i} - x_{j})(y_{i} - y_{j}) \\\\\n  ~ & = & \\text{E}[X] \\cdot \\text{E}[Y] - \\text{E}[XY] \\\\\n  \\end{array}\\]\n\n\n\n\n\nconstructive or destructive waves\n\n\n\nimage source: Fissics\n\n\n\n\nAre resultant numbers large or small?\nUnits? (e.g. “burger-fries”)"
  },
  {
    "objectID": "posts/07_correlation/07_correlation.html#standardization",
    "href": "posts/07_correlation/07_correlation.html#standardization",
    "title": "7: Correlation",
    "section": "Standardization",
    "text": "Standardization\n\n\n\nz-score\n\\[\\begin{array}{ccc}\nz & = & \\frac{x - \\bar{x}}{s} \\\\\n~ & = & \\frac{\\text{deviation}}{\\text{standard deviation}} \\\\\n\\end{array}\\]\n\n\n\n\n\nCorrelation\n\\[\\begin{array}{ccc}\n  r & = & \\frac{\\sum_{i=1}^{n} (x_{i} - \\bar{x})(y_{i} - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_{i} - \\bar{x})}\\sqrt{\\sum_{i=1}^{n} (y_{i} - \\bar{y})}} \\\\\n  ~ & = & \\frac{\\text{Cov}(X,Y)}{\\text{SD}(X) \\cdot \\text{SD}(Y)} \\\\\n  ~ & = & \\frac{1}{n-1}\\sum_{i=1}^{n}\\left(\\frac{x_{i}-\\bar{x}}{s_{x}}\\right)\\left(\\frac{y_{i}-\\bar{y}}{s_{y}}\\right) \\\\\n\\end{array}\\]\n\n\n\nClaim: The correlation coefficient \\(r\\) has a mathematical range in \\([-1,1]\\):\n\\[-1 \\leq r \\leq 1\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n[refer to a Calculus-based Probability course]"
  },
  {
    "objectID": "posts/07_correlation/07_correlation.html#demonstration",
    "href": "posts/07_correlation/07_correlation.html#demonstration",
    "title": "7: Correlation",
    "section": "Demonstration",
    "text": "Demonstration\n\n12345"
  },
  {
    "objectID": "posts/07_correlation/07_correlation.html#examples",
    "href": "posts/07_correlation/07_correlation.html#examples",
    "title": "7: Correlation",
    "section": "Examples",
    "text": "Examples\nCompute the correlation between flavor and aftertaste\n\ncor(coffee_df$flavor, coffee_df$aftertaste)\n\n[1] 0.8956718\n\n\nCompute the correlation between uniformity and clean_cup\n\ncor(coffee_df$uniformity, coffee_df$clean_cup,\n    use = \"pairwise.complete.obs\")\n\n[1] 0.5262187\n\n\nCompute the correlation between aroma and sweetness\n\ncoffee_df |&gt;\n  summarize(r = cor(aroma, sweetness,\n                    use = \"pairwise.complete.obs\"))\n\n# A tibble: 1 × 1\n      r\n  &lt;dbl&gt;\n1 0.253"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html",
    "href": "posts/08_linear_regression/08_linear_regression.html",
    "title": "8: Linear Regression",
    "section": "",
    "text": "library(\"gt\")        #great tables\nlibrary(\"HistData\")  #historical data sets\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\nliquor_df &lt;- data.frame(\n  year = 2014:2022,\n  LLV = c(129, 54, 103, 50, 15, 10, 18, 49,57)\n)\nMM_df &lt;- data.frame(\n  S = c(0.08, 0.12, 0.54, 1.23, 1.82, 2.72, 4.94, 10.00),\n  v = c(0.15, 0.21, 0.7, 1.1, 1.3, 1.5, 1.7, 1.8)\n)"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#start",
    "href": "posts/08_linear_regression/08_linear_regression.html#start",
    "title": "8: Linear Regression",
    "section": "Start",
    "text": "Start\n\n\n\nGoal: Make predictions\nObjective: Perform linear regression and compute coefficients of determination\n\n\n\n\n\n\n\nlinear regression\n\n\n\nimage source: XKCD"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#linear-regression-in-r",
    "href": "posts/08_linear_regression/08_linear_regression.html#linear-regression-in-r",
    "title": "8: Linear Regression",
    "section": "Linear Regression in R",
    "text": "Linear Regression in R\n\nresponse variable (y): LLV\nexplanatory variable (x): year\n\n\nlin_fit &lt;- lm(LLV ~ year, data = liquor_df)\n\nIn a model equation, the tilde ~ is read as “explained by”. In this model, we can say that the response variable LLV is explained by the year."
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#prediction-in-r",
    "href": "posts/08_linear_regression/08_linear_regression.html#prediction-in-r",
    "title": "8: Linear Regression",
    "section": "Prediction in R",
    "text": "Prediction in R\nWe use the predict function where the input is a data frame. In this example, we are predicting the number of judicial referrals for liquor law violations in the year 2023.\n\nyhat &lt;- predict(lin_fit,\n                newdata = data.frame(year = 2023))"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#validation",
    "href": "posts/08_linear_regression/08_linear_regression.html#validation",
    "title": "8: Linear Regression",
    "section": "Validation",
    "text": "Validation\nIn this simple example, we know the true answer: there were 262 judicial referrals for liquor law violations in the year 2023\n\n# true value\ny &lt;- 262\n\n\n# absolute error\nabs(y - yhat)\n\n       1 \n250.8611 \n\n\n\n# relative error\nabs(y - yhat) / y\n\n        1 \n0.9574852 \n\n\nWhy was the prediction so inaccurate?"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#scatterplot",
    "href": "posts/08_linear_regression/08_linear_regression.html#scatterplot",
    "title": "8: Linear Regression",
    "section": "Scatterplot",
    "text": "Scatterplot\nIt is a good idea to look at the data (when practical).\n\nliquor_df |&gt;\n  ggplot() +\n  geom_point(aes(x = factor(year), y = LLV),\n             size = 4, color = \"black\") +\n  labs(title = \"Liquor Law Violations\",\n       subtitle = \"Princeton University, main campus\",\n       caption = \"SML 201\",\n       x = \"year\", y = \"judicial referrals\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#smooth",
    "href": "posts/08_linear_regression/08_linear_regression.html#smooth",
    "title": "8: Linear Regression",
    "section": "Smooth",
    "text": "Smooth\nOn the visual size, linear regression summarizes a scatterplot.\n\nliquor_df |&gt;\n  ggplot(aes(x = year, y = LLV)) +\n  geom_point(size = 4, color = \"black\") +\n  geom_smooth(formula = \"y ~ x\", method = \"lm\", se = FALSE,\n              color = \"blue\", linewidth = 2) +\n  labs(title = \"Liquor Law Violations\",\n       subtitle = \"Princeton University, main campus\",\n       caption = \"SML 201\",\n       x = \"year\", y = \"judicial referrals\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#centroid",
    "href": "posts/08_linear_regression/08_linear_regression.html#centroid",
    "title": "8: Linear Regression",
    "section": "Centroid",
    "text": "Centroid\nClaim: a linear regression model goes through the centroid\n\\[(\\bar{x}, \\bar{y})\\]\n\nxbar &lt;- mean(liquor_df$year)\nybar &lt;- mean(liquor_df$LLV)\n\nliquor_df |&gt;\n  ggplot(aes(x = year, y = LLV)) +\n  geom_point(size = 4, color = \"black\") +\n  geom_vline(xintercept = xbar, color = \"green\") +\n  geom_hline(yintercept = ybar, color = \"green\") +\n  geom_smooth(formula = \"y ~ x\", method = \"lm\", se = FALSE,\n              color = \"blue\", linewidth = 2) +\n  labs(title = \"Linear Regression\",\n       subtitle = \"A linear regression model goes through the centroid \",\n       caption = \"SML 201\",\n       x = \"year\", y = \"judicial referrals\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#vector-space",
    "href": "posts/08_linear_regression/08_linear_regression.html#vector-space",
    "title": "8: Linear Regression",
    "section": "Vector Space",
    "text": "Vector Space\nHow was the line drawn?\n\n\n\nWhere to draw the line?"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#linear-model",
    "href": "posts/08_linear_regression/08_linear_regression.html#linear-model",
    "title": "8: Linear Regression",
    "section": "Linear Model",
    "text": "Linear Model\n\\[\\hat{y} = a + bx\\]\n\n\\(\\hat{y}\\): predicted value\n\\(a\\): intercept\n\\(b\\): slope"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#residuals",
    "href": "posts/08_linear_regression/08_linear_regression.html#residuals",
    "title": "8: Linear Regression",
    "section": "Residuals",
    "text": "Residuals\nA residual is the difference between a predicted value and its true value.\n\nliquor_df &lt;- liquor_df |&gt;\n  mutate(predictions = predict(lin_fit,\n                               newdata = data.frame(year = liquor_df$year)),\n         residuals = predictions - LLV)\n\n\nliquor_df |&gt;\n  ggplot(aes(x = year, y = LLV)) +\n  geom_segment(aes(x = year, y = predictions, \n                   xend = year, yend = LLV), \n               color = \"purple\", linewidth = 3) +\n  geom_point(size = 4, color = \"black\") +\n  geom_smooth(formula = \"y ~ x\", method = \"lm\", se = FALSE,\n              color = \"blue\", linewidth = 2) +\n  geom_point(aes(x = year, y = predictions),\n             color = \"red\", size = 4) +\n  labs(title = \"Linear Regression\",\n       subtitle = \"black: true values\\nred: predictions\\npurple: residuals\",\n       caption = \"SML 201\",\n       x = \"year\", y = \"judicial referrals\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#corollary-residual-balance",
    "href": "posts/08_linear_regression/08_linear_regression.html#corollary-residual-balance",
    "title": "8: Linear Regression",
    "section": "Corollary: Residual Balance",
    "text": "Corollary: Residual Balance\nClaim: the average of the residuals is zero\n\nmean(liquor_df$residuals)\n\n[1] 1.616879e-12"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#method-of-least-squares",
    "href": "posts/08_linear_regression/08_linear_regression.html#method-of-least-squares",
    "title": "8: Linear Regression",
    "section": "Method of Least Squares",
    "text": "Method of Least Squares\nIdea: The best-fit line is where the sum-of-squared residuals is minimized.\n\\[E(a,b) = \\sum_{i=1}^{n} (y_{i} - a - bx_{i})^{2}\\]\nClaim: \\[a = \\frac{ (\\sum y_{i})(\\sum x_{i}^{2}) - (\\sum x_{i})(\\sum x_{i}y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }, \\quad b = \\frac{ n\\sum x_{i}y_{i} - (\\sum x_{i})(\\sum y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }\\]\n\n\n\n\n\n\n(optional) Proof\n\n\n\n\n\nSearch for a critical point by setting the partial derivatives (along with the Chain Rule) equal to zero.\n\\[0 = \\frac{\\partial E}{\\partial a} = -2\\sum_{i = 1}^{n} (y_{i} - a - bx_{i}) = 2an + 2b\\sum_{i = 1}^{n}x_{i} - 2\\sum_{i = 1}^{n} y_{i}\\] \\[0 = \\frac{\\partial E}{\\partial b} = -2\\sum_{i = 1}^{n} (y_{i} - a - bx_{i})x_{i} = 2a\\sum_{i = 1}^{n}x_{i} + 2b\\sum_{i = 1}^{n}x_{i}^{2} - 2\\sum_{i = 1}^{n} x_{i}y_{i}\\]\nCreate a matrix system of equations.\n\\[\\left[  \\begin{array}{cc}\n  n & \\sum_{i = 1}^{n}x_{i} \\\\\n  \\sum_{i = 1}^{n}x_{i} & \\sum_{i = 1}^{n}x_{i}^{2} \\\\\n  \\end{array}\\right]\n  \\left[  \\begin{array}{c}  a \\\\ b \\end{array}\\right]\n  =\n  \\left[  \\begin{array}{c}  \\sum_{i = 1}^{n} y_{i} \\\\ \\sum_{i = 1}^{n} x_{i}y_{i} \\end{array}\\right]\n  \\]\nEmploy a matrix inverse.\n$$\n\\[\\begin{array}{rcl}\n  \\left[  \\begin{array}{c}  a \\\\ b \\end{array}\\right] & = &\n  \\left[  \\begin{array}{cc}\n  n & \\sum_{i = 1}^{n}x_{i} \\\\\n  \\sum_{i = 1}^{n}x_{i} & \\sum_{i = 1}^{n}x_{i}^{2} \\\\\n  \\end{array}\\right]^{-1}\\left[  \\begin{array}{c}  \\sum_{i = 1}^{n} y_{i} \\\\ \\sum_{i = 1}^{n} x_{i}y_{i} \\end{array}\\right] \\\\\n  \n  ~ & ~ & ~ \\\\\n  \n  \\left[  \\begin{array}{c}  a \\\\ b \\end{array}\\right] & = & \\frac{1}{n\\sum x_{i}^{2} - (\\sum x_{i})^{2}} \\left[  \\begin{array}{cc}\n  \\sum_{i = 1}^{n}x_{i}^{2} & -\\sum_{i = 1}^{n}x_{i} \\\\\n  -\\sum_{i = 1}^{n}x_{i} & n \\\\\n  \\end{array}\\right]  \\left[  \\begin{array}{c}  \\sum_{i = 1}^{n} y_{i} \\\\ \\sum_{i = 1}^{n} x_{i}y_{i} \\end{array}\\right] \\\\\n  \n  ~ & ~ & ~ \\\\\n  \n  \\left[  \\begin{array}{c}  a \\\\ b \\end{array}\\right] & = & \\frac{1}{n\\sum x_{i}^{2} - (\\sum x_{i})^{2}}\n  \\left[  \\begin{array}{c}  (\\sum y_{i})(\\sum x_{i}^{2}) - (\\sum x_{i})(\\sum x_{i}y_{i}) \\\\  n\\sum x_{i}y_{i} - (\\sum x_{i})(\\sum y_{i}) \\end{array}\\right] \\\\\n\\end{array}\\]\n$$"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#lm",
    "href": "posts/08_linear_regression/08_linear_regression.html#lm",
    "title": "8: Linear Regression",
    "section": "LM",
    "text": "LM\n\\[\\hat{y} = a + bx\\]\n\nlm(LLV ~ year, data = liquor_df)\n\n\nCall:\nlm(formula = LLV ~ year, data = liquor_df)\n\nCoefficients:\n(Intercept)         year  \n   17307.79        -8.55  \n\n\nFor every increase in year, the number of judicial referrals decreases by 8.55.\n\nPrediction\nPredict the number of judicial referrals for liquor law violations in the year 2023.\n\na &lt;- summary(lin_fit)$coefficients[1]\nb &lt;- summary(lin_fit)$coefficients[2]\n\na + b*(2023)\n\n[1] 11.13889\n\npredict(lin_fit, newdata = data.frame(year = 2023))\n\n       1 \n11.13889"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#definition",
    "href": "posts/08_linear_regression/08_linear_regression.html#definition",
    "title": "8: Linear Regression",
    "section": "Definition",
    "text": "Definition\nThe coefficient of determination is defined as\n\\[R^{2} = \\frac{\\text{explained variance}}{\\text{total variance}}\\]\nFor analysis of variance:\n\nexplained variation: \\(\\sum(\\hat{y} - \\bar{y})^{2}\\)\nunexplained variation: \\(\\sum(y - \\hat{y})^{2}\\)\ntotal variation = explained variation + unexplained variation \\[\\sum(y - \\bar{y})^{2} = \\sum(\\hat{y} - \\bar{y})^{2} + \\sum(y - \\hat{y})^{2}\\]"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#inference",
    "href": "posts/08_linear_regression/08_linear_regression.html#inference",
    "title": "8: Linear Regression",
    "section": "Inference",
    "text": "Inference\nWhy is that denoted “\\(R^2\\)”? For linear regression, the coefficient of determination is literally the square of the correlation coefficient (\\(r\\))\n\ncorrelation \\(-1 \\leq r \\leq 1\\) implies coefficient of determination \\[0 \\leq R^{2} \\leq 1\\]\nwant more “explained variation”, thus higher \\(R^{2}\\) means a better model"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#guidelines",
    "href": "posts/08_linear_regression/08_linear_regression.html#guidelines",
    "title": "8: Linear Regression",
    "section": "Guidelines",
    "text": "Guidelines\n\n\nIn this course, we will simply follow the Pearson suggestions for interpreting coefficient of determination values:\n\n\\(0 \\leq R^{2} &lt; 0.4\\): poor model\n\\(0.4 \\leq R^{2} &lt; 0.7\\): good model\n\\(0.7 \\leq R^{2} \\leq 1.0\\): great model\n\n\n\n\n\n\n\nKarl Pearson"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#model-statistics",
    "href": "posts/08_linear_regression/08_linear_regression.html#model-statistics",
    "title": "8: Linear Regression",
    "section": "Model Statistics",
    "text": "Model Statistics\nIn R, we can use summary to access model statistics.\n\nsummary(lin_fit)\n\n\nCall:\nlm(formula = LLV ~ year, data = liquor_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-38.89 -25.54 -12.44  32.01  40.91 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 17307.789   9047.707   1.913   0.0973 .\nyear           -8.550      4.483  -1.907   0.0982 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 34.73 on 7 degrees of freedom\nMultiple R-squared:  0.3419,    Adjusted R-squared:  0.2479 \nF-statistic: 3.637 on 1 and 7 DF,  p-value: 0.09819\n\n\nHere in SML 201, we will use the “Adjusted R-squared” value that accounts for the number of predictor variables. Treat negative adjusted R-squared values simply as zero variation explained, and then look for the highest adjusted R-squared values.\n\nsummary(lin_fit)$adj.r.squared\n\n[1] 0.2478815"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#francis-galton",
    "href": "posts/08_linear_regression/08_linear_regression.html#francis-galton",
    "title": "8: Linear Regression",
    "section": "Francis Galton",
    "text": "Francis Galton\n\n\n\n\n\nSir Francis Galton\n\n\n\n\n\n\n1822 - 1911\ncousin of Charles Darwin\ncorrelation discovery\n\n1846: August Bravais\n1888: Francis Galton"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#do-tall-parents-have-tall-children",
    "href": "posts/08_linear_regression/08_linear_regression.html#do-tall-parents-have-tall-children",
    "title": "8: Linear Regression",
    "section": "Do tall parents have tall children?",
    "text": "Do tall parents have tall children?\nThe Galton data set in the HistData package has two variables\n\nparents’ height (see documentation for weighted formula)\nchild’s height\n\nfor about 200 families\n\nheredity_df &lt;- HistData::Galton"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#scatterplot-1",
    "href": "posts/08_linear_regression/08_linear_regression.html#scatterplot-1",
    "title": "8: Linear Regression",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nheredity_df |&gt;\n  ggplot(aes(x = parent, y = child)) +\n  geom_point() +\n  geom_smooth(formula = \"y ~ x\", method = \"lm\", se = FALSE) +\n  labs(title = \"Do tall parents have tall children?\",\n       subtitle = \"Galton Survey of Heights\",\n       caption = \"SML 201\",\n       x = \"parent's heights (weighted average)\",\n       y = \"child's height\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#linear-model-1",
    "href": "posts/08_linear_regression/08_linear_regression.html#linear-model-1",
    "title": "8: Linear Regression",
    "section": "Linear Model",
    "text": "Linear Model\n\nlin_fit &lt;- lm(child ~ parent, data = heredity_df)\n\n# slope\nsummary(lin_fit)$coefficients[2]\n\n[1] 0.6462906\n\n\nFor every one inch increase in parents’ height, the child’s height increases by about 0.65 inches."
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#prediction-1",
    "href": "posts/08_linear_regression/08_linear_regression.html#prediction-1",
    "title": "8: Linear Regression",
    "section": "Prediction",
    "text": "Prediction\nIf the parents are 69 inches in height, what do we predict for the height of the child?\n\npredict(lin_fit, newdata = data.frame(parent = 69))\n\n       1 \n68.53558 \n\n\nIf the parents are 58 inches in height, what do we predict for the height of the child?\n\npredict(lin_fit, newdata = data.frame(parent = 58))\n\n       1 \n61.42638"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#regression-to-the-mean",
    "href": "posts/08_linear_regression/08_linear_regression.html#regression-to-the-mean",
    "title": "8: Linear Regression",
    "section": "Regression to the Mean",
    "text": "Regression to the Mean\nIn these early studies of heredity, Galton coined the phrase\n\\[\\text{regression to the mean}\\]\nand similar calculations have been called regression ever since."
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#michaelis-and-menton",
    "href": "posts/08_linear_regression/08_linear_regression.html#michaelis-and-menton",
    "title": "8: Linear Regression",
    "section": "Michaelis and Menton",
    "text": "Michaelis and Menton\n\n\n\n\n\nLeonor Michaelis and Maud Menton\n\n\n\n\n\nLeonor Michaelis\n\nBerlin University (1897)\n\nMaud Menton\n\nUniversity of Toronto (1911)\n\nDie Kinetik der Invertinwirkung (1913)\n\\[v = \\frac{V_{\\text{max}}[S]}{K_{m} + [S]}\\]\n\n\\(v\\): reaction rate (micromolars per minute)\n\\([S]\\): substrate concentration (micromolars)"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#enzyme-kinetics",
    "href": "posts/08_linear_regression/08_linear_regression.html#enzyme-kinetics",
    "title": "8: Linear Regression",
    "section": "Enzyme Kinetics",
    "text": "Enzyme Kinetics\n\n\n\n\n\nEssential Cell Biology\n\n\n\n\n\nThe reaction rates of the reaction S \\(\\rightarrow\\) P catalyzed by enzyme E were determined under conditions such that only very little product was formed. Compute the maximum reaction velocity asymptote \\(V_{\\text{max}}\\) and the Michaelis-Menton constant \\(K_{m}\\)\n\n\n\n\n\n\n\n\nMichaelis Menton Experiment\n\n\nreaction rate vs substrate concentration\n\n\nS\nv\n\n\n\n\n0.08\n0.15\n\n\n0.12\n0.21\n\n\n0.54\n0.70\n\n\n1.23\n1.10\n\n\n1.82\n1.30\n\n\n2.72\n1.50\n\n\n4.94\n1.70\n\n\n10.00\n1.80\n\n\n\nDie Kinetik der Invertinwirkung"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#scatterplot-2",
    "href": "posts/08_linear_regression/08_linear_regression.html#scatterplot-2",
    "title": "8: Linear Regression",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nreaction rate (lower case v)\nsubstrate concentration (capital S)\n\n\nMM_df |&gt;\n  ggplot(aes(x = S, y = v)) +\n  geom_smooth(formula = \"y ~ x\", method = \"loess\", se = TRUE) +\n  geom_point(color = \"black\", size = 4) +\n  labs(title = \"Michaelis Menton Enzyme Kinetics Experiment\",\n       subtitle = \"reaction rate vs substrate concentration\",\n       caption = \"Essential Cell Biology\",\n       x = \"substrate concentration (micromolars)\",\n       y = \"reaction rate (micromolars per minute)\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#transformation",
    "href": "posts/08_linear_regression/08_linear_regression.html#transformation",
    "title": "8: Linear Regression",
    "section": "Transformation",
    "text": "Transformation\nHans Lineweaver (George Washington Univ., 1934)\n\\[v = \\frac{V_{\\text{max}}[S]}{K_{m} + [S]} \\quad\\rightarrow\\quad \\frac{1}{v} = \\frac{K_{m}}{V_{\\text{max}}} \\cdot \\frac{1}{[S]} + \\frac{1}{V_{\\text{max}}}\\]\nThe reciprocal of the reaction rate is linear with respect to the reciprocal of the substrate concentration."
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#double-reciprocal-plot",
    "href": "posts/08_linear_regression/08_linear_regression.html#double-reciprocal-plot",
    "title": "8: Linear Regression",
    "section": "Double-Reciprocal Plot",
    "text": "Double-Reciprocal Plot\n\nMM_df &lt;- MM_df |&gt;\n  mutate(rS = 1/S,\n         rv = 1/v)\n\n\nMM_df |&gt;\n  ggplot(aes(x = rS, y = rv)) +\n  geom_smooth(formula = \"y ~ x\", method = \"lm\", se = FALSE) +\n  geom_point(color = \"black\", size = 4) +\n  labs(title = \"Michaelis Menton Enzyme Kinetics Experiment\",\n       subtitle = \"Double Reciprocal Plot\",\n       caption = \"Essential Cell Biology\",\n       x = \"1/[S] (1/micromoles)\",\n       y = \"1/v (minutes per micromolar)\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#linear-model-2",
    "href": "posts/08_linear_regression/08_linear_regression.html#linear-model-2",
    "title": "8: Linear Regression",
    "section": "Linear Model",
    "text": "Linear Model\n\nlin_fit &lt;- lm(rv ~ rS, data = MM_df)"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#coefficient-of-determination-1",
    "href": "posts/08_linear_regression/08_linear_regression.html#coefficient-of-determination-1",
    "title": "8: Linear Regression",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\n\nsummary(lin_fit)$adj.r.squared\n\n[1] 0.9995051"
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#extracting-the-constants",
    "href": "posts/08_linear_regression/08_linear_regression.html#extracting-the-constants",
    "title": "8: Linear Regression",
    "section": "Extracting the Constants",
    "text": "Extracting the Constants\n\nslope &lt;- lin_fit$coefficients[2]\nintercept &lt;- lin_fit$coefficients[1]\n\nVmax &lt;- 1/intercept\nKm &lt;- slope * Vmax\n\nprint(paste(\"The Vmax asymptote is \", Vmax))\n\n[1] \"The Vmax asymptote is  1.9894653306508\"\n\nprint(paste(\"The Michaelis-Menton constant is \", Km))\n\n[1] \"The Michaelis-Menton constant is  0.991986524114476\""
  },
  {
    "objectID": "posts/08_linear_regression/08_linear_regression.html#mm-experiment-revisited",
    "href": "posts/08_linear_regression/08_linear_regression.html#mm-experiment-revisited",
    "title": "8: Linear Regression",
    "section": "MM Experiment Revisited",
    "text": "MM Experiment Revisited\n\nMM_df |&gt;\n  ggplot(aes(x = S, y = v)) +\n  geom_smooth(formula = \"y ~ x\", method = \"loess\", se = TRUE) +\n  geom_point(color = \"black\", size = 4) +\n  geom_abline(slope = Km, intercept = 0, color = \"red\") +\n  geom_abline(slope = 0, intercept = Vmax, color = \"purple\") +\n  labs(title = \"Michaelis Menton Enzyme Kinetics Experiment\",\n       subtitle = \"maximum reaction velocity asymptote (purple)\\nMichaelis-Menton constant (red slope)\",\n       caption = \"Essential Cell Biology\",\n       x = \"substrate concentration (micromolars)\",\n       y = \"reaction rate (micromolars per minute)\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/09_multiple_regression/09_multiple_linear.html",
    "href": "posts/09_multiple_regression/09_multiple_linear.html",
    "title": "9: Multiple Linear Regression",
    "section": "",
    "text": "library(\"corrplot\")  #visualize correlations simultaneously\nlibrary(\"gt\")        #great tables\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\noakland_green &lt;- \"#003831\"\noakland_yellow &lt;- \"#EFB21E\"\n\n# user-defined function\ncor2text &lt;- function(x,y, num_digits = 4){\n  # This function will compute a correlation, round the result, and describe the results\n  # INPUTS:\n  ## x: numerical vector\n  ## y: numerical vector\n  ## num_digits: number of digits for rounding (default: 4)\n  # OUTPUT: string\n  \n  r = cor(x,y, use = \"pairwise.complete.obs\")\n  \n  cor_des &lt;- case_when(\n    r &gt;= 0.7 ~ \"strongly and positively correlated\",\n    r &gt;= 0.4 & r &lt; 0.7 ~ \"slightly and positively correlated\",\n    r &lt;= -0.4 & r &gt; -0.7 ~ \"slightly and negatively correlated\",\n    r &lt;= -0.7 ~ \"strongly and negatively correlated\",\n    .default = \"virtually uncorrelated\"\n  )\n  \n  #return\n  paste0(\"r = \", round(r, num_digits),\n         \", \", cor_des)\n}\nbb_df &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/data/baseball_data_90s.csv\")\n\noffense_cats &lt;- c(\"R\", \"H\", \"X2B\", \"X3B\", \"HR\", \"BB\", \"SO\", \"SB\")\ndefense_cats &lt;- c(\"RA\", \"ER\", \"HA\", \"HRA\", \"BBA\", \"SOA\", \"E\", \"FP\")"
  },
  {
    "objectID": "posts/09_multiple_regression/09_multiple_linear.html#start",
    "href": "posts/09_multiple_regression/09_multiple_linear.html#start",
    "title": "9: Multiple Linear Regression",
    "section": "Start",
    "text": "Start\n\n\n\nGoal: Expand to larger regression models\nObjective: Include multiple linear terms and an interaction term\n\n\n\n\n\n\n\nMoneyball (2011)"
  },
  {
    "objectID": "posts/09_multiple_regression/09_multiple_linear.html#story",
    "href": "posts/09_multiple_regression/09_multiple_linear.html#story",
    "title": "9: Multiple Linear Regression",
    "section": "Story",
    "text": "Story\n\nBookFinance\n\n\n\n\n\n\n\nMoneyball (2003)\n\n\n\n\n\n\nOakland Athletics fielded a competitive team despite having a payroll size around 1/3 of some other franchises\nTraditional scouting vs modern statistics\nIdea: Can we identify qualities (variables) in baseball players that lead to more wins?\n\n\n\n\n\n\n\n\nMLB Team Salaries, 2002\n\n\n\nimage source: Wikimedia Commons"
  },
  {
    "objectID": "posts/09_multiple_regression/09_multiple_linear.html#data",
    "href": "posts/09_multiple_regression/09_multiple_linear.html#data",
    "title": "9: Multiple Linear Regression",
    "section": "Data",
    "text": "Data\n\nLahmanOffenseDefense\n\n\nToday’s data set comes from the Lahman package, which contains a lot of historical data about Major League Baseball.\n\nLahman package CRAN page\n\n\n\n\nR: runs\nH: hits\nX2B: doubles\nX3B: triples\nHR: home runs\nBB: walks\nSO: strikeouts (by hitters)\nSB: stolen bases\n\n\n\n\nRA: runs allowed\nER: earned runs\nHA: hits allowed\nHRA: home runs allowed\nBBA: walks allowed\nSOA: strikeouts (by pitchers)\nE: errors\nFP: fielding percentage"
  },
  {
    "objectID": "posts/09_multiple_regression/09_multiple_linear.html#correlation",
    "href": "posts/09_multiple_regression/09_multiple_linear.html#correlation",
    "title": "9: Multiple Linear Regression",
    "section": "Correlation",
    "text": "Correlation\n\nOffenseCodeDefenseCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbb_df |&gt;\n  ggplot(aes(x = R, y = W)) +\n  geom_point(color = oakland_green) + \n  labs(title = \"Wins vs Runs Scored\",\n       subtitle = paste0(\"r = \", round(cor_value, 4), \n                         \", slightly and positively correlated\"),\n       caption = \"seasons 1990 to 1999\",\n       x = \"runs scored\",\n       y = \"wins\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncor_value &lt;- cor(bb_df$RA, bb_df$W)\n\nbb_df |&gt;\n  ggplot(aes(x = RA, y = W)) +\n  geom_point(color = oakland_green) + \n  labs(title = \"Wins vs Runs Allowed\",\n       subtitle = paste0(\"r = \", round(cor_value, 4), \n                         \", virtually uncorrelated\"),\n       caption = \"seasons 1990 to 1999\",\n       x = \"runs allowed\",\n       y = \"wins\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/09_multiple_regression/09_multiple_linear.html#correlation-matrices",
    "href": "posts/09_multiple_regression/09_multiple_linear.html#correlation-matrices",
    "title": "9: Multiple Linear Regression",
    "section": "Correlation Matrices",
    "text": "Correlation Matrices\n\nOffenseCodeDefenseCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbb_df |&gt;\n  select(any_of(offense_cats)) |&gt;\n  cor() |&gt;\n  corrplot.mixed(order = \"FPC\",\n                 upper = \"ellipse\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbb_df |&gt;\n  select(any_of(defense_cats)) |&gt;\n  cor() |&gt;\n  corrplot.mixed(order = \"FPC\",\n                 upper = \"ellipse\")\n\n\n\n\n\n\n\n\n\n\nNew Directions\n\n\n\n\n\nSo far, a sabermetrician might observe and ask\n\nWins are correlated with runs scored\nWhat correlates well with runs scored?"
  },
  {
    "objectID": "posts/09_multiple_regression/09_multiple_linear.html#model-equation",
    "href": "posts/09_multiple_regression/09_multiple_linear.html#model-equation",
    "title": "9: Multiple Linear Regression",
    "section": "Model Equation",
    "text": "Model Equation\n\nMathCodeInterceptSlopeDetermination\n\n\n\\[\\text{Runs} = \\beta_{0} + \\beta_{1}(\\text{Hits})\\]\n\n\\(\\beta_{0}\\): intercept\n\\(\\beta_{1}\\): change in Runs with respect to Hits\n\n\n\n\nlm(R ~ H, data = bb_df)\n\n\nCall:\nlm(formula = R ~ H, data = bb_df)\n\nCoefficients:\n(Intercept)            H  \n  -108.7903       0.5934  \n\n\n\n\\(\\beta_{0} \\approx -108.7903\\)\n\\(\\beta_{1} \\approx 0.5934\\)\n\n\\[\\text{Runs} = -108.7903 + 0.5934(\\text{Hits})\\]\n\n\nIn a hypothetical scenario where a team has zero hits,\n\\[\\text{Runs} = -108.7903 + 0.5934(0)\\] the model estimates that the baseball team will win about negative 109 games in a season.\n\nsee note about “Removing the intercept” below\n\n\n\nWe continue to intercept the rate of change (or slope)\n\\[\\beta_{1} \\approx 0.5934\\]\nwith language like\n\nFor every additional hit, the number of runs increases by about 0.5934.\n\n\n\nWe can get a sense of how useful this model can be with the coefficient of determination.\n\nmod1 &lt;- lm(R ~ H, data = bb_df) #baseline model\nsummary(mod1)$adj.r.squared\n\n[1] 0.6893222\n\n\n\nAccording to the coefficient of determination, this model (with “Hits” as an explanatory variable) explains about 69 percent of the variance in runs scored.\n\n\n\n\n\n\n\n\n\n\nRemoving the Intercept\n\n\n\n\n\nSometimes an analyst might want to remove the intercept term (here: zero hits should imply zero runs?)\n\\[\\text{Runs} = \\beta_{1}(\\text{Hits})\\]\n\nmod0 &lt;- lm(R ~ H - 1, data = bb_df) #removed intercept\nmod0\n\n\nCall:\nlm(formula = R ~ H - 1, data = bb_df)\n\nCoefficients:\n    H  \n0.517  \n\n\n\nsummary(mod0)$adj.r.squared #removed intercept\n\n[1] 0.993489\n\nsummary(mod1)$adj.r.squared #baseline model\n\n[1] 0.6893222\n\n\nWhile removing the intercept seems great in this simple example, in practice removing the intercept does not tend to generalize to larger models or inclusion of additional data."
  },
  {
    "objectID": "posts/09_multiple_regression/09_multiple_linear.html#different-models-different-coefficients",
    "href": "posts/09_multiple_regression/09_multiple_linear.html#different-models-different-coefficients",
    "title": "9: Multiple Linear Regression",
    "section": "Different Models, Different Coefficients",
    "text": "Different Models, Different Coefficients\n\n\n\n\n\n\nCoefficients are different in different models\n\n\n\n\n\n\n\n\n\n\n\n\n\nEarly Baseball Stats Models\n\n\nComparing the coefficients\n\n\ncoefs\nmod1\nmod2\n\n\n\n\nbeta_0\n-108.7903\n-124.4358\n\n\nbeta_1\n0.5934\n0.4382\n\n\nbeta_2\n-\n0.4395\n\n\n\nSML 201\n\n\n\n\n\n\n\n\n\nmod_stats_df &lt;- data.frame(\n  coefs = c(\"beta_0\", \"beta_1\", \"beta_2\"),\n  mod1 = c(-108.7903, 0.5934, \"-\"),\n  mod2 = c(-124.4358, 0.4382, 0.4395)\n)\n\nmod_stats_df |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"SML 201\") |&gt;\n  tab_header(\n    title = \"Early Baseball Stats Models\",\n    subtitle = \"Comparing the coefficients\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = oakland_green),\n      cell_text(color = oakland_yellow)\n    ),\n    locations = cells_body(columns = mod1)\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = oakland_yellow),\n      cell_text(color = oakland_green)\n    ),\n    locations = cells_body(columns = mod2)\n  )\n\n\n\n\nIn practice, we tend to explore several models through trial-and-error. After choosing a model, we then scrutinize the interpretation of the \\(\\beta\\) coefficients."
  },
  {
    "objectID": "posts/09_multiple_regression/09_multiple_linear.html#augmentation",
    "href": "posts/09_multiple_regression/09_multiple_linear.html#augmentation",
    "title": "9: Multiple Linear Regression",
    "section": "Augmentation",
    "text": "Augmentation\nWe can attach new columns and calculations using mutate.\n\nbb_df &lt;- bb_df |&gt;\n  mutate(BA = H/AB,               #batting average\n         OBP = (H + BB + HBP)/AB, #on-base percentage\n         SLG = (H + X2B + 2*X3B + 3*HR)/AB, #slugging percentage\n         OPS = OBP + SLG)         #on-base plus slugging\n\n\nStats like runs, hits, walks, and strikeouts are called count statistics. Baseball players tend to accumulate count statistics with more playing time.\nStats like BA, OBP, and SLGare called rate statistics. These baseball statistics are adjusted over playing time.\nThese derived statistics may be better to evaluate individual baseball players (rather than whole teams).\nAside: yes, it may be silly to add together OBP and SLG (i.e. two rate statistics), but baseball writers really like this calculation."
  },
  {
    "objectID": "posts/09_multiple_regression/09_multiple_linear.html#using-the-derived-statistics",
    "href": "posts/09_multiple_regression/09_multiple_linear.html#using-the-derived-statistics",
    "title": "9: Multiple Linear Regression",
    "section": "Using the Derived Statistics",
    "text": "Using the Derived Statistics\nWe build a model for different allocations of explanatory variables.\n\nfit_BA  &lt;- lm(R ~ BA,  data = bb_df)\nfit_OBP &lt;- lm(R ~ OBP, data = bb_df)\nfit_SLG &lt;- lm(R ~ SLG, data = bb_df)\nfit_OPS &lt;- lm(R ~ OBP + SLG, data = bb_df)"
  },
  {
    "objectID": "posts/09_multiple_regression/09_multiple_linear.html#measuring-the-models",
    "href": "posts/09_multiple_regression/09_multiple_linear.html#measuring-the-models",
    "title": "9: Multiple Linear Regression",
    "section": "Measuring the Models",
    "text": "Measuring the Models\nWe use the coefficients of determination to help us rank the models.\n\nsummary(fit_BA)$adj.r.squared\n\n[1] 0.3445441\n\nsummary(fit_OBP)$adj.r.squared\n\n[1] 0.4279833\n\nsummary(fit_SLG)$adj.r.squared\n\n[1] 0.433078\n\nsummary(fit_OPS)$adj.r.squared\n\n[1] 0.4902423"
  },
  {
    "objectID": "posts/09_multiple_regression/09_multiple_linear.html#picking-the-best-model",
    "href": "posts/09_multiple_regression/09_multiple_linear.html#picking-the-best-model",
    "title": "9: Multiple Linear Regression",
    "section": "Picking the Best Model",
    "text": "Picking the Best Model\n\ngtCode\n\n\n\n\n\n\n\n\n\n\nDerived Baseball Stats Models\n\n\nComparing the coefficients of determination\n\n\nmodels\nr2_vals\n\n\n\n\nfit_BA\n0.3445\n\n\nfit_OBP\n0.4280\n\n\nfit_SLG\n0.4331\n\n\nfit_OPS\n0.4902\n\n\n\nSML 201\n\n\n\n\n\n\n\n\n\n\n\nmod_stats_df2 &lt;- data.frame(\n  models = paste0(\"fit_\", c(\"BA\", \"OBP\", \"SLG\", \"OPS\")),\n  r2_vals = round(c(summary(fit_BA)$adj.r.squared,\n                    summary(fit_OBP)$adj.r.squared,\n                    summary(fit_SLG)$adj.r.squared,\n                    summary(fit_OPS)$adj.r.squared), 4)\n)\n\nmod_stats_df2 |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"SML 201\") |&gt;\n  tab_header(\n    title = \"Derived Baseball Stats Models\",\n    subtitle = \"Comparing the coefficients of determination\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = oakland_yellow),\n      cell_text(color = oakland_green,\n                weight = \"bold\")\n    ),\n    locations = cells_body(columns = c(models, r2_vals),\n                           rows = r2_vals == max(r2_vals))\n    # finds maximum value programmatically\n  )"
  },
  {
    "objectID": "posts/09_multiple_regression/09_multiple_linear.html#before-interaction",
    "href": "posts/09_multiple_regression/09_multiple_linear.html#before-interaction",
    "title": "9: Multiple Linear Regression",
    "section": "Before Interaction",
    "text": "Before Interaction\nFirst, let us try another multiple linear regression model\n\nVariablesCodeInterceptSlopesDetermination\n\n\n\\[\\text{Wins} = \\beta_{0} + \\beta_{1}(\\text{Runs Scored}) + \\beta_{2}(\\text{Runs Allowed})\\]\n\nresponse variable: Wins\nexplanatory variables:\n\nRuns Scored (offense)\nRuns Allowed (defense)\n\n\n\n\n\nlm(W ~ R + RA, data = bb_df)\n\n\nCall:\nlm(formula = W ~ R + RA, data = bb_df)\n\nCoefficients:\n(Intercept)            R           RA  \n   42.50059      0.12530     -0.07692  \n\n\n\n\nIn a hypothetical scenario where a team has zero runs and zero runs allowed,\n\\[\\text{Wins} = 42.5006 + 0.1253(0) - 0.0769(0)\\]\nthe model estimates that the baseball team will win about 43 games in a season.\n\n\nIn regression, we say that we control for other variables by treating other variables as constants.\n\\[\\text{Wins} = 42.5006 + 0.1253(\\text{Runs Scored}) - 0.0769(\\text{Runs Allowed})\\]\n\nHolding runs allowed constant, for every additional run scored, the number of wins increases by about 0.1253.\nHolding runs scored constant, for every additional run allowed, the number of wins decreases by about 0.0769.\n\n\n\nWith our usage of the coefficient of determination\n\nwithout_interaction &lt;- lm(W ~ R + RA, data = bb_df)\n\nsummary(without_interaction)$adj.r.squared\n\n[1] 0.700125\n\n\n\nAccording to the coefficient of determination, this model (with 2 explanatory variables) explains about 70 percent of the variance in wins."
  },
  {
    "objectID": "posts/09_multiple_regression/09_multiple_linear.html#with-an-interaction-term",
    "href": "posts/09_multiple_regression/09_multiple_linear.html#with-an-interaction-term",
    "title": "9: Multiple Linear Regression",
    "section": "With an Interaction Term",
    "text": "With an Interaction Term\nNow let us explore an interaction term\n\nVariablesCodeInterceptSlopesDetermination\n\n\n\\[\\begin{array}{rcl}\n\\text{Wins} & = & \\beta_{0} \\\\\n& & + \\beta_{1}(\\text{Runs Scored}) \\\\\n& & + \\beta_{2}(\\text{Runs Allowed}) \\\\\n& & + \\beta_{3}(\\text{Runs Scored})(\\text{Runs Allowed}) \\\\\n\\end{array}\\]\n\nresponse variable: Wins\nexplanatory variables:\n\nRuns Scored\nRuns Allowed\ninteraction bteween Runs Scored and Runs Allowed\n\n\n\n\n\nlm(W ~ R + RA + R:RA, data = bb_df)\n\n\nCall:\nlm(formula = W ~ R + RA + R:RA, data = bb_df)\n\nCoefficients:\n(Intercept)            R           RA         R:RA  \n -8.896e+01    3.082e-01    1.090e-01   -2.555e-04  \n\n\n\n\nIn a hypothetical scenario where a team has zero runs and zero runs allowed,\n\\[\\text{Wins} = -88.96 + 0.3082(0) + 0.1090(0) - 0.0002(0)(0)\\]\nthe model estimates that the baseball team will win about -89 games in a season.\n\n\nTo get a sense of how many runs a MLB team allows in a season, we can use the summary command.\n\nsummary(bb_df$RA)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  448.0   655.2   721.0   726.5   794.0  1103.0 \n\n\n\nStrong Defense\nSuppose that a MLB team has good defensive skills and allows about 650 runs in a season (i.e. around the 20th percentile).\n\\[\\begin{array}{rcl}\n\\text{Wins} & = & -88.96 + 0.3082(\\text{Runs Scored}) + 0.1090(\\text{Runs Allowed}) - 0.0002(\\text{Runs Scored})(\\text{Runs Allowed}) \\\\\n~ & = & -88.96 + 0.3082(\\text{Runs Scored}) + 0.1090(650) - 0.0002(\\text{Runs Scored})(650) \\\\\n  ~ & = & -18.11 + 0.1782(\\text{Runs Scored})\\\\\n\\end{array}\\]\n\nHolding runs allowed constant at 650, for every additional run scored, the number of wins increases by about 0.1782.\n\n\n\nWeak Defense\nSuppose that a MLB team has weak defensive skills and allows about 800 runs in a season (i.e. around the 80th percentile).\n\\[\\begin{array}{rcl}\n\\text{Wins} & = & -88.96 + 0.3082(\\text{Runs Scored}) + 0.1090(\\text{Runs Allowed}) - 0.0002(\\text{Runs Scored})(\\text{Runs Allowed}) \\\\\n~ & = & -88.96 + 0.3082(\\text{Runs Scored}) + 0.1090(800) - 0.0002(\\text{Runs Scored})(800) \\\\\n  ~ & = & -1.76 + 0.1482(\\text{Runs Scored})\\\\\n\\end{array}\\]\n\nHolding runs allowed constant at 800, for every additional run scored, the number of wins increases by about 0.1482.\n\n\n\n\nWith our usage of the coefficient of determination\n\nwith_interaction &lt;- lm(W ~ R + RA + R:RA, data = bb_df)\n\nsummary(without_interaction)$adj.r.squared\n\n[1] 0.700125\n\nsummary(with_interaction)$adj.r.squared\n\n[1] 0.764043\n\n\n\nAccording to the coefficient of determination, this model (with the interaction term) explains about 76 percent of the variance in wins."
  },
  {
    "objectID": "posts/09_multiple_regression/09_multiple_linear.html#milestone-home-run-totals",
    "href": "posts/09_multiple_regression/09_multiple_linear.html#milestone-home-run-totals",
    "title": "9: Multiple Linear Regression",
    "section": "Milestone Home Run Totals",
    "text": "Milestone Home Run Totals\n\nRecordsCodeZ-ScoresSwitchWhat If\n\n\n\n\n\n\n\nRoger Maris\n\n\n\n\n\n\nRoger Maris hit 61 home runs in 1961\nAaron Judge hit 62 home runs in 2022\n\nAmerican League records\n\n\n\n\n\n\n\n\nAaron Judge\n\n\n\n\n\n\n\nLahman::Batting |&gt;\n  filter(yearID %in% c(\"1961\", \"2022\")) |&gt;\n  filter(AB &gt;= 100) |&gt;\n  group_by(yearID) |&gt;\n  summarize(xbar = mean(HR, na.rm = TRUE),\n            s = sd(HR, na.rm = TRUE))\n\n# A tibble: 2 × 3\n  yearID  xbar     s\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   1961  10.8 10.8 \n2   2022  10.6  8.91\n\n\n\n\n\\[z_{M} = \\frac{61 - \\bar{x}_{2022}}{s_{2022}} = \\frac{61 - 10.7615}{10.7928} \\approx 4.6548\\] Roger Maris’ home run record was about 4.6548 standard deviations above the mean in 1961.\n\\[z_{J} = \\frac{62 - \\bar{x}_{2022}}{s_{2022}} = \\frac{62 - 10.5729}{8.9065} \\approx 5.7741\\]\nAaron Judge’s home run record was about 5.7741 standard deviations above the mean in 2022.\n\n\n\\[4.6548 = \\frac{x_{M} - \\bar{x}_{2022}}{s_{2022}} = \\frac{x_{M} - 10.5729}{8.9065} \\Rightarrow x_{M} \\approx 52.0388\\]\n\\[5.7741 = \\frac{x_{J} - \\bar{x}_{1961}}{s_{1961}} = \\frac{x_{J} - 10.7615}{10.7928} \\Rightarrow x_{J} \\approx 73.0802\\]\n\n\n\n\n\n\n\nRoger Maris\n\n\n\n\n\n\nRoger Maris would have hit 52 home runs is 2022\nAaron Judge would have hit 73 home runs in 1961\n\n\n\n\n\n\n\nAaron Judge"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html",
    "href": "posts/10_modeling_cats/10_modeling_categories.html",
    "title": "10: Modeling Categorical Variables",
    "section": "",
    "text": "library(\"janitor\")   #tools for data cleaning\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\nloan_df &lt;- readr::read_csv(\"loan_data_set.csv\") |&gt;\n  janitor::clean_names()"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#start",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#start",
    "title": "10: Modeling Categorical Variables",
    "section": "Start",
    "text": "Start\n\n\n\nGoal: Utilize categorical variables in regression models\nObjective: Explore one-hot encoding and start classification\n\n\n\n\n\n\n\nmultiple slopes!"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#data",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#data",
    "title": "10: Modeling Categorical Variables",
    "section": "Data",
    "text": "Data\n\nDescriptionScenarioResponse VariableExplanatory Variables\n\n\n\n\n“Dream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.”\n\nSource: Kaggle\n\n\n\n\n\n\n\nDream Home Finance\n\n\n\n\n\n\n“Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set.”\n\n\nWe will try to predict the loan amount (i.e. numerical variable)\n\nLoan amount (in thousands of dollars)\n\n\n\n\nGender (of primary applicant)\nMarital status (of primary applicant)\nDependents\nEducation\nSelf-employed\nApplicant income (monthly, in dollars)\nCo-applicant income (monthly, in dollars)\nloan amount terms (in months)\nCredit history\nProperty area"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#cleaning",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#cleaning",
    "title": "10: Modeling Categorical Variables",
    "section": "Cleaning",
    "text": "Cleaning\n\nremove rows that have missing values in the response variable (loan_amount)\nconvert dependents to a numerical variable\n\nhere, replace “+” with nothing\n\ncombine “income” columns\n\nensure all dollar amounts are in the same units (thousands of dollars)\n\nconvert credit_history to a factor variable (i.e. categorical)\nretain relevant columns\n\n\nloan_df &lt;- loan_df |&gt;\n  filter(!is.na(loan_amount)) |&gt;\n  mutate(dependents_num = as.numeric(\n    str_replace(dependents, \"\\\\+\", \"\")\n  )) |&gt;\n  mutate(income = applicant_income/1000 + coapplicant_income/1000) |&gt;\n  mutate(credit_history = factor(credit_history)) |&gt;\n  select(loan_amount, income, dependents_num, gender, married, education, self_employed, credit_history, property_area, loan_status)\n\nAfter cleaning the data, we should report the size of the resultant data frame\n\nnrow(loan_df) #number of observations\n\n[1] 592\n\nncol(loan_df) #number of variables\n\n[1] 10\n\n\nand the structure of the data frame.\n\nstr(loan_df, give.attr = FALSE)\n\ntibble [592 × 10] (S3: tbl_df/tbl/data.frame)\n $ loan_amount   : num [1:592] 128 66 120 141 267 95 158 168 349 70 ...\n $ income        : num [1:592] 6.09 3 4.94 6 9.61 ...\n $ dependents_num: num [1:592] 1 0 0 0 2 0 3 2 1 2 ...\n $ gender        : chr [1:592] \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ married       : chr [1:592] \"Yes\" \"Yes\" \"Yes\" \"No\" ...\n $ education     : chr [1:592] \"Graduate\" \"Graduate\" \"Not Graduate\" \"Graduate\" ...\n $ self_employed : chr [1:592] \"No\" \"Yes\" \"No\" \"No\" ...\n $ credit_history: Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 1 2 2 2 ...\n $ property_area : chr [1:592] \"Rural\" \"Urban\" \"Urban\" \"Urban\" ...\n $ loan_status   : chr [1:592] \"N\" \"Y\" \"Y\" \"Y\" ..."
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#scenario-1",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#scenario-1",
    "title": "10: Modeling Categorical Variables",
    "section": "Scenario 1:",
    "text": "Scenario 1:\n\nresponse variable: loan_amount\nexplanatory variables:\n\nincome\ndependents_num"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#build-the-model",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#build-the-model",
    "title": "10: Modeling Categorical Variables",
    "section": "Build the Model",
    "text": "Build the Model\n\nmod1 &lt;- lm(loan_amount ~ income + dependents_num, data = loan_df)"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#interpretation",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#interpretation",
    "title": "10: Modeling Categorical Variables",
    "section": "Interpretation",
    "text": "Interpretation\nSometimes we interpret the slopes to see if the found coefficients make sense in context.\n\nmod1\n\n\nCall:\nlm(formula = loan_amount ~ income + dependents_num, data = loan_df)\n\nCoefficients:\n   (Intercept)          income  dependents_num  \n        84.518           8.048           6.943  \n\n\n\nHolding dependents constant, for each $1000 increase in monthly income, the loan amount increases by about $8000\n\n\nHolding income constant, for each additional dependent, the loan amount increases by about $7000"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#determination",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#determination",
    "title": "10: Modeling Categorical Variables",
    "section": "Determination",
    "text": "Determination\n\nsummary(mod1)$adj.r.squared\n\n[1] 0.3955349\n\n\n\nFor this baseline model, the coefficient of determination states that we can explain about 40 percent of the variance in loan amount with these two numerical explanatory variables."
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#scatterplot",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#scatterplot",
    "title": "10: Modeling Categorical Variables",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nloan_df |&gt;\n  filter(!is.na(credit_history)) |&gt;\n  ggplot(aes(x = income, y = loan_amount, color = credit_history)) +\n  geom_point() +\n  geom_smooth(formula = \"y ~ x\",\n              method = \"lm\",\n              se = FALSE) +\n  labs(title = \"Dream Home Finance\",\n       subtitle = \"Interaction Plot\",\n       caption = \"SML 201\",\n       x = \"combined monthly income (thousands)\",\n       y = \"loan amount (thousands)\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#model-2",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#model-2",
    "title": "10: Modeling Categorical Variables",
    "section": "Model 2",
    "text": "Model 2\n\nmod2_without_interaction &lt;- lm(loan_amount ~ income + \n                                 dependents_num +\n                                 credit_history,\n                               data = loan_df)"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#model-statistics",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#model-statistics",
    "title": "10: Modeling Categorical Variables",
    "section": "Model Statistics",
    "text": "Model Statistics\n\nsummary(mod2_without_interaction)\n\n\nCall:\nlm(formula = loan_amount ~ income + dependents_num + credit_history, \n    data = loan_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-372.17  -28.38   -5.71   21.62  356.55 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      82.5113     8.2910   9.952  &lt; 2e-16 ***\nincome            7.6934     0.4386  17.539  &lt; 2e-16 ***\ndependents_num    8.8323     2.8231   3.129  0.00185 ** \ncredit_history1   2.2122     7.9957   0.277  0.78214    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 65.84 on 526 degrees of freedom\n  (62 observations deleted due to missingness)\nMultiple R-squared:  0.3908,    Adjusted R-squared:  0.3873 \nF-statistic: 112.5 on 3 and 526 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#interpretation-1",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#interpretation-1",
    "title": "10: Modeling Categorical Variables",
    "section": "Interpretation",
    "text": "Interpretation\n\\[\\begin{array}{rcl}\nY & = & \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}X_{3} \\\\\n~ & \\approx & 81.7427 + 6.4019X_{1} + 9.3093X_{2} + 1.9206X_{3}\n\\end{array}\\]\nwhere\n\\[X_{1}: \\text{income}, \\quad X_{2}: \\text{number of dependents}\\]\nand\n\\[X_{3} = \\begin{cases} 1, & \\text{passed credit check} \\\\\n0, & \\text{did not pass credit check} \\\\ \\end{cases}\\]\n\nPassing the credit check increases the home loan by about $2000 (??)"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#interaction-term",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#interaction-term",
    "title": "10: Modeling Categorical Variables",
    "section": "Interaction Term",
    "text": "Interaction Term\n\\[\\begin{array}{rcl}\nY & = & \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}X_{3} \\\\\n\\end{array}\\]\n\nmod2_with_interaction &lt;- lm(loan_amount ~ income +\n                              dependents_num +\n                              income:credit_history,\n                            data = loan_df)"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#refined-interpretation",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#refined-interpretation",
    "title": "10: Modeling Categorical Variables",
    "section": "Refined Interpretation",
    "text": "Refined Interpretation\n\nsummary(mod2_with_interaction)\n\n\nCall:\nlm(formula = loan_amount ~ income + dependents_num + income:credit_history, \n    data = loan_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-327.08  -28.02   -4.70   21.74  346.79 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             81.7427     4.6271  17.666  &lt; 2e-16 ***\nincome                   6.4019     0.6430   9.957  &lt; 2e-16 ***\ndependents_num           9.3093     2.8080   3.315 0.000979 ***\nincome:credit_history1   1.9206     0.7048   2.725 0.006646 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 65.38 on 526 degrees of freedom\n  (62 observations deleted due to missingness)\nMultiple R-squared:  0.3992,    Adjusted R-squared:  0.3957 \nF-statistic: 116.5 on 3 and 526 DF,  p-value: &lt; 2.2e-16\n\n\n\\[\\begin{array}{rcl}\nY & = & \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}X_{3} \\\\\n~ & = & 81.7427 + 6.4019X_{1} + 9.3093X_{2} + 1.9206X_{1}X_{3} \\\\\n\\end{array}\\]\n\nFor a family with \\(X_{2} = 0\\) dependents and bad credit history (\\(X_{3} = 0\\))\n\n\\[Y = 81.7427 + 6.4019X_{1}\\]\n\nFor each $1000 increase in monthly income, the loan amount increases by about $6400.\n\n\nFor a family with \\(X_{2} = 0\\) dependents and good credit history (\\(X_{3} = 1\\))\n\n\\[Y = 81.7427 + 6.4019X_{1} + 1.9206X_{1}\\]\n\nFor each $1000 increase in monthly income, the loan amount increases by about $8300."
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#determination-1",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#determination-1",
    "title": "10: Modeling Categorical Variables",
    "section": "Determination",
    "text": "Determination\n\nsummary(mod2_with_interaction)$adj.r.squared\n\n[1] 0.3957492\n\n\n\nFor this model with an interaction term and the two original explanatory variables, the coefficient of determination shows that we can explain about 40 percent of the variance in the loan amount.\n\n\n\n\n\n\n\nHow Complex Should Models Be?\n\n\n\n\n\nDid you catch that?\n\nmod1 had a coefficient of determination of about 0.3955\nmod2_with_interaction had a coefficient of determination of about 0.3957\n\nWith hardly any gains in explaning variance, an analyst might opt to continue with the simpler model (here: mod1) moving forward in an analysis since it eases interpretability"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#scatterplot-1",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#scatterplot-1",
    "title": "10: Modeling Categorical Variables",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nloan_df |&gt;\n  filter(!is.na(credit_history)) |&gt;\n  ggplot(aes(x = income, y = loan_amount, color = education)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(formula = \"y ~ x\",\n              method = \"lm\",\n              se = FALSE) +\n  labs(title = \"Dream Home Finance\",\n       subtitle = \"Parallel Slopes?\",\n       caption = \"SML 201\",\n       x = \"combined monthly income (thousands)\",\n       y = \"loan amount (thousands)\") +\n  scale_color_manual(values = c(\"gray\", \"darkgreen\")) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#model-3",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#model-3",
    "title": "10: Modeling Categorical Variables",
    "section": "Model 3",
    "text": "Model 3\n\nmod3_without_interaction &lt;- lm(loan_amount ~ income + \n                                 dependents_num +\n                                 education,\n                               data = loan_df)"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#model-statistics-1",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#model-statistics-1",
    "title": "10: Modeling Categorical Variables",
    "section": "Model Statistics",
    "text": "Model Statistics\n\nsummary(mod3_without_interaction)\n\n\nCall:\nlm(formula = loan_amount ~ income + dependents_num + education, \n    data = loan_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-388.68  -28.33   -6.10   20.14  402.21 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            89.1379     4.7793  18.651  &lt; 2e-16 ***\nincome                  7.8672     0.4314  18.238  &lt; 2e-16 ***\ndependents_num          7.4332     2.7706   2.683  0.00751 ** \neducationNot Graduate -17.4739     6.8909  -2.536  0.01148 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 66.82 on 575 degrees of freedom\n  (13 observations deleted due to missingness)\nMultiple R-squared:  0.4043,    Adjusted R-squared:  0.4012 \nF-statistic: 130.1 on 3 and 575 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#interpretation-2",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#interpretation-2",
    "title": "10: Modeling Categorical Variables",
    "section": "Interpretation",
    "text": "Interpretation\n\\[\\begin{array}{rcl}\nY & = & \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}X_{3} \\\\\n~ & \\approx & 89.1379 + 7.8672X_{1} + 7.4332X_{2} - 17.4739X_{3}\n\\end{array}\\]\nwhere\n\\[X_{1}: \\text{income}, \\quad X_{2}: \\text{number of dependents}\\]\nand\n\\[X_{3} = \\begin{cases} 1, & \\text{did not graduate from college} \\\\\n0, & \\text{graduated college} \\\\ \\end{cases}\\]\n\nThose who did not graduate from college had a home loan value that was lower by about $17500."
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#interaction-term-1",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#interaction-term-1",
    "title": "10: Modeling Categorical Variables",
    "section": "Interaction Term",
    "text": "Interaction Term\n\\[\\begin{array}{rcl}\nY & = & \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}X_{3} \\\\\n\\end{array}\\]\n\nmod3_with_interaction &lt;- lm(loan_amount ~ income +\n                              dependents_num +\n                              income:education,\n                            data = loan_df)"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#refined-interpretation-1",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#refined-interpretation-1",
    "title": "10: Modeling Categorical Variables",
    "section": "Refined Interpretation",
    "text": "Refined Interpretation\n\nsummary(mod3_with_interaction)\n\n\nCall:\nlm(formula = loan_amount ~ income + dependents_num + income:education, \n    data = loan_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-395.44  -28.18   -6.58   21.03  401.70 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   87.3028     4.6628  18.723   &lt;2e-16 ***\nincome                         7.9834     0.4278  18.662   &lt;2e-16 ***\ndependents_num                 7.1590     2.7728   2.582   0.0101 *  \nincome:educationNot Graduate  -2.3047     1.2066  -1.910   0.0566 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 66.98 on 575 degrees of freedom\n  (13 observations deleted due to missingness)\nMultiple R-squared:  0.4014,    Adjusted R-squared:  0.3983 \nF-statistic: 128.5 on 3 and 575 DF,  p-value: &lt; 2.2e-16\n\n\n\\[\\begin{array}{rcl}\nY & = & \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}X_{3} \\\\\n~ & = & 87.3028 + 7.9834X_{1} + 7.1590X_{2} - 2.3047X_{1}X_{3} \\\\\n\\end{array}\\]\n\nFor a family with \\(X_{2} = 2\\) dependents and graduated from college (\\(X_{3} = 0\\))\n\n\\[Y = 87.3028 + 7.9834X_{1} + 14.3180\\]\n\nFor each $1000 increase in monthly income, the loan amount increases by about $8000.\n\n\nFor a family with \\(X_{2} = 2\\) dependents and did not graduate from college (\\(X_{3} = 1\\))\n\n\\[Y = 81.7427 + 7.9834X_{1} + 14.3180 - 2.3047X_{1}\\]\n\nFor each $1000 increase in monthly income, the loan amount increases by about $5600."
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#determination-2",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#determination-2",
    "title": "10: Modeling Categorical Variables",
    "section": "Determination",
    "text": "Determination\n\nsummary(mod3_with_interaction)$adj.r.squared\n\n[1] 0.3983014\n\n\n\nFor this model with an interaction term and the two original explanatory variables, the coefficient of determination shows that we can explain about 40 percent of the variance in the loan amount.\n\n\n\n\n\n\n\nReordering Factors\n\n\n\n\n\nYou may have noticed that R tends to output categorical variables in alphabetical order by default (e.g. the bars in a bar chart). If you want to customize the order presented, you would need to employ a factor variable where you can explicitly set the levels.\n\nloan_df &lt;- loan_df |&gt;\n  mutate(education_fac = \n           factor(education,\n                  levels = c(\"Not Graduate\", \"Graduate\")))\n\nNow, the software will treat “Not Graduate” as the baseline and “Graduate” as the augmentation.\n\nmod4 &lt;- lm(loan_amount ~ income + dependents_num + \n             education_fac,\n           data = loan_df)\nsummary(mod4)\n\n\nCall:\nlm(formula = loan_amount ~ income + dependents_num + education_fac, \n    data = loan_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-388.68  -28.33   -6.10   20.14  402.21 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            71.6640     6.7245  10.657  &lt; 2e-16 ***\nincome                  7.8672     0.4314  18.238  &lt; 2e-16 ***\ndependents_num          7.4332     2.7706   2.683  0.00751 ** \neducation_facGraduate  17.4739     6.8909   2.536  0.01148 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 66.82 on 575 degrees of freedom\n  (13 observations deleted due to missingness)\nMultiple R-squared:  0.4043,    Adjusted R-squared:  0.4012 \nF-statistic: 130.1 on 3 and 575 DF,  p-value: &lt; 2.2e-16\n\n\n\nloan_df |&gt;\n  filter(!is.na(education_fac)) |&gt;\n  ggplot(aes(x = income, y = loan_amount, color = education_fac)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(formula = \"y ~ x\",\n              method = \"lm\",\n              se = FALSE) +\n  labs(title = \"Dream Home Finance\",\n       subtitle = \"Parallel Slopes?\",\n       caption = \"SML 201\",\n       x = \"combined monthly income (thousands)\",\n       y = \"loan amount (thousands)\") +\n  scale_color_manual(values = c(\"gray\", \"#E77500\")) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#one-hot-encoding",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#one-hot-encoding",
    "title": "10: Modeling Categorical Variables",
    "section": "One-Hot Encoding",
    "text": "One-Hot Encoding\nSimilar to previous calculations, our software employs dummy variables or one-hot encoding to represent categories as ones and zeroes.\n\nloan_df |&gt;\n  mutate(rural_bool = ifelse(property_area == \"Rural\", 1, 0),\n         semiu_bool = ifelse(property_area == \"Semiurban\", 1, 0),\n         urban_bool = ifelse(property_area == \"Urban\", 1, 0)) |&gt;\n  select(property_area, rural_bool, semiu_bool, urban_bool) |&gt;\n  sample_n(size = 10, replace = FALSE)\n\n# A tibble: 10 × 4\n   property_area rural_bool semiu_bool urban_bool\n   &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Rural                  1          0          0\n 2 Semiurban              0          1          0\n 3 Semiurban              0          1          0\n 4 Semiurban              0          1          0\n 5 Urban                  0          0          1\n 6 Semiurban              0          1          0\n 7 Semiurban              0          1          0\n 8 Semiurban              0          1          0\n 9 Urban                  0          0          1\n10 Urban                  0          0          1\n\n\n\\[\\begin{array}{rcl}\nY & = & \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{3} + \\beta_{4}X_{4} \\\\\n\\end{array}\\]\nwhere\n\\[X_{1}: \\text{income}, \\quad X_{2}: \\text{number of dependents}\\]\nand\n\\[\\begin{array}{rcl}\n  X_{3} & = & \\begin{cases} 1, \\text{semiurban area} \\\\ 0, \\text{otherwise} \\\\ \\end{cases} \\\\\n  X_{4} & = & \\begin{cases} 1, \\text{urban area} \\\\ 0, \\text{otherwise} \\\\ \\end{cases} \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#model-5",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#model-5",
    "title": "10: Modeling Categorical Variables",
    "section": "Model 5",
    "text": "Model 5\n\nmod5 &lt;- lm(loan_amount ~ income + \n             dependents_num +\n             property_area,\n           data = loan_df)"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#model-statistics-2",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#model-statistics-2",
    "title": "10: Modeling Categorical Variables",
    "section": "Model Statistics",
    "text": "Model Statistics\n\nsummary(mod5)\n\n\nCall:\nlm(formula = loan_amount ~ income + dependents_num + property_area, \n    data = loan_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-402.72  -28.36   -7.05   19.80  409.16 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             89.1639     6.1868  14.412   &lt;2e-16 ***\nincome                   8.0575     0.4275  18.849   &lt;2e-16 ***\ndependents_num           6.9669     2.7760   2.510   0.0124 *  \nproperty_areaSemiurban  -3.3175     6.8229  -0.486   0.6270    \nproperty_areaUrban     -10.8084     7.1188  -1.518   0.1295    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 67.1 on 574 degrees of freedom\n  (13 observations deleted due to missingness)\nMultiple R-squared:  0.4002,    Adjusted R-squared:  0.396 \nF-statistic: 95.75 on 4 and 574 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#interpretation-3",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#interpretation-3",
    "title": "10: Modeling Categorical Variables",
    "section": "Interpretation",
    "text": "Interpretation\n\\[\\begin{array}{rcl}\nY & = & \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{3} + \\beta_{4}X_{4} \\\\\n~ & = & 89.1639 + 8.0575X_{1} + 6.9669X_{2} - 3.3175X_{3} - 10.8084X_{4} \\\\\n\\end{array}\\]\nFor a family with one dependent (\\(X_{2} = 1\\)) and a combined monthly income of $5000 (\\(X_{1} = 5\\)),\n\nif they are seeking a home in a rural area (\\(X_{3} = 0, X_{4} = 0\\)), the loan amount is predicted to be about 136 thousand\nif they are seeking a home in a semi-urban area (\\(X_{3} = 1, X_{4} = 0\\)), the loan amount is predicted to be about 133 thousand (i.e. 3 thousand less than the baseline rural scenario)\nif they are seeking a home in an urban area (\\(X_{3} = 0, X_{4} = 1\\)), the loan amount is predicted to be about 126 thousand (i.e. 10 thousand less than the baseline rural scenario)"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#machine-learning",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#machine-learning",
    "title": "10: Modeling Categorical Variables",
    "section": "Machine Learning",
    "text": "Machine Learning\nIf the response variable is …\n\nnumerical \\(\\rightarrow\\) regression task\ncategorical \\(\\rightarrow\\) classification task"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#one-hot-encoding-1",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#one-hot-encoding-1",
    "title": "10: Modeling Categorical Variables",
    "section": "One-Hot Encoding",
    "text": "One-Hot Encoding\n\nloan_df &lt;- loan_df |&gt;\n  mutate(approved = ifelse(loan_status == \"Y\", 1, 0),\n         approved_fac = factor(approved,\n                               levels = c(0,1)))"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#scatterplot-2",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#scatterplot-2",
    "title": "10: Modeling Categorical Variables",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nloan_df |&gt;\n  ggplot(aes(x = income, y = approved)) +\n  geom_point(aes(color = approved_fac)) +\n  geom_smooth(formula = \"y ~ x\",\n              method = \"lm\",\n              se = FALSE) +\n  labs(title = \"Dream Home Finance\",\n       subtitle = \"Linear Regression?\",\n       caption = \"SML 201\",\n       x = \"combined monthly income (thousands)\",\n       y = \"loan status\") +\n  scale_color_manual(values = c(\"gray\", \"darkgreen\")) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#logistic-function",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#logistic-function",
    "title": "10: Modeling Categorical Variables",
    "section": "Logistic Function",
    "text": "Logistic Function\n\n\n\n\n\nlogistic function\n\n\n\n\n\n\ndomain: \\((-\\infty, \\infty)\\)\nrange: \\((0,1)\\)\none-to-one and invertible"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#logistic-regression",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#logistic-regression",
    "title": "10: Modeling Categorical Variables",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n# generalized linear model\nmod6 &lt;- glm(approved_fac ~ income,\n            data = loan_df,\n            family = \"binomial\")"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#extended-model",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#extended-model",
    "title": "10: Modeling Categorical Variables",
    "section": "Extended Model",
    "text": "Extended Model\n\n# generalized linear model\nmod7 &lt;- glm(approved_fac ~ income + dependents_num + credit_history +\n              education + property_area + loan_amount,\n            data = loan_df,\n            family = \"binomial\")"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#logistic-fit",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#logistic-fit",
    "title": "10: Modeling Categorical Variables",
    "section": "Logistic Fit",
    "text": "Logistic Fit\n\nloan_df &lt;- loan_df |&gt;\n  mutate(preds = predict(mod7,\n                         data.frame(income = loan_df$income,\n                                    dependents_num = loan_df$dependents_num,\n                                    credit_history = loan_df$credit_history,\n                                    education = loan_df$education,\n                                    property_area = loan_df$property_area,\n                                    loan_amount = loan_df$loan_amount),\n                         type = \"response\"))"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#machine-predictions",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#machine-predictions",
    "title": "10: Modeling Categorical Variables",
    "section": "Machine Predictions",
    "text": "Machine Predictions\nIt appears that this company rejects about 30 percent of loan applications. Let us make a cutoff point at the 30th percentile in the predictions.\n\ncutoff &lt;- quantile(loan_df$preds, 0.30, na.rm = TRUE)\n\nloan_df |&gt;\n  filter(!is.na(preds)) |&gt;\n  ggplot(aes(x = preds)) +\n  geom_histogram(binwidth = 0.05, color = princeton_black, fill = \"white\") +\n  geom_vline(xintercept = cutoff, color = princeton_orange,\n             linetype = 3, linewidth = 4) +\n  labs(title = \"Machine-Made Predictions\",\n       subtitle = \"through logistic regression\",\n       caption = \"SML 201\",\n       x = \"&lt;== reject ... approve ==&gt;\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#validation",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#validation",
    "title": "10: Modeling Categorical Variables",
    "section": "Validation",
    "text": "Validation\n\nloan_df |&gt;\n  filter(!is.na(preds)) |&gt;\n  mutate(pred_class = ifelse(preds &gt; cutoff, 1, 0)) |&gt;\n  janitor::tabyl(approved_fac, pred_class)\n\n approved_fac  0   1\n            0 97  66\n            1 62 305\n\n\n\\[\\text{accuracy} = \\frac{97 + 305}{97 + 66 + 62 + 305} \\approx 0.7585\\]\nSo far, this automated system would classify the loan applications correctly about 76 percent of the time."
  },
  {
    "objectID": "posts/10_modeling_cats/10_modeling_categories.html#prediction",
    "href": "posts/10_modeling_cats/10_modeling_categories.html#prediction",
    "title": "10: Modeling Categorical Variables",
    "section": "Prediction",
    "text": "Prediction\nPicture a Princeton graduate who makes $10k per month, has two dependents, has good credit, and is seeking a $500k loan for a house in an urban area. Our model says …\n\nthis_prediction = predict(mod7,\n                          data.frame(income = 10,\n                                     dependents_num = 2,\n                                     credit_history = \"1\",\n                                     education = \"Graduate\",\n                                     property_area = \"Urban\",\n                                     loan_amount = 500),\n                          type = \"response\")\n\nprint(paste0(\"The computer model says that we should \",\n             ifelse(this_prediction &gt; cutoff, \"approve\", \"reject\"),\n             \" this application.\"))\n\n[1] \"The computer model says that we should approve this application.\"\n\n\n\n\n\n\n\n\nWhat if the categorical response has more than two levels?\n\n\n\n\n\nIn later machine learning classes, you will encounter\n\nsupport vector machines\nrandom forests"
  },
  {
    "objectID": "posts/11_nonlinear_regression/11_nonlinear_regression.html",
    "href": "posts/11_nonlinear_regression/11_nonlinear_regression.html",
    "title": "11: Nonlinear Regression",
    "section": "",
    "text": "library(\"gt\")\nlibrary(\"janitor\")   #tools for data cleaning\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\n# user-defined function\ncor2text &lt;- function(x,y, num_digits = 4){\n  # This function will compute a correlation, round the result, and describe the results\n  # INPUTS:\n  ## x: numerical vector\n  ## y: numerical vector\n  ## num_digits: number of digits for rounding (default: 4)\n  # OUTPUT: string\n  \n  r = cor(x,y, use = \"pairwise.complete.obs\")\n  \n  cor_des &lt;- case_when(\n    r &gt;= 0.7 ~ \"strongly and positively correlated\",\n    r &gt;= 0.4 & r &lt; 0.7 ~ \"slightly and positively correlated\",\n    r &lt;= -0.4 & r &gt; -0.7 ~ \"slightly and negatively correlated\",\n    r &lt;= -0.7 ~ \"strongly and negatively correlated\",\n    .default = \"virtually uncorrelated\"\n  )\n  \n  #return\n  paste0(\"r = \", round(r, num_digits),\n         \", \", cor_des)\n}\nhurricane_df &lt;- readr::read_csv(\"hurricane.csv\") |&gt;\n  janitor::clean_names()"
  },
  {
    "objectID": "posts/11_nonlinear_regression/11_nonlinear_regression.html#start",
    "href": "posts/11_nonlinear_regression/11_nonlinear_regression.html#start",
    "title": "11: Nonlinear Regression",
    "section": "Start",
    "text": "Start\n\n\n\nGoal: Explore nonlinear variables and regression\nObjective: Employ polynomial and exponential fits\n\n\n\n\n\n\n\npower functions\n\n\n\nimage source: Lumen Learning"
  },
  {
    "objectID": "posts/11_nonlinear_regression/11_nonlinear_regression.html#data",
    "href": "posts/11_nonlinear_regression/11_nonlinear_regression.html#data",
    "title": "11: Nonlinear Regression",
    "section": "Data",
    "text": "Data\n\nDescriptionResponse VariableOther Variables\n\n\nHurricanes that affected the United States from 1994 to 2018\n\nSource: Weather Underground\n\n\n\nWe will try to predict the number of hurricanes that occur this year\n\ndamage (in millions of dollars)\n\n\n\n\nyear\nstorms: number of tropical storms\nhurricanes: number of tropical storms that became hurricanes\ndeaths\nretired_names: hurricanes that were so noteworthy that meteorologists decided not to reuse the name"
  },
  {
    "objectID": "posts/11_nonlinear_regression/11_nonlinear_regression.html#degree-2",
    "href": "posts/11_nonlinear_regression/11_nonlinear_regression.html#degree-2",
    "title": "11: Nonlinear Regression",
    "section": "Degree 2",
    "text": "Degree 2\n\nd2_fit &lt;- lm(damage ~ poly(year, 2, raw = TRUE),\n             data = hurricane_df)\nsummary(d2_fit)\n\n\nCall:\nlm(formula = damage ~ poly(year, 2, raw = TRUE), data = hurricane_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24221 -13950  -5211   3268  96126 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                -2.857e+08  4.853e+08  -0.589    0.562\npoly(year, 2, raw = TRUE)1  2.838e+05  4.839e+05   0.587    0.563\npoly(year, 2, raw = TRUE)2 -7.048e+01  1.206e+02  -0.584    0.565\n\nResidual standard error: 27980 on 22 degrees of freedom\nMultiple R-squared:  0.09059,   Adjusted R-squared:  0.007915 \nF-statistic: 1.096 on 2 and 22 DF,  p-value: 0.3519\n\n\n\nd2_fit &lt;- lm(damage ~ poly(year, 2, raw = FALSE),\n             data = hurricane_df)\nsummary(d2_fit)\n\n\nCall:\nlm(formula = damage ~ poly(year, 2, raw = FALSE), data = hurricane_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24221 -13950  -5211   3268  96126 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)                    16855       5596   3.012  0.00641 **\npoly(year, 2, raw = FALSE)1    38056      27979   1.360  0.18756   \npoly(year, 2, raw = FALSE)2   -16351      27979  -0.584  0.56490   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 27980 on 22 degrees of freedom\nMultiple R-squared:  0.09059,   Adjusted R-squared:  0.007915 \nF-statistic: 1.096 on 2 and 22 DF,  p-value: 0.3519\n\n\n\n\n\n\n\n\n(optional) Orthogonal Bases\n\n\n\n\n\nThe use of poly in regression tasks has a choice\n\nraw = TRUE employs the power function basis\n\n\\[y = \\beta_{0} + \\beta_{1}x + \\beta_{2}x^{2} + \\beta_{3}x^{3} + ...\\] but internal calculations are highly redundant and prone to be unstable.\n\nraw = FALSE (the default setting) employs orthogonal polynomials to avoid the redundancy and aim for numerical stability. The Chebyshev Polynomials\n\n\\[\\{1, x, 2x^{2} - 1, 4x^{3} - 3x, ... \\}\\] are an example of a basis of orthogonal polynomials, but poly employs an algorithm whose weights are based on the explanatory variable."
  },
  {
    "objectID": "posts/11_nonlinear_regression/11_nonlinear_regression.html#degree-3",
    "href": "posts/11_nonlinear_regression/11_nonlinear_regression.html#degree-3",
    "title": "11: Nonlinear Regression",
    "section": "Degree 3",
    "text": "Degree 3\n\nd3_fit &lt;- lm(damage ~ poly(year, 3, raw = FALSE),\n             data = hurricane_df)\nsummary(d3_fit)\n\n\nCall:\nlm(formula = damage ~ poly(year, 3, raw = FALSE), data = hurricane_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-22618 -17007  -7188   4362  95214 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)                    16855       5693   2.961  0.00746 **\npoly(year, 3, raw = FALSE)1    38056      28465   1.337  0.19554   \npoly(year, 3, raw = FALSE)2   -16351      28465  -0.574  0.57178   \npoly(year, 3, raw = FALSE)3    14414      28465   0.506  0.61788   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28460 on 21 degrees of freedom\nMultiple R-squared:  0.1016,    Adjusted R-squared:  -0.02679 \nF-statistic: 0.7913 on 3 and 21 DF,  p-value: 0.5123"
  },
  {
    "objectID": "posts/11_nonlinear_regression/11_nonlinear_regression.html#degree-4",
    "href": "posts/11_nonlinear_regression/11_nonlinear_regression.html#degree-4",
    "title": "11: Nonlinear Regression",
    "section": "Degree 4",
    "text": "Degree 4\n\nd4_fit &lt;- lm(damage ~ poly(year, 4, raw = FALSE),\n             data = hurricane_df)\nsummary(d4_fit)\n\n\nCall:\nlm(formula = damage ~ poly(year, 4, raw = FALSE), data = hurricane_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-29270 -11311  -2570   3618  86557 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)                    16855       5541   3.042  0.00644 **\npoly(year, 4, raw = FALSE)1    38056      27706   1.374  0.18477   \npoly(year, 4, raw = FALSE)2   -16351      27706  -0.590  0.56169   \npoly(year, 4, raw = FALSE)3    14414      27706   0.520  0.60861   \npoly(year, 4, raw = FALSE)4    40779      27706   1.472  0.15662   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 27710 on 20 degrees of freedom\nMultiple R-squared:  0.1894,    Adjusted R-squared:  0.02724 \nF-statistic: 1.168 on 4 and 20 DF,  p-value: 0.3546"
  },
  {
    "objectID": "posts/11_nonlinear_regression/11_nonlinear_regression.html#linear-revisited",
    "href": "posts/11_nonlinear_regression/11_nonlinear_regression.html#linear-revisited",
    "title": "11: Nonlinear Regression",
    "section": "Linear Revisited",
    "text": "Linear Revisited\n\nd1_fit &lt;- lm(damage ~ poly(year, 1, raw = FALSE),\n             data = hurricane_df)\nsummary(d1_fit)\n\n\nCall:\nlm(formula = damage ~ poly(year, 1, raw = FALSE), data = hurricane_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-25764 -11413  -6317  -1132  99721 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)                   16855       5515   3.056   0.0056 **\npoly(year, 1, raw = FALSE)    38056      27576   1.380   0.1808   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 27580 on 23 degrees of freedom\nMultiple R-squared:  0.07647,   Adjusted R-squared:  0.03632 \nF-statistic: 1.904 on 1 and 23 DF,  p-value: 0.1808"
  },
  {
    "objectID": "posts/11_nonlinear_regression/11_nonlinear_regression.html#graphing-polynomial-fits",
    "href": "posts/11_nonlinear_regression/11_nonlinear_regression.html#graphing-polynomial-fits",
    "title": "11: Nonlinear Regression",
    "section": "Graphing Polynomial Fits",
    "text": "Graphing Polynomial Fits\n\nnew_data = data.frame(year = hurricane_df$year)\nhurricane_wide &lt;- hurricane_df |&gt;\n  mutate(d1_preds = predict(d1_fit, new_data),\n         d2_preds = predict(d2_fit, new_data),\n         d3_preds = predict(d3_fit, new_data),\n         d4_preds = predict(d4_fit, new_data))\n\n\nhurricane_long &lt;- hurricane_wide |&gt;\n  pivot_longer(cols = matches(\"preds\"),\n               names_to = \"model_id\",\n               values_to = \"preds\")\n\n\nhurricane_long |&gt;\n  ggplot(aes(x = year, y = damage)) +\n  geom_point(color = \"gray40\", size = 4) +\n  geom_line(aes(x = year, y = preds,\n                color = model_id)) +\n  labs(title = \"US Hurricanes\",\n       subtitle = cor2text(hurricane_df$year,\n                           hurricane_df$damage),\n       caption = \"SML 201\",\n       x = \"year\", y = \"total damage (millions of dollars)\") +\n  scale_x_continuous(breaks = 1994:2018,\n                     labels = as.character(1994:2018)) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html",
    "title": "12: Binomial Distribution",
    "section": "",
    "text": "library(\"gt\")        #great tables\nlibrary(\"patchwork\") #easy side-by-side plots\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\""
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#start",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#start",
    "title": "12: Binomial Distribution",
    "section": "Start",
    "text": "Start\n\n\n\nGoal: Introduce concepts of probability\nObjective: Compute probabilities with the discrete binomial distribution"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#terminology",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#terminology",
    "title": "12: Binomial Distribution",
    "section": "Terminology",
    "text": "Terminology\n\nDefinitionForkFrameworks\n\n\nFor event \\(X\\) with outcomes \\(x\\), the probability \\(P(x)\\) has properties\n\neach probability is between zero and one (inclusive inequalities)\n\n\\[0 \\leq P(x) \\leq 1\\]\n\nall probabilities add up to one (i.e. 100 percent)\n\n\\[\\sum_{x \\in X} P(x) = 1\\]\n\n\nIf a pair of parents have two children, should the genders of the children be represented as\n\n{two girls, mixed, two boys}\n\nOR\n\n{{girl, girl}, {girl, boy}, {boy, girl}, {boy, boy}}\n\n\n\n\nClassical probability: fraction from known observations\n\nexample: With two children, the probability of having one girl and one boy is \\(\\frac{2}{4}\\)\n\nFrequentist probability: If we could repeat an experiment infinitely many iterations, what would the proportion be?\n\nexample: Surveying all families with multiple children, among the first two children, the probability of having one girl and one boy is converging toward 50 percent.\nSML 201\n\nBayesian probability: A posterior distribution is the update from multiplying likelihoods to the prior distribution.\n\nexample: If the first child is a girl, what is the probability that the second child is a boy?\nSML 320"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#coins",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#coins",
    "title": "12: Binomial Distribution",
    "section": "Coins",
    "text": "Coins\n\nFair Coin3 Coins4 Coins5 CoinsCodeExactly\n\n\nFor a flip of a fair coin, the probability of observing “Heads” is 50 percent. The complement, observing “Tails”, also has a probability of 50 percent.\n\\[P(H) = 0.50, \\quad P(T) = 0.50\\] \n\n\n\n\nc(\"HHH\", \"THH\", \"HTH\", \"TTH\", \"HHT\", \"THT\", \"HTT\", \"TTT\")\n\n\n\n\n\n\nc(\"HHHH\", \"THHH\", \"HTHH\", \"TTHH\", \"HHTH\", \"THTH\", \"HTTH\", \"TTTH\", \n\"HHHT\", \"THHT\", \"HTHT\", \"TTHT\", \"HHTT\", \"THTT\", \"HTTT\", \"TTTT\"\n)\n\n\n\n\n\n\nc(\"HHHHH\", \"THHHH\", \"HTHHH\", \"TTHHH\", \"HHTHH\", \"THTHH\", \"HTTHH\", \n\"TTTHH\", \"HHHTH\", \"THHTH\", \"HTHTH\", \"TTHTH\", \"HHTTH\", \"THTTH\", \n\"HTTTH\", \"TTTTH\", \"HHHHT\", \"THHHT\", \"HTHHT\", \"TTHHT\", \"HHTHT\", \n\"THTHT\", \"HTTHT\", \"TTTHT\", \"HHHTT\", \"THHTT\", \"HTHTT\", \"TTHTT\", \n\"HHTTT\", \"THTTT\", \"HTTTT\", \"TTTTT\")\n\n\n\n\n\n# 3 coins\ncoin &lt;- c(\"H\", \"T\")\ndf &lt;- data.frame(expand.grid(coin, coin, coin)) |&gt;\n  tidyr::unite(\"obs\", c(\"Var1\", \"Var2\", \"Var3\"),\n               sep = \"\", remove = FALSE)\ndput(df$obs)\n\n# 4 coins\ndf &lt;- data.frame(expand.grid(coin, coin, coin, coin)) |&gt;\n  tidyr::unite(\"obs\", c(\"Var1\", \"Var2\", \"Var3\", \"Var4\"),\n               sep = \"\", remove = FALSE)\ndput(df$obs)\n\n# 5 coins\ndf &lt;- data.frame(expand.grid(coin, coin, coin, coin, coin)) |&gt;\n  tidyr::unite(\"obs\", c(\"Var1\", \"Var2\", \"Var3\", \"Var4\", \"Var5\"),\n               sep = \"\", remove = FALSE)\ndput(df$obs)\n\n\n\n\nExample: For 6 coin flips, in how many permutations do we observe exactly 2 heads?\none instance:\n\n\\[{H, H, T, T, T, T}\\]\n\nanswer:\n\n\\[\\frac{6!}{2!4!} = 15\\]"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#choose",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#choose",
    "title": "12: Binomial Distribution",
    "section": "Choose",
    "text": "Choose\n\nDefinitionDiscussionBinomial Distribution\n\n\n\n\n\\[\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\]\n\nsaid ``n choose k’’\nnote \\(0! = 1\\) (to avoid dividing by zero)\n\n\n\n\n\nfrom The Simpsons\n\n\n\n\n\n\n\n\n\nIn SML 201, you will never be directly asked for the distinction between\n\npermutations: number of arrangements when order matters\ncombinations: number of arrangements when order does not matter\n\nExample: To open a combination lock, you need to apply the correct permutation\nBut \\(\\binom{n}{k}\\) is “nCr” on a calculator!\n\nThis choose operator keeps track of the number of permutations in a certain combination\n\n\n\n\n\n\ncombination lock\n\n\n\n\n\n\n\\[P(x = k) = \\binom{n}{k} p^{k}(1-p)^{n-k}\\]\n\n\\(0 \\leq k \\leq n\\), where \\(n\\) and \\(k\\) are whole numbers\n\\(0 \\leq p \\leq 1\\)"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#example-squirtle",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#example-squirtle",
    "title": "12: Binomial Distribution",
    "section": "Example: Squirtle",
    "text": "Example: Squirtle\n\nSetupBattle ArenaMathRDistribution\n\n\n\n\nHistorically, Squirtle defeats Charizard 32% of the time. If there are 5 battles, what is the probability that Squirtle wins exactly 2 times?\n\n\n\n\n\n\nSquirtle the underdog\n\n\n\n\n\n\n\n\nc(\"SSSSS\", \"CSSSS\", \"SCSSS\", \"CCSSS\", \"SSCSS\", \"CSCSS\", \"SCCSS\", \n\"CCCSS\", \"SSSCS\", \"CSSCS\", \"SCSCS\", \"CCSCS\", \"SSCCS\", \"CSCCS\", \n\"SCCCS\", \"CCCCS\", \"SSSSC\", \"CSSSC\", \"SCSSC\", \"CCSSC\", \"SSCSC\", \n\"CSCSC\", \"SCCSC\", \"CCCSC\", \"SSSCC\", \"CSSCC\", \"SCSCC\", \"CCSCC\", \n\"SSCCC\", \"CSCCC\", \"SCCCC\", \"CCCCC\")\n\n\n\nBut these observations have different weights!\nThe outcomes are not uniformly distributed\n\n\n\n\\[P(k = 2) = \\overbrace{\\binom{5}{2}}^{\\text{number of permutations}}\\underbrace{(0.32)^{2}}_{\\text{Squirtle wins}}\\overbrace{(0.68)^{3}}^{\\text{Charizard wins}}\\]\n\n\nHistorically, Squirtle defeats Charizard 32% of the time. If there are 5 battles, what is the probability that Squirtle wins exactly 2 times?\n\n\\(n = 5\\)\n\\(k = 2\\)\n\\(p = 0.32\\)\n\n\ndbinom(2, 5, 0.32)\n\n[1] 0.3219784\n\n\n\n\n\nk_obs &lt;- 2\nn     &lt;- 5\np     &lt;- 0.32\nlabels &lt;- TRUE\n\n# make data frame\nk_vals &lt;- 0:n\npk     &lt;- dbinom(k_vals, n, p)\nk_bool &lt;- k_vals %in% k_obs\ndf_binom &lt;- data.frame(k_vals, pk, k_bool)\n\n# compute requested probability\nanswer_prob = round(sum(dbinom(k_obs, n, p)), 4)\n\n# define bar plot\nthis_plot &lt;- if(labels){\n  df_binom |&gt;\n    ggplot(aes(x = factor(k_vals), y = pk, color = k_bool, fill = k_bool)) +\n    geom_bar(stat = \"identity\") +\n    geom_label(aes(x = factor(k_vals), y = pk, label = round(pk, 4)),\n               color = \"black\", fill = \"white\") +\n    labs(subtitle = paste0(\"n = \", n, \", k = \", list(k_obs), \", p = \", p, \", P(k = \", list(k_obs), \") = \", answer_prob),\n         caption = \"SML 201\",\n         y = \"probability\") +\n    theme(\n      legend.position = \"bottom\",\n      panel.background = element_blank()\n    )\n} else{\n  df_binom |&gt;\n    ggplot(aes(x = factor(k_vals), y = pk, color = k_bool, fill = k_bool)) +\n    geom_bar(stat = \"identity\") +\n    labs(subtitle = paste0(\"n = \", n, \", k = \", list(k_obs), \", p = \", p, \", P(k = \", list(k_obs), \") = \", answer_prob),\n         caption = \"SML 201\",\n         y = \"probability\") +\n    theme(\n      legend.position = \"bottom\",\n      panel.background = element_blank()\n    )\n}\n\n# plot bar chart\nthis_plot +\n  \n  # particular to this example\n  scale_color_manual(values = c(\"black\", \"#ca7721\")) +\n  scale_fill_manual(values = c(\"gray70\", \"#297383\")) +\n  labs(title = \"Squirtle Wins\", \n       x = \"wins\", y = \"probability\")"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#example-charizard",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#example-charizard",
    "title": "12: Binomial Distribution",
    "section": "Example: Charizard",
    "text": "Example: Charizard\n\nSetupMathRFunctionDistribution\n\n\n\n\nHistorically, Charizard defeats Squirtle 68% of the time. If there are 5 battles, what is the probability that Charizard wins exactly 3 times?\n\n\n\n\n\n\nCharizard the favored\n\n\n\n\n\n\n\\[P(k = 3) = \\overbrace{\\binom{5}{3}}^{\\text{number of permutations}}\\underbrace{(0.68)^{3}}_{\\text{Charizard wins}}\\overbrace{(0.32)^{2}}^{\\text{Squirtle wins}}\\]\n\n\nHistorically, Charizard defeats Squirtle 68% of the time. If there are 5 battles, what is the probability that Charizard wins exactly 3 times?\n\n\\(n = 5\\)\n\\(k = 3\\)\n\\(p = 0.68\\)\n\n\ndbinom(3, 5, 0.68)\n\n[1] 0.3219784\n\n\n\n\nLet us make a user-defined function to help us visualize the binomial distribution in the future.\n\nvbinom &lt;- function(k_obs, n, p, labels = TRUE){\n# make data frame\nk_vals &lt;- 0:n\npk     &lt;- dbinom(k_vals, n, p)\nk_bool &lt;- k_vals %in% k_obs\ndf_binom &lt;- data.frame(k_vals, pk, k_bool)\n\n# compute requested probability\nanswer_prob = round(sum(dbinom(k_obs, n, p)), 4)\n\n# define bar plot\nthis_plot &lt;- if(labels){\n  df_binom |&gt;\n    ggplot(aes(x = factor(k_vals), y = pk, color = k_bool, fill = k_bool)) +\n    geom_bar(stat = \"identity\") +\n    geom_label(aes(x = factor(k_vals), y = pk, label = round(pk, 4)),\n               color = \"black\", fill = \"white\") +\n    labs(subtitle = paste0(\"n = \", n, \", k = \", list(k_obs), \", p = \", p, \", P(k = \", list(k_obs), \") = \", answer_prob),\n         caption = \"SML 201\",\n         y = \"probability\") +\n    theme(\n      legend.position = \"bottom\",\n      panel.background = element_blank()\n    )\n} else{\n  df_binom |&gt;\n    ggplot(aes(x = factor(k_vals), y = pk, color = k_bool, fill = k_bool)) +\n    geom_bar(stat = \"identity\") +\n    labs(subtitle = paste0(\"n = \", n, \", k = \", list(k_obs), \", p = \", p, \", P(k = \", list(k_obs), \") = \", answer_prob),\n         caption = \"SML 201\",\n         y = \"probability\") +\n    theme(\n      legend.position = \"bottom\",\n      panel.background = element_blank()\n    )\n}\n\n# plot bar chart\nthis_plot\n}\n\n\n\n\nvbinom(3, 5, 0.68) +\n  # particular to this example\n  scale_color_manual(values = c(\"black\", \"#297383\")) +\n  scale_fill_manual(values = c(\"gray70\", \"#ca7721\")) +\n  labs(title = \"Charizard Wins\", \n       x = \"wins\", y = \"probability\")"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#example-boba",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#example-boba",
    "title": "12: Binomial Distribution",
    "section": "Example: Boba",
    "text": "Example: Boba\n\nSetupExactlyAt Most\n\n\n\n\nThere are 4 parking spaces in front of the boba place. Suppose that each parking space tends to be occupied about 57 percent of the time.\n\n\n\n\n\n\nboba!\n\n\n\n\n\n\nThere are 4 parking spaces in front of the boba place. Suppose that each parking space tends to be occupied about 57 percent of the time. What is the probability that exactly 3 of the parking spaces are open?\n\ndbinom(3, 4, 0.43)\n\n[1] 0.181276\n\n\n\nvbinom(3, 4, 0.43) +\n  labs(title = \"Exactly 3 parking spaces\",\n       x = \"open parking spaces\")\n\n\n\n\n\n\n\n\n\n\nThere are 4 parking spaces in front of the boba place. Suppose that each parking space tends to be occupied about 57 percent of the time. What is the probability that at most 2 of the parking spaces are open?\n\nsum(dbinom(0:2, 4, 0.43))\n\n[1] 0.784536\n\n\n\nvbinom(0:2, 4, 0.43) +\n  labs(title = \"Probability of at most 2 open parking spaces\",\n       x = \"open parking spaces\")\n\n\n\n\n\n\n\n\n\npbinom(2, 4, 0.43) #same as sum(dbinom(0:2, 4, 0.43))\n\n[1] 0.784536"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#cumulative-probability",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#cumulative-probability",
    "title": "12: Binomial Distribution",
    "section": "Cumulative Probability",
    "text": "Cumulative Probability\nThe pbinom function in R computes a cumulative probability (i.e. adds up probabilities).\n\\[P(i \\leq k) = \\sum_{i = 1}^{k}\\binom{n}{i}p^{i}(1-p)^{n-i}\\]\nIn R,\n\npbinom(k, n, p)\n# is the same as\nsum(dbinom(0:k, n, p))"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#example-parking",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#example-parking",
    "title": "12: Binomial Distribution",
    "section": "Example: Parking",
    "text": "Example: Parking\n\nSetupMore Than\n\n\n\n\nThere are 32 parking spaces on a certain stretch of Nassau Street. Suppose that each parking space tends to be occupied about 81 percent of the time.\n\n\n\n\n\n\nNassau Street\n\n\n\n\n\n\nThere are 32 parking spaces on a certain stretch of Nassau Street. Suppose that each parking space tends to be occupied about 81 percent of the time. What is the probability that more than 5 of the parking spaces are open?\n\nvbinom(6:32, 32, 0.19, labels = FALSE) +\n  labs(title = \"Probability of more than 5 open parking spaces\",\n       x = \"open parking spaces\")\n\n\n\n\n\n\n\n\nEach of the following are equivalent ways to compute the requested probability (“more than 5”))\n\nsum(dbinom(6:32, 32, 0.19))\n\n[1] 0.5853578\n\n1 - pbinom(5, 32, 0.19)\n\n[1] 0.5853578\n\npbinom(5, 32, 0.19, lower.tail = FALSE)\n\n[1] 0.5853578"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#venetian-elections",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#venetian-elections",
    "title": "12: Binomial Distribution",
    "section": "Venetian Elections",
    "text": "Venetian Elections\n\n\n\n\n\nElection of Doge\n\n\n\n\n\n\nleft: “Sorting process for the election of the Doge of Venice”, Jacob von Sandrart, 1687\n\nimage credit: The Ballot Boy\n\nright: elementary balls-and-urns probability setup\n\nimage credit: Professor Joyce, Clark University\n\n\n\n\n\n\n\n\nurns"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#discrete",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#discrete",
    "title": "12: Binomial Distribution",
    "section": "Discrete",
    "text": "Discrete\nIn R, discrete sampling is handled with sample.\n\nsample(LETTERS[1:6], size = 5)\n\n[1] \"F\" \"C\" \"D\" \"A\" \"E\"\n\n\nAsking for too many observations here leads to an error.\n\n#sample(LETTERS[1:6], size = 7)\n\nBe default, the sampling is performed without replacement. Sometimes we will need to perform sampling with replacement.\n\nsample(LETTERS[1:6], size = 7, replace = TRUE)\n\n[1] \"C\" \"E\" \"D\" \"E\" \"A\" \"C\" \"F\"\n\n\nSometimes, we want to suppress randomization. More precisely, we want to replicate the creation of the pseudo-random numbers. This is useful for working on simulations with groups of researchers (i.e. get the same answers).\n\nset.seed(201)\nsample(LETTERS[1:6], size = 7, replace = TRUE)\n\n[1] \"C\" \"F\" \"B\" \"A\" \"E\" \"F\" \"E\""
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#continuous",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#continuous",
    "title": "12: Binomial Distribution",
    "section": "Continuous",
    "text": "Continuous\nThe most common function for pseudo-random number generation in R is runif\n\npronounced “r unif” (i.e. random numbers from a uniform distribution)\ndefault: random numbers between zero and one\n\n\nrunif(5)\n\n[1] 0.09931186 0.28766776 0.47009467 0.67783012 0.60524130\n\n\n\n\n\n\n\n\n(optional) Math: Discrete versus Continuous\n\n\n\n\n\nIn math classes, we tend to have the students pick up simplistic definitions:\n\ndiscrete data: can be written as a list\n\nmath: “countable”\n\ncontinous data: function can be drawn without lifting your pencil\n\nmath: “uncountable”\n\n\nIf anyone is curious, here are more rigorous definitions from Real Analysis\n\nA set \\(X\\) is discrete if \\(\\exists \\epsilon\\in\\mathbb{R}\\) such that\n\n\\[\\forall x\\in X, \\exists c \\in B(x,\\epsilon) \\text{ such that } c \\in \\mathbb{R}/X\\] where \\(B\\) is a ball of center \\(x\\) and radius \\(\\epsilon\\).\n\nA set \\(X\\) is continuous if\n\n\\[\\forall x \\in X, \\forall\\epsilon \\in \\mathbb{R}, \\exists c \\in B(x,\\epsilon) \\text{ such that } c\\in X\\]"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#not-uniform",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#not-uniform",
    "title": "12: Binomial Distribution",
    "section": "Not Uniform",
    "text": "Not Uniform\nThe previous sampling examples were chosing from uniform distributions (i.e. each outcome was equally likely). We will have situations that are not uniform. For example\n\\[P(red) = 1/3, \\quad P(green) = 2/3\\]\n\nsample(c(\"red\", \"green\"), size = 20, replace = TRUE, prob = c(1/3, 2/3))\n\n [1] \"green\" \"green\" \"green\" \"green\" \"green\" \"green\" \"green\" \"red\"   \"green\"\n[10] \"green\" \"green\" \"green\" \"red\"   \"red\"   \"green\" \"green\" \"green\" \"red\"  \n[19] \"green\" \"green\""
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#weighted-mean",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#weighted-mean",
    "title": "12: Binomial Distribution",
    "section": "Weighted Mean",
    "text": "Weighted Mean\n\nDefinitionExample\n\n\n\nsample mean\n\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}\\]\n\nweighted mean\n\n\\[\\bar{x} = \\frac{\\sum_{i=1}^{n} w_{i}*x_{i}}{\\sum_{i=1}^{n} w_{i}}\\]\n\n\nThis banana slicer is popular on Amazon. We will replicate the calculation for the average rating.\n\n\nThe average rating is not \\[\\frac{1 + 2 + 3 + 4 + 5}{5}\\] but rather\n\nstars &lt;- 1:5\nprobs &lt;- c(4, 4, 8, 13, 71)/100\n\n# weighted mean\nsum(probs * stars) / sum(probs)\n\n[1] 4.43\n\n\n\n\n\n\n\n\nbanana slicer"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#voter-info",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#voter-info",
    "title": "12: Binomial Distribution",
    "section": "Voter Info",
    "text": "Voter Info\n\nset.seed(201)\ncollege &lt;- c(\"Butler\", \"Forbes\", \"Mathey\", \"New College West\", \"Rockefeller\", \"Whitman\", \"Yeh College\")\nn &lt;- length(college)\ndelegates &lt;- sample(11:20, size = n, replace = FALSE)\ntaco_prop &lt;- round(runif(n), 2)\n\n# if you want to experiment with this example later, you can explicitly set the weights here\n# college &lt;- c(\"Butler\", \"Forbes\", \"Mathey\", \"New College West\", \"Rockefeller\", \"Whitman\", \"Yeh College\")\n# delegates &lt;- c(13, 16, 17, 12, 11, 15, 18)\n# taco_prop &lt;- c(27, 10, 29, 57, 68, 61, 47) / 100\n\ndf_election &lt;- data.frame(\n  college, delegates, taco_prop\n)"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#complement",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#complement",
    "title": "12: Binomial Distribution",
    "section": "Complement",
    "text": "Complement\n\ndf_election &lt;- df_election |&gt;\n  mutate(sandwich_prop = 1 - taco_prop)"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#simulate-one-election",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#simulate-one-election",
    "title": "12: Binomial Distribution",
    "section": "Simulate One Election",
    "text": "Simulate One Election\n\nset.seed(201)\ndf_election &lt;- df_election |&gt;\n  rowwise() |&gt;\n  mutate(picks = sample(c(\"Taco\", \"Sandwich\"), \n                        size = 1,\n                        prob = c(taco_prop, sandwich_prop)))"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#count-votes",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#count-votes",
    "title": "12: Binomial Distribution",
    "section": "Count Votes",
    "text": "Count Votes\nIn this example, the votes are tabulated as a weighted mean.\n\ndf_election &lt;- df_election |&gt;\n  mutate(picks_bool = ifelse(picks == \"Taco\", 1, 0))\n\ntaco_share &lt;- sum(df_election$delegates * df_election$picks_bool) / sum(df_election$delegates)\n\nprint(taco_share)\n\n[1] 0.254902"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#classify-result",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#classify-result",
    "title": "12: Binomial Distribution",
    "section": "Classify Result",
    "text": "Classify Result\n\nelection_result &lt;- case_when(\n  taco_share &lt; 0.5 ~ \"Sandwich Won\",\n  taco_share &gt; 0.5 ~ \"Taco Won\",\n  .default = \"tie\"\n)\n\nprint(election_result)\n\n[1] \"Sandwich Won\""
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#for-loops",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#for-loops",
    "title": "12: Binomial Distribution",
    "section": "for loops",
    "text": "for loops\nA common programming tool is the for loop. We tend to use letters \\(i\\), \\(j\\), and \\(k\\) historically as counter variables.\n\nN &lt;- 7 #number of iterations\n\n\nfor(i in 1:N){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n\n\n\nfor(i in 1:N){\n  print(i^2)\n}\n\n[1] 1\n[1] 4\n[1] 9\n[1] 16\n[1] 25\n[1] 36\n[1] 49\n\n\n\nfor(i in 1:N){\n  print(LETTERS[1:i])\n}\n\n[1] \"A\"\n[1] \"A\" \"B\"\n[1] \"A\" \"B\" \"C\"\n[1] \"A\" \"B\" \"C\" \"D\"\n[1] \"A\" \"B\" \"C\" \"D\" \"E\"\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\"\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\""
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#election-simulation",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#election-simulation",
    "title": "12: Binomial Distribution",
    "section": "Election Simulation",
    "text": "Election Simulation\n\nN &lt;- 1337 #number of simulations\n\n# pre-allocate space for storing results\ntaco_share_vec &lt;- rep(NA, N)\n\nfor(i in 1:N){\n  df_election &lt;- df_election |&gt;\n  rowwise() |&gt;\n    mutate(picks = sample(c(\"Taco\", \"Sandwich\"), \n                          size = 1,\n                          prob = c(taco_prop, sandwich_prop)))\n  df_election &lt;- df_election |&gt;\n    mutate(picks_bool = ifelse(picks == \"Taco\", 1, 0))\n  \n  taco_share_vec[i] &lt;- sum(df_election$delegates * df_election$picks_bool) / sum(df_election$delegates)\n}\n\ndf_simulation &lt;- data.frame(iter = 1:N, taco_share_vec)\n\n\n\n\n\n\n\nOn the computer processing of simulations\n\n\n\n\n\nIf your computer is older\n\ntook a while to draw maps\ntook a while to produce network visuals\n\nyou may reduce the number of iterations from 1337 to 201 (for example)."
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#visualize-the-simulation",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#visualize-the-simulation",
    "title": "12: Binomial Distribution",
    "section": "Visualize the Simulation",
    "text": "Visualize the Simulation\nNow, taco_share_vec is itself a numerical variable, so we can visualize its distribution with a histogram.\n\ndf_simulation |&gt;\n  ggplot(aes(x = taco_share_vec)) +\n  geom_histogram(bins = 20, color = princeton_black, fill = princeton_orange) +\n  geom_vline(xintercept = 0.5, color = \"#2A668F\", \n             linewidth = 3, linetype = 3) +\n  labs(title = \"Taco versus Sandwich!\",\n       subtitle = \"Simulated delegation voting of the residential colleges\",\n       caption = \"SML 201\",\n       x = \"taco vote share\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/12_binomial_distribution/12_binomial_distribution.html#simulation-statistics",
    "href": "posts/12_binomial_distribution/12_binomial_distribution.html#simulation-statistics",
    "title": "12: Binomial Distribution",
    "section": "Simulation Statistics",
    "text": "Simulation Statistics\nTo wrap up this example,\n\n“Taco” had this vote share on average:\n\n\nmean(df_simulation$taco_share_vec)\n\n[1] 0.4101442\n\n\nTo compute how often “Taco” would win probabilistically, we could\n\ncount the iterations where taco_share_vec &gt; 0.50\ndivide by the number of iterations\n\n\nmean(df_simulation$taco_share_vec &gt; 0.50)\n\n[1] 0.2984293\n\n\nPreview: the standard error is the standard deviation of a sampling distribution.\n\nsd(df_simulation$taco_share_vec)\n\n[1] 0.1734641"
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html",
    "href": "posts/13_normal_distribution/13_normal_distribution.html",
    "title": "13: Normal Distribution",
    "section": "",
    "text": "Libraries and Helper Functions\n\n\n\n\n\n\nlibrary(\"gt\")        #great tables\nlibrary(\"janitor\")   #helps with counts and proportions\nlibrary(\"patchwork\") #side-by-side plots\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n\n# helper function\nvnorm &lt;- function(x, mu = 0, sigma = 1, section = \"lower\"){\n  \n  # bell curve\n  x_vals &lt;- seq(mu - 4*sigma, mu + 4*sigma, length.out = 201)\n  y_vals &lt;- dnorm(x_vals, mu, sigma)\n  df_for_graph &lt;- data.frame(x_vals, y_vals)\n\n  # outline shaded regions\n  if(length(x) == 1){\n    shade_left &lt;- rbind(c(x[1],0), df_for_graph |&gt;\n                        filter(x_vals &lt; x[1]))\n    shade_right &lt;- rbind(c(x[1],0), df_for_graph |&gt;\n                        filter(x_vals &gt; x[1]))\n  }\n  if(length(x) == 2){\n    shade_between &lt;- rbind(c(x[1],0),\n                       df_for_graph |&gt;\n                         filter(x_vals &gt; x[1] &\n                                  x_vals &lt; x[2]),\n                       c(x[2],0))\n    shade_tails &lt;- rbind(df_for_graph |&gt;\n                        filter(x_vals &lt; x[1]),\n                     c(x[1],0),\n                     c(x[2],0),\n                     df_for_graph |&gt;\n                        filter(x_vals &gt; x[2]))\n  }\n  \n  # compute requested probability\n  # prob_val &lt;- case_when(\n  #   section == \"upper\" ~ round(1 - pnorm(x,mu,sigma), 4),\n  #   section == \"between\" ~ round(diff(pnorm(x,mu,sigma), 4)),\n  #   section == \"tails\" ~ round(1 - diff(pnorm(x,mu,sigma), 4)),\n  #   .default = round(pnorm(x,mu,sigma), 4)\n  # )\n  \n  if(section == \"lower\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_left,\n                   fill = \"#E77500\",) +\n      # geom_segment(aes(x = x, y = 0,\n      #                  xend = x, yend = dnorm(x,mu,sigma))) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(pnorm(x,mu,sigma), 4)\n  }\n  if(section == \"upper\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_right,\n                   fill = \"#E77500\",) +\n      # geom_segment(aes(x = x, y = 0,\n      #                  xend = x, yend = dnorm(x,mu,sigma))) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- 1 - round(pnorm(x,mu,sigma), 4)\n  }\n  if(section == \"between\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_between,\n                   fill = \"#E77500\",) +\n     #  geom_segment(aes(x = x[1], y = 0,\n     #                   xend = x[1], yend = dnorm(x[1],mu,sigma)))\n     # +\n     #  geom_segment(aes(x = x[2], y = 0,\n     #                   xend = x[2], yend = dnorm(x[2],mu,sigma))) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(diff(pnorm(x,mu,sigma)), 4)\n  }\n  if(section == \"tails\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_tails,\n                   fill = \"#E77500\",) +\n     #  geom_segment(aes(x = x[1], y = 0,\n     #                   xend = x[1], yend = dnorm(x[1],mu,sigma)))\n     # +\n     #  geom_segment(aes(x = x[2], y = 0,\n     #                   xend = x[2], yend = dnorm(x[2],mu,sigma))) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(1 - diff(pnorm(x,mu,sigma)), 4)\n  }\n  \n  # plot bell curve\n  bell_curve + \n    labs(subtitle = paste0(\"Probability: \", prob_val),\n         caption = \"SML 201\", y = \"\") +\n    theme_minimal()\n}\n\n\ngraph_noise &lt;- function(x,y){\n  this_df &lt;- data.frame(x = x, y = y)\n  lin_fit &lt;- lm(y ~ x, data = this_df)\n  coef_det &lt;- summary(lin_fit)$adj.r.squared\n  \n  this_df |&gt;\n    ggplot(aes(x,y)) +\n    geom_point(color = princeton_orange) +\n    geom_smooth(formula = \"y ~ x\",\n                method = \"lm\",\n                se = TRUE) +\n    labs(title = \"The Signal and the Noise\",\n         subtitle = paste0(\"Coefficient of Determination: \", round(coef_det, 4)),\n         caption = \"SML 201\") +\n    theme_minimal()\n}"
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html#start",
    "href": "posts/13_normal_distribution/13_normal_distribution.html#start",
    "title": "13: Normal Distribution",
    "section": "Start",
    "text": "Start\n\n\n\nGoal: Introduce social science\nObjective: Compute probabilities with the normal distribution"
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html#frequentist-probability",
    "href": "posts/13_normal_distribution/13_normal_distribution.html#frequentist-probability",
    "title": "13: Normal Distribution",
    "section": "Frequentist Probability",
    "text": "Frequentist Probability\n\nFrequentist probability: If we could repeat an experiment infinitely many iterations, what would the proportion be?"
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html#for-loops",
    "href": "posts/13_normal_distribution/13_normal_distribution.html#for-loops",
    "title": "13: Normal Distribution",
    "section": "for loops",
    "text": "for loops\nA common programming tool is the for loop. We tend to use letters \\(i\\), \\(j\\), and \\(k\\) historically as counter variables.\n\nN &lt;- 7 #number of iterations\n\n\nfor(i in 1:N){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n\n\n\nfor(i in 1:N){\n  print(i^2)\n}\n\n[1] 1\n[1] 4\n[1] 9\n[1] 16\n[1] 25\n[1] 36\n[1] 49\n\n\n\nfor(i in 1:N){\n  print(LETTERS[1:i])\n}\n\n[1] \"A\"\n[1] \"A\" \"B\"\n[1] \"A\" \"B\" \"C\"\n[1] \"A\" \"B\" \"C\" \"D\"\n[1] \"A\" \"B\" \"C\" \"D\" \"E\"\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\"\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\"\n\n\n\nfor(i in 1:N){\n  print(paste0(\"When i = \", i, \" , the first \", i, \" LETTERS are \", \n         paste0(LETTERS[1:i], collapse = \"\")))\n}\n\n[1] \"When i = 1 , the first 1 LETTERS are A\"\n[1] \"When i = 2 , the first 2 LETTERS are AB\"\n[1] \"When i = 3 , the first 3 LETTERS are ABC\"\n[1] \"When i = 4 , the first 4 LETTERS are ABCD\"\n[1] \"When i = 5 , the first 5 LETTERS are ABCDE\"\n[1] \"When i = 6 , the first 6 LETTERS are ABCDEF\"\n[1] \"When i = 7 , the first 7 LETTERS are ABCDEFG\""
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html#election-simulation",
    "href": "posts/13_normal_distribution/13_normal_distribution.html#election-simulation",
    "title": "13: Normal Distribution",
    "section": "Election Simulation",
    "text": "Election Simulation\n\nN &lt;- 1337 #number of simulations\n\nstart_time &lt;- Sys.time()\n\n# pre-allocate space for storing results\ntaco_share_vec &lt;- rep(NA, N)\n\nfor(i in 1:N){\n  df_election &lt;- df_election |&gt;\n  rowwise() |&gt;\n    mutate(picks = sample(c(\"Taco\", \"Sandwich\"), \n                          size = 1,\n                          prob = c(taco_prop, sandwich_prop)))\n  df_election &lt;- df_election |&gt;\n    mutate(picks_bool = ifelse(picks == \"Taco\", 1, 0))\n  \n  taco_share_vec[i] &lt;- sum(df_election$delegates * df_election$picks_bool) / sum(df_election$delegates)\n}\n\ndf_simulation &lt;- data.frame(iter = 1:N, taco_share_vec)\n\nprint(Sys.time() - start_time)\n\nTime difference of 6.510849 secs\n\n\n\n\n\n\n\n\nOn the computer processing of simulations\n\n\n\n\n\nIf your computer is older\n\ntook a while to draw maps\ntook a while to produce network visuals\n\nyou may reduce the number of iterations from 1337 to 201 (for example)."
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html#visualize-the-simulation",
    "href": "posts/13_normal_distribution/13_normal_distribution.html#visualize-the-simulation",
    "title": "13: Normal Distribution",
    "section": "Visualize the Simulation",
    "text": "Visualize the Simulation\nNow, taco_share_vec is itself a numerical variable, so we can visualize its distribution with a histogram.\n\ndf_simulation |&gt;\n  ggplot(aes(x = taco_share_vec)) +\n  geom_histogram(bins = 20, color = princeton_black, fill = princeton_orange) +\n  geom_vline(xintercept = 0.5, color = \"#2A668F\", \n             linewidth = 3, linetype = 3) +\n  labs(title = \"Taco versus Sandwich!\",\n       subtitle = \"Simulated delegation voting of the residential colleges\",\n       caption = \"SML 201\",\n       x = \"taco vote share\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html#simulation-statistics",
    "href": "posts/13_normal_distribution/13_normal_distribution.html#simulation-statistics",
    "title": "13: Normal Distribution",
    "section": "Simulation Statistics",
    "text": "Simulation Statistics\nTo wrap up this example,\n\n“Taco” had this vote share on average:\n\n\nmean(df_simulation$taco_share_vec)\n\n[1] 0.4101442\n\n\nTo compute how often “Taco” would win probabilistically, we could\n\ncount the iterations where taco_share_vec &gt; 0.50\ndivide by the number of iterations\n\n\nmean(df_simulation$taco_share_vec &gt; 0.50)\n\n[1] 0.2984293\n\n\nPreview: the standard error is the standard deviation of a sampling distribution.\n\nsd(df_simulation$taco_share_vec)\n\n[1] 0.1734641"
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html#density-plot",
    "href": "posts/13_normal_distribution/13_normal_distribution.html#density-plot",
    "title": "13: Normal Distribution",
    "section": "Density Plot",
    "text": "Density Plot\nNow, taco_share_vec is itself a numerical variable, so we can visualize its distribution with a denisty plot.\n\ndf_simulation |&gt;\n  ggplot(aes(x = taco_share_vec)) +\n  geom_density(color = princeton_black, fill = princeton_orange) +\n  geom_vline(xintercept = 0.5, color = \"#2A668F\", \n             linewidth = 3, linetype = 3) +\n  labs(title = \"Taco versus Sandwich!\",\n       subtitle = \"Simulated delegation voting of the residential colleges\",\n       caption = \"SML 201\",\n       x = \"taco vote share\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html#examples",
    "href": "posts/13_normal_distribution/13_normal_distribution.html#examples",
    "title": "13: Normal Distribution",
    "section": "Examples",
    "text": "Examples\n\nFewerMoreBetweenCharacterize\n\n\n\n\nSuppose that the incubation period—that is, the time between being infected with the virus and showing symptoms—for Covid-19 is normally distributed with a mean of 8 days and a standard deviation of 3 days. Find the probability that a randomly selected case demonstrated symptoms in fewer than 7 days.\n\n\n\n\n\n\nCovid-19\n\n\n\nimage credit: University of Chicago\npnorm(7, 8, 3)\nvnorm(7, 8, 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGirl Scout Thin Mint cookies have a mean size of 0.25 ounces. Find the probability that one randomly selected cookie has a size of more than 0.27 ounces if the standard deviation is 0.03 ounces. Assume a normal distribution.\n\n\n\n\n\n\nThin Mints\n\n\n\npnorm(0.27, 0.25, 0.03, lower.tail = FALSE)\nvnorm(0.27, 0.25, 0.03, section = \"upper\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe cones in the eye detect light. The absorption rate of cones is normally distributed. In particular, the “green” cones have a mean of 535 nanometers and a standard deviation of 65 nanometers. If an incoming ray of light has wavelengths between 550 and 575 nanometers, calculate the percentage of that ray of light that will be absorbed by the green cones.\n\n\n\n\n\n\neye cones\n\n\n\npnorm(575, 535, 65) - pnorm(550, 535, 65)\nvnorm(c(550, 575), 535, 65, section = \"between\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose that the number of french fries in the batches at In-n-Out are normally distributed with a mean of 42 french fries and a standard deviation of 3.7 french fries. Your friend tells you that the In-n-Out employee is flirting with you if you end up with a french fry count in the top 5 percent. How should we characterize the top 5 percent of french fries?\n\n\n\n\n\n\nDouble Double\n\n\n\nqnorm(0.95, 42, 3.7)\nvnorm(qnorm(0.95, 42, 3.7), 42, 3.7, section = \"upper\")"
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html#toward-significance",
    "href": "posts/13_normal_distribution/13_normal_distribution.html#toward-significance",
    "title": "13: Normal Distribution",
    "section": "Toward Significance",
    "text": "Toward Significance\nTraditionally, scientists sought out extreme values among the 5 percent probability combined in the tails. What are the MCV levels for these regions?\n\nqnorm(c(0.025, 0.975), 90, 5)\n\n[1] 80.20018 99.79982\n\n\n\nvnorm(qnorm(c(0.025, 0.975), 90, 5), \n      90, 5, section = \"tails\") +\n  labs(title = \"Two-Tailed Significance\",\n       x = \"fL/cell\")"
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html#simulation-1",
    "href": "posts/13_normal_distribution/13_normal_distribution.html#simulation-1",
    "title": "13: Normal Distribution",
    "section": "Simulation",
    "text": "Simulation\nSuppose that we have 10000 patients whose MCV has mean \\(\\mu = 90\\) fL/cell with a standard deviation of \\(\\sigma = 5\\) fL/cell. From the simulation, what percentage of patients were in each diagnosis category (reference range, microcyctic anemia, macrocyctic anemia)?\n\nnum_patients &lt;- 1e5\ndf_mcv &lt;- data.frame(\n  id = 1:num_patients,\n  mcv = rnorm(num_patients, 90, 5)\n)\n\n\ndf_mcv &lt;- df_mcv |&gt;\n  mutate(diagnosis = case_when(\n    mcv &gt;= 96 ~ \"macrocyctic anemia\",\n    mcv &lt;= 80 ~ \"microcyctic anemia\",\n    .default = \"reference range\"\n  ))\n\n\n# janitor package helps find proportions\ndf_mcv |&gt;\n  tabyl(diagnosis) |&gt;\n  adorn_totals(\"row\") |&gt;\n  adorn_pct_formatting()\n\n          diagnosis      n percent\n macrocyctic anemia  11542   11.5%\n microcyctic anemia   2208    2.2%\n    reference range  86250   86.2%\n              Total 100000  100.0%"
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html#exogenous",
    "href": "posts/13_normal_distribution/13_normal_distribution.html#exogenous",
    "title": "13: Normal Distribution",
    "section": "Exogenous",
    "text": "Exogenous\nSuppose that we have 10000 patients from a different population whose MCV has mean \\(\\mu = 88\\) fL/cell with a standard deviation of \\(\\sigma = 3\\) fL/cell. From the simulation, what percentage of patients were in each diagnosis category (reference range, microcyctic anemia, macrocyctic anemia)?\n\ndf_mcv &lt;- df_mcv |&gt;\n  mutate(mcv2 = rnorm(num_patients, 88, 3))\n\n\ndf_mcv &lt;- df_mcv |&gt;\n  mutate(diagnosis2 = case_when(\n    mcv2 &gt;= 96 ~ \"macrocyctic anemia\",\n    mcv2 &lt;= 80 ~ \"microcyctic anemia\",\n    .default = \"reference range\"\n  ))\n\ndf_mcv |&gt;\n  tabyl(diagnosis2) |&gt;\n  adorn_totals(\"row\") |&gt;\n  adorn_pct_formatting()\n\n         diagnosis2      n percent\n macrocyctic anemia    396    0.4%\n microcyctic anemia    373    0.4%\n    reference range  99231   99.2%\n              Total 100000  100.0%"
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html#gaussian-white-noise",
    "href": "posts/13_normal_distribution/13_normal_distribution.html#gaussian-white-noise",
    "title": "13: Normal Distribution",
    "section": "Gaussian White Noise",
    "text": "Gaussian White Noise\n\nn &lt;- 201 #number of data points\nx_vals &lt;- 1:n\ny_vals &lt;- 0.201*x_vals + rnorm(n, 0, 3)\n\ngraph_noise(x_vals, y_vals)"
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html#polynomials",
    "href": "posts/13_normal_distribution/13_normal_distribution.html#polynomials",
    "title": "13: Normal Distribution",
    "section": "Polynomials",
    "text": "Polynomials\n\nn &lt;- 201 #number of data points\nx_vals &lt;- 1:n\ny_vals &lt;- (x_vals - n/2)^2 + rnorm(n, 0, 2*n)\n\ngraph_noise(x_vals, y_vals)"
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html#seasonal",
    "href": "posts/13_normal_distribution/13_normal_distribution.html#seasonal",
    "title": "13: Normal Distribution",
    "section": "Seasonal",
    "text": "Seasonal\n\nn &lt;- 201 #number of data points\nx_vals &lt;- 1:n\ny_vals &lt;- sin((2*pi/75)*x_vals) + rnorm(n, 0, 0.201)\n\ngraph_noise(x_vals, y_vals)"
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html#heteroscedasticity",
    "href": "posts/13_normal_distribution/13_normal_distribution.html#heteroscedasticity",
    "title": "13: Normal Distribution",
    "section": "Heteroscedasticity",
    "text": "Heteroscedasticity\nHeteroscedasticity is a situation where we can no longer assume that the variance is the same for all observations.\n\nn &lt;- 201 #number of data points\nx_vals &lt;- 1:n\ny_vals &lt;- 0.201*x_vals + 100 +\n  rnorm(n, 0, x_vals/10)\n\ngraph_noise(x_vals, y_vals) +\n  labs(title = \"Heteroscedasticity\")"
  },
  {
    "objectID": "posts/13_normal_distribution/13_normal_distribution.html#grading-curves",
    "href": "posts/13_normal_distribution/13_normal_distribution.html#grading-curves",
    "title": "13: Normal Distribution",
    "section": "Grading Curves",
    "text": "Grading Curves\n\nSetupScenario 1Scenario 2ComparisonCodes\n\n\nIn a past teaching job, Derek would apply a grading curve as follows:\n\n“A” to the top 9 percentile\n“A-” to the next 9 percentile\netc.\n\n\n\nSuppose that a class of Calculus students took an exam and the grading yielded a sample average of \\(\\bar{x} = 70\\) and a sample standard deviation of \\(s = 15\\) percentage points. Derek’s curve would look like\n\n\n\n\n\n\n\n\n\n\n\nOn a recent exam, data science students had a sample average of \\(\\bar{x} = 60\\) and a sample standard deviation of \\(s = 4.97\\) percentage points. Derek’s curve would look like\n\n\n\n\n\n\n\n\n\n\n\n\n# patchwork\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\nxbar &lt;- 70\ns &lt;- 15\n\ncurve_percentiles &lt;- seq(28, 100, by = 9)/100\nbreakpoints &lt;- qnorm(curve_percentiles, xbar, s)\ndf_vert &lt;- data.frame(\n  x_breakpoints = breakpoints,\n  y_breakpoints = dnorm(breakpoints, xbar, s)\n)\n\np1 &lt;- vnorm(qnorm(0.28, xbar, s), xbar, s, section = \"upper\") +\n  geom_segment(aes(x = x_breakpoints, \n                   y = 0,\n                   xend = x_breakpoints, \n                   yend = y_breakpoints),\n               data = df_vert) +\n  labs(title = \"Grading Curve for a Hypothetical Exam\",\n       x = \"percent\") +\n  scale_x_continuous(\n    breaks = breakpoints,\n    labels = c(\"C-\", \"C\", \"C+\", \"B-\", \"B\", \"B+\", \"A-\", \"A\", \"\"),\n    limits = c(50, 100)\n  )\n\np1\n\n\nxbar &lt;- 90.38\ns &lt;- 8.28\n\ncurve_percentiles &lt;- seq(28, 100, by = 9)/100\nbreakpoints &lt;- qnorm(curve_percentiles, xbar, s)\ndf_vert &lt;- data.frame(\n  x_breakpoints = breakpoints,\n  y_breakpoints = dnorm(breakpoints, xbar, s)\n)\n\np2 &lt;- vnorm(qnorm(0.28, xbar, s), xbar, s, section = \"upper\") +\n  geom_segment(aes(x = x_breakpoints, \n                   y = 0,\n                   xend = x_breakpoints, \n                   yend = y_breakpoints),\n               data = df_vert) +\n  labs(title = \"Grading Curve for Exam 1\",\n       x = \"percent\") +\n  scale_x_continuous(\n    breaks = breakpoints,\n    labels = c(\"C-\", \"C\", \"C+\", \"B-\", \"B\", \"B+\", \"A-\", \"A\", \"\"),\n    limits = c(50, 100)\n  )\n\np2"
  },
  {
    "objectID": "posts/14_estimators/14_estimators.html",
    "href": "posts/14_estimators/14_estimators.html",
    "title": "14: Estimators",
    "section": "",
    "text": "Libraries and Helper Functions\n\n\n\n\n\n\nlibrary(\"gt\")        #great tables\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n\nvbinom &lt;- function(k_obs, n, p, labels = TRUE){\n# make data frame\nk_vals &lt;- 0:n\npk     &lt;- dbinom(k_vals, n, p)\nk_bool &lt;- k_vals %in% k_obs\ndf_binom &lt;- data.frame(k_vals, pk, k_bool)\n\n# compute requested probability\nanswer_prob = round(sum(dbinom(k_obs, n, p)), 4)\n\n# define bar plot\nthis_plot &lt;- if(labels){\n  df_binom |&gt;\n    ggplot(aes(x = factor(k_vals), y = pk, color = k_bool, fill = k_bool)) +\n    geom_bar(stat = \"identity\") +\n    geom_label(aes(x = factor(k_vals), y = pk, label = round(pk, 4)),\n               color = \"black\", fill = \"white\") +\n    labs(subtitle = paste0(\"n = \", n, \", k = \", list(k_obs), \", p = \", p, \", P(k = \", list(k_obs), \") = \", answer_prob),\n         caption = \"SML 201\",\n         y = \"probability\") +\n    theme(\n      legend.position = \"bottom\",\n      panel.background = element_blank()\n    )\n} else{\n  df_binom |&gt;\n    ggplot(aes(x = factor(k_vals), y = pk, color = k_bool, fill = k_bool)) +\n    geom_bar(stat = \"identity\") +\n    labs(subtitle = paste0(\"n = \", n, \", k = \", list(k_obs), \", p = \", p, \", P(k = \", list(k_obs), \") = \", answer_prob),\n         caption = \"SML 201\",\n         y = \"probability\") +\n    theme(\n      legend.position = \"bottom\",\n      panel.background = element_blank()\n    )\n}\n\n# plot bar chart\nthis_plot\n}\n\n\n# helper function\nvnorm &lt;- function(x, mu = 0, sigma = 1, section = \"lower\"){\n  \n  # bell curve\n  x_vals &lt;- seq(mu - 4*sigma, mu + 4*sigma, length.out = 201)\n  y_vals &lt;- dnorm(x_vals, mu, sigma)\n  df_for_graph &lt;- data.frame(x_vals, y_vals)\n\n  # outline shaded regions\n  if(length(x) == 1){\n    shade_left &lt;- rbind(c(x[1],0), df_for_graph |&gt;\n                        filter(x_vals &lt; x[1]))\n    shade_right &lt;- rbind(c(x[1],0), df_for_graph |&gt;\n                        filter(x_vals &gt; x[1]))\n  }\n  if(length(x) == 2){\n    shade_between &lt;- rbind(c(x[1],0),\n                       df_for_graph |&gt;\n                         filter(x_vals &gt; x[1] &\n                                  x_vals &lt; x[2]),\n                       c(x[2],0))\n    shade_tails &lt;- rbind(df_for_graph |&gt;\n                        filter(x_vals &lt; x[1]),\n                     c(x[1],0),\n                     c(x[2],0),\n                     df_for_graph |&gt;\n                        filter(x_vals &gt; x[2]))\n  }\n  if(section == \"lower\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_left,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(pnorm(x,mu,sigma), 4)\n  }\n  if(section == \"upper\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_right,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- 1 - round(pnorm(x,mu,sigma), 4)\n  }\n  if(section == \"between\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_between,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(diff(pnorm(x,mu,sigma)), 4)\n  }\n  if(section == \"tails\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_tails,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(1 - diff(pnorm(x,mu,sigma)), 4)\n  }\n  \n  # plot bell curve\n  bell_curve + \n    labs(subtitle = paste0(\"Probability: \", prob_val),\n         caption = \"SML 201\", y = \"\") +\n    theme_minimal()\n}\n\n\n\n\n\n\n\n\n\nGoal: Estimate known population statistics\nObjective: Use density plots to explore biased and unbiased estimators\n\n\n\n\n\n\n\nd20\n\n\n\n\n\n\n\nThe mean corpusular volume or mean cell volume (MCV) is the average volume of a red blood cell. The following information was gathered, adapted, heavily rounded from the Wikipedia page, and should not constitute medical advice. For these mathematical examples, assume that the mean MCV is \\(\\mu = 90\\) fL/cell with a standard deviation of \\(\\sigma = 5\\) fL/cell and that we can apply the normal distribution based on numerous blood tests.\n\n\n\n\n\n\nEstimating Populations\n\n\n\n\nHow did we know that the average MCV was 90 for the entire population?\n\n\nWe actually don’t know the population mean.\nHistorical data points toward a sample mean of \\(\\bar{x} = 90\\) fL/cell.\nWhy are we allowed to go from the sample mean \\(\\bar{x}\\) to the population mean \\(\\mu\\)?"
  },
  {
    "objectID": "posts/14_estimators/14_estimators.html#start",
    "href": "posts/14_estimators/14_estimators.html#start",
    "title": "14: Estimators",
    "section": "",
    "text": "Goal: Estimate known population statistics\nObjective: Use density plots to explore biased and unbiased estimators\n\n\n\n\n\n\n\nd20"
  },
  {
    "objectID": "posts/14_estimators/14_estimators.html#recap-mean-corpuscular-volume",
    "href": "posts/14_estimators/14_estimators.html#recap-mean-corpuscular-volume",
    "title": "14: Estimators",
    "section": "",
    "text": "The mean corpusular volume or mean cell volume (MCV) is the average volume of a red blood cell. The following information was gathered, adapted, heavily rounded from the Wikipedia page, and should not constitute medical advice. For these mathematical examples, assume that the mean MCV is \\(\\mu = 90\\) fL/cell with a standard deviation of \\(\\sigma = 5\\) fL/cell and that we can apply the normal distribution based on numerous blood tests.\n\n\n\n\n\n\nEstimating Populations\n\n\n\n\nHow did we know that the average MCV was 90 for the entire population?\n\n\nWe actually don’t know the population mean.\nHistorical data points toward a sample mean of \\(\\bar{x} = 90\\) fL/cell.\nWhy are we allowed to go from the sample mean \\(\\bar{x}\\) to the population mean \\(\\mu\\)?"
  },
  {
    "objectID": "posts/14_estimators/14_estimators.html#sample-mean",
    "href": "posts/14_estimators/14_estimators.html#sample-mean",
    "title": "14: Estimators",
    "section": "Sample Mean",
    "text": "Sample Mean\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}\\]\n\nN &lt;- 10000 #number of experiment iterations\nsamp_dist &lt;- rep(NA, N) #allocate space to store sample calculations\n\nfor(i in 1:N){\n\n  #roll d20 n = 10 times\n  these_rolls &lt;- sample(d20_die, size = 10, replace = TRUE)\n  \n  #record this sample stat\n  samp_dist[i] &lt;- mean(these_rolls)\n}\n\ndf_for_graph &lt;- data.frame(\n  id = 1:N,\n  samp_dist = samp_dist\n)\n\n\ndf_for_graph |&gt;\n  ggplot() +\n  geom_density(aes(x = samp_dist), \n               color = \"black\", linewidth = 2) +\n  geom_vline(aes(xintercept = mu), \n             color = \"red\", linewidth = 3) +\n  geom_vline(aes(xintercept = mean(samp_dist)), \n             color = princeton_orange, linetype = 2, linewidth = 3) +\n  labs(title = \"Biased or Unbiased?\",\n       subtitle = \"black: sample distribution\\norange: average of samples\\nred: true population stat\",\n       caption = \"SML 201\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe say that the sample mean \\(\\bar{x}\\) is an unbiased estimator for the population mean \\(\\mu\\)."
  },
  {
    "objectID": "posts/14_estimators/14_estimators.html#sample-median",
    "href": "posts/14_estimators/14_estimators.html#sample-median",
    "title": "14: Estimators",
    "section": "Sample Median",
    "text": "Sample Median\n\\[\\nu = \\frac{1}{2}\\left(x_{\\lfloor\\frac{n+1}{2}\\rfloor} + x_{\\lceil\\frac{n+1}{2}\\rceil}\\right)\\]\nFor this experiment, Derek wanted an asymmetrical outcome space.\n\\[\\{1, 2, 3, 14, 16, 18, 20\\}\\] whose population median is \\(\\nu = 14\\).\n\nd7_die &lt;- c(1, 2, 3, 14, 16, 18, 20)\nN &lt;- 10000 #number of experiment iterations\nsamp_dist &lt;- rep(NA, N) #allocate space to store sample calculations\n\nfor(i in 1:N){\n\n  #roll d20 n = 10 times\n  these_rolls &lt;- sample(d7_die, size = 10, replace = TRUE)\n  \n  #record this sample stat\n  samp_dist[i] &lt;- median(these_rolls)\n}\n\ndf_for_graph &lt;- data.frame(\n  id = 1:N,\n  samp_dist = samp_dist\n)\n\n\ndf_for_graph |&gt;\n  ggplot() +\n  geom_density(aes(x = samp_dist), \n               color = \"black\", linewidth = 2) +\n  geom_vline(aes(xintercept = 14), \n             color = \"red\", linewidth = 3) +\n  geom_vline(aes(xintercept = mean(samp_dist)), \n             color = princeton_orange, linetype = 2, linewidth = 3) +\n  labs(title = \"Biased or Unbiased?\",\n       subtitle = \"black: sample distribution\\norange: average of samples\\nred: true population stat\",\n       caption = \"SML 201\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSince the modes do not appear to align with the population median, we say that the sample median is a biased estimator of the population median. In our visualizations today, the bias is the horizontal distance between the red and orange lines."
  },
  {
    "objectID": "posts/14_estimators/14_estimators.html#population-variance",
    "href": "posts/14_estimators/14_estimators.html#population-variance",
    "title": "14: Estimators",
    "section": "Population Variance",
    "text": "Population Variance\n\\[\\sigma^{2} = \\frac{1}{N}\\sum_{i=1}^{N} (x_{i} - \\mu)^{2}\\]\n\npop_var &lt;- function(x){\n  N &lt;- length(x)\n  mu &lt;- mean(x)\n  sigma_sq &lt;- sum((x - mu)^{2})/N\n  \n  #return\n  sigma_sq\n}\n\n\nN &lt;- 10000 #number of experiment iterations\nsamp_dist &lt;- rep(NA, N) #allocate space to store sample calculations\n\nfor(i in 1:N){\n\n  #roll d20 n = 10 times\n  these_rolls &lt;- sample(d20_die, size = 10, replace = TRUE)\n  \n  #record this sample stat\n  samp_dist[i] &lt;- pop_var(these_rolls)\n}\n\ndf_for_graph &lt;- data.frame(\n  id = 1:N,\n  samp_dist = samp_dist\n)\n\n\ndf_for_graph |&gt;\n  ggplot() +\n  geom_density(aes(x = samp_dist), \n               color = \"black\", linewidth = 2) +\n  geom_vline(aes(xintercept = sigma_sq), \n             color = \"red\", linewidth = 3) +\n  geom_vline(aes(xintercept = mean(samp_dist)), \n             color = princeton_orange, linetype = 2, linewidth = 3) +\n  labs(title = \"Biased or Unbiased?\",\n       subtitle = \"black: sample distribution\\norange: average of samples\\nred: true population stat\",\n       caption = \"SML 201\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUpon reapplying the population variance formula to the samples, we see that the resulting sampling distribution tends to underestimate the true population variance. So far, this formula is a biased estimator of the population variance."
  },
  {
    "objectID": "posts/14_estimators/14_estimators.html#sample-variance",
    "href": "posts/14_estimators/14_estimators.html#sample-variance",
    "title": "14: Estimators",
    "section": "Sample Variance",
    "text": "Sample Variance\n\\[s^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}\\]\n\nsamp_var &lt;- function(x){\n  n &lt;- length(x)\n  xbar &lt;- mean(x)\n  \n  # Bessel's correction: \"n-1\"\n  s_sq &lt;- sum((x - xbar)^{2})/(n-1)\n  \n  #return\n  s_sq\n}\n\n\nN &lt;- 10000 #number of experiment iterations\nsamp_dist &lt;- rep(NA, N) #allocate space to store sample calculations\n\nfor(i in 1:N){\n\n  #roll d20 n = 10 times\n  these_rolls &lt;- sample(d20_die, size = 10, replace = TRUE)\n  \n  #record this sample stat\n  samp_dist[i] &lt;- samp_var(these_rolls)\n}\n\ndf_for_graph &lt;- data.frame(\n  id = 1:N,\n  samp_dist = samp_dist\n)\n\n\ndf_for_graph |&gt;\n  ggplot() +\n  geom_density(aes(x = samp_dist), \n               color = \"black\", linewidth = 2) +\n  geom_vline(aes(xintercept = sigma_sq), \n             color = \"red\", linewidth = 3) +\n  geom_vline(aes(xintercept = mean(samp_dist)), \n             color = princeton_orange, linetype = 2, linewidth = 3) +\n  labs(title = \"Biased or Unbiased?\",\n       subtitle = \"black: sample distribution\\norange: average of samples\\nred: true population stat\",\n       caption = \"SML 201\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNow, with Bessel’s correction, we say that the sample variance is an unbiased estimator for the population variance.\n\n\n\n\n\n\n(optional) Bessel’s Correction\n\n\n\n\n\nClaim: The sample variance formula with \\(n-1\\)\n\\[s^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}\\] is an unbiased estimator for the population variance \\(\\sigma^{2}\\).\nProof: (omitted)\n\ntakes about 4 Calculus lectures to explain"
  },
  {
    "objectID": "posts/14_estimators/14_estimators.html#sample-standard-deviation",
    "href": "posts/14_estimators/14_estimators.html#sample-standard-deviation",
    "title": "14: Estimators",
    "section": "Sample Standard Deviation",
    "text": "Sample Standard Deviation\n\\[s= \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}}\\]\n\nset.seed(20241029)\nN &lt;- 10000 #number of experiment iterations\nsamp_dist &lt;- rep(NA, N) #allocate space to store sample calculations\n\nfor(i in 1:N){\n\n  #roll d20 n = 10 times\n  these_rolls &lt;- sample(d20_die, size = 10, replace = TRUE)\n  \n  #record this sample stat\n  samp_dist[i] &lt;- sqrt(samp_var(these_rolls))\n}\n\ndf_for_graph &lt;- data.frame(\n  id = 1:N,\n  samp_dist = samp_dist\n)\n\n\ndf_for_graph |&gt;\n  ggplot() +\n  geom_density(aes(x = samp_dist), \n               color = \"black\", linewidth = 2) +\n  geom_vline(aes(xintercept = sigma), \n             color = \"red\", linewidth = 3) +\n  geom_vline(aes(xintercept = mean(samp_dist)), \n             color = princeton_orange, linetype = 2, linewidth = 3) +\n  labs(title = \"Biased or Unbiased?\",\n       subtitle = \"black: sample distribution\\norange: average of samples\\nred: true population stat\",\n       caption = \"SML 201\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIt can be mathematically proven that the sample standard deviation is a biased estimator (as it tends to underestimate the true population standard deviation), but the bias is treated as negligible in practice."
  },
  {
    "objectID": "posts/14_estimators/14_estimators.html#sample-extrema",
    "href": "posts/14_estimators/14_estimators.html#sample-extrema",
    "title": "14: Estimators",
    "section": "Sample Extrema",
    "text": "Sample Extrema\n\nN &lt;- 10000 #number of experiment iterations\nsamp_dist &lt;- rep(NA, N) #allocate space to store sample calculations\n\nfor(i in 1:N){\n\n  #roll d20 n = 10 times\n  these_rolls &lt;- sample(d20_die, size = 10, replace = TRUE)\n  \n  #record this sample stat\n  samp_dist[i] &lt;- min(these_rolls)\n}\n\ndf_for_graph &lt;- data.frame(\n  id = 1:N,\n  samp_dist = samp_dist\n)\n\n\ndf_for_graph |&gt;\n  ggplot() +\n  geom_density(aes(x = samp_dist), \n               color = \"black\", linewidth = 2) +\n  geom_vline(aes(xintercept = 1), \n             color = \"red\", linewidth = 3) +\n  geom_vline(aes(xintercept = mean(samp_dist)), \n             color = princeton_orange, linetype = 2, linewidth = 3) +\n  labs(title = \"Biased or Unbiased?\",\n       subtitle = \"black: sample distribution\\norange: average of samples\\nred: true population stat\",\n       caption = \"SML 201\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIt is quickly apparent that the sample minimum (or maximum) is a biased estimator for the population minimum (or maximum)."
  },
  {
    "objectID": "posts/14_estimators/14_estimators.html#sample-proportion",
    "href": "posts/14_estimators/14_estimators.html#sample-proportion",
    "title": "14: Estimators",
    "section": "Sample Proportion",
    "text": "Sample Proportion\n\nN &lt;- 10000 #number of experiment iterations\nsamp_dist &lt;- rep(NA, N) #allocate space to store sample calculations\n\nfor(i in 1:N){\n\n  #roll d20 n = 10 times\n  these_rolls &lt;- sample(d20_die, size = 10, replace = TRUE)\n  \n  #record this sample stat\n  samp_dist[i] &lt;- mean(these_rolls &lt; 12)\n}\n\ndf_for_graph &lt;- data.frame(\n  id = 1:N,\n  samp_dist = samp_dist\n)\n\n\ndf_for_graph |&gt;\n  ggplot() +\n  geom_density(aes(x = samp_dist), \n               color = \"black\", linewidth = 2) +\n  geom_vline(aes(xintercept = 11/20), \n             color = \"red\", linewidth = 3) +\n  geom_vline(aes(xintercept = mean(samp_dist)), \n             color = princeton_orange, linetype = 2, linewidth = 3) +\n  labs(title = \"Biased or Unbiased?\",\n       subtitle = \"black: sample distribution\\norange: average of samples\\nred: true population stat\",\n       caption = \"SML 201\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIt may not be apparent from this empirical experiment, but the sample proportion is said to be an unbiased estimator for the population proportion."
  },
  {
    "objectID": "posts/14_estimators/14_estimators.html#summary",
    "href": "posts/14_estimators/14_estimators.html#summary",
    "title": "14: Estimators",
    "section": "Summary",
    "text": "Summary\nWhen are we allowed to use sample statistics to describe the overall population. We proceed by using unbiased estimators (and the sample standard deviation).\n\ngt tablecode\n\n\n\n\n\n\n\n\n\n\nStatistical Estimators\n\n\nfrom samples to populations\n\n\nestimator\nbias\n\n\n\n\nsample mean\nunbiased\n\n\nsample median\nbiased\n\n\npopulation variance\nbiased\n\n\nsample variance\nunbiased\n\n\nsample standard deviation\nnegligible bias\n\n\nsample minimum\nbiased\n\n\nsample maximum\nbiased\n\n\nsample proportion\nunbiased\n\n\n\nSML 201\n\n\n\n\n\n\n\n\n\n\n\ndata.frame(\n  estimator = c(\"sample mean\", \"sample median\", \"population variance\", \"sample variance\", \"sample standard deviation\", \"sample minimum\", \"sample maximum\", \"sample proportion\"),\n  bias = c(\"unbiased\", \"biased\", \"biased\", \"unbiased\", \"negligible bias\", \"biased\", \"biased\", \"unbiased\")\n) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"SML 201\") |&gt;\n  tab_header(\n    title = \"Statistical Estimators\",\n    subtitle = \"from samples to populations\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = cell_text(color = \"red\"),\n    locations = cells_body(rows = bias == \"biased\")\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = princeton_orange),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(rows = bias == \"unbiased\")\n  )"
  },
  {
    "objectID": "posts/14_estimators/14_estimators.html#horizontal-translation",
    "href": "posts/14_estimators/14_estimators.html#horizontal-translation",
    "title": "14: Estimators",
    "section": "Horizontal Translation",
    "text": "Horizontal Translation\n\nmu_vals &lt;- seq(-4, 4, by = 0.5)\n\\(x \\in \\mu \\pm 1\\)\n\\(\\sigma = 1\\)\n\n\nmu_vals &lt;- seq(-4, 4, by = 0.5)\nN &lt;- length(mu_vals)\n\nfor(i in 1:N){\n  this_plot &lt;- vnorm(c(mu_vals[i] - 1, mu_vals[i] + 1), \n                     mu_vals[i], 1, section = \"between\") +\n    labs(title = \"Changing Mean\") +\n    xlim(-5, 5)\n  ggsave(paste0(\"images/norm_plot\", LETTERS[i], \".png\"), this_plot)\n}\n\npng_files &lt;- Sys.glob(\"images/norm_plot*.png\")\n\ngifski::gifski(\n  png_files,\n  \"norm_animation.gif\",      #output file name\n  height = 800, width = 800, #you may change the resolution\n  delay = 1/2                #seconds\n)\n\n\n\n\nbell curve, changing means"
  },
  {
    "objectID": "posts/14_estimators/14_estimators.html#increasing-sample-size",
    "href": "posts/14_estimators/14_estimators.html#increasing-sample-size",
    "title": "14: Estimators",
    "section": "Increasing Sample Size",
    "text": "Increasing Sample Size\nHere we will use the vbinom function with various increasing values for the sample size \\(n\\) and a constant value for the population proportion \\(p\\).\n\nn_vals &lt;- seq(9, 30)\n\n\nn_vals &lt;- seq(9, 30)\np      &lt;- 0.37\nN      &lt;- length(n_vals)\n\nfor(i in 1:N){\n  this_plot &lt;- vbinom(round(n_vals[i]/4):round(3*n_vals[i]/4), \n                      n_vals[i], p, labels = FALSE) +\n    labs(title = \"Central Limit Theorem\")\n  ggsave(paste0(\"images/CLT_plot\", LETTERS[i], \".png\"), this_plot)\n}\n\npng_files &lt;- Sys.glob(\"images/CLT_plot*.png\")\n\ngifski::gifski(\n  png_files,\n  \"CLT_animation.gif\",    #output file name\n  height = 800, width = 800, #you may change the resolution\n  delay = 1/3                #seconds\n)\n\n\n\n\nCentral Limit Theorem"
  },
  {
    "objectID": "posts/14_estimators/14_estimators.html#example-computing-cluster",
    "href": "posts/14_estimators/14_estimators.html#example-computing-cluster",
    "title": "14: Estimators",
    "section": "Example: Computing Cluster",
    "text": "Example: Computing Cluster\nFor a request to use the campus computing cluster, and knowing that your independent jobs’ duration times are normally distributed with a mean of one hour and a standard deviation of 10 minutes, answer the following inquiries."
  },
  {
    "objectID": "posts/14_estimators/14_estimators.html#one-observation",
    "href": "posts/14_estimators/14_estimators.html#one-observation",
    "title": "14: Estimators",
    "section": "One Observation",
    "text": "One Observation\n\nWhat is the probability that a randomly selected job has a duration of under 57 minutes.\n\n\npnorm(57, 60, 10)\n\n[1] 0.3820886\n\n\n\nvnorm(57, 60, 10) +\n  labs(title = \"Campus Computing Example (n = 1 job)\") +\n  xlim(20, 100)"
  },
  {
    "objectID": "posts/14_estimators/14_estimators.html#average-observation",
    "href": "posts/14_estimators/14_estimators.html#average-observation",
    "title": "14: Estimators",
    "section": "Average Observation",
    "text": "Average Observation\n\nWhat is the probability that the average duration of your 28 jobs is less than 57 minutes?\n\n\npnorm(57, 60, 10/sqrt(28))\n\n[1] 0.05620529\n\n\n\nvnorm(57, 60, 10/sqrt(28)) +\n  labs(title = \"Campus Computing Example (n = 28 jobs)\") +\n  xlim(20, 100)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntuition\n\n\n\nAs the sample size (\\(n\\)) increases, it is less likely to observe an average in the tails of a normal distribution."
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html",
    "title": "15: Confidence Intervals",
    "section": "",
    "text": "Libraries and Helper Functions\n\n\n\n\n\n\nlibrary(\"bayesrules\") #data set: Pulse of the Nation\nlibrary(\"ggimage\")    #customize images for scatterpoint points\nlibrary(\"ggtext\")     #adorn ggplot text\nlibrary(\"gt\")         #great tables\nlibrary(\"infer\")      #streamlined code for inference tasks\nlibrary(\"janitor\")    #compute proportions easily\nlibrary(\"tidyverse\")  #tools for data wrangling and visualization\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n# data set: Pulse of the Nation\ndata(pulse_of_the_nation)\npulse_df &lt;- pulse_of_the_nation\n\n# data set: SML 201 demographics survey\ndemo_df &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/04_categories/sml201survey.csv\")\n\n\n# helper function\nvnorm &lt;- function(x, mu = 0, sigma = 1, section = \"lower\"){\n  \n  # bell curve\n  x_vals &lt;- seq(mu - 4*sigma, mu + 4*sigma, length.out = 201)\n  y_vals &lt;- dnorm(x_vals, mu, sigma)\n  df_for_graph &lt;- data.frame(x_vals, y_vals)\n\n  # outline shaded regions\n  if(length(x) == 1){\n    shade_left &lt;- rbind(c(x[1],0), df_for_graph |&gt;\n                        filter(x_vals &lt; x[1]))\n    shade_right &lt;- rbind(c(x[1],0), df_for_graph |&gt;\n                        filter(x_vals &gt; x[1]))\n  }\n  if(length(x) == 2){\n    shade_between &lt;- rbind(c(x[1],0),\n                       df_for_graph |&gt;\n                         filter(x_vals &gt; x[1] &\n                                  x_vals &lt; x[2]),\n                       c(x[2],0))\n    shade_tails &lt;- rbind(df_for_graph |&gt;\n                        filter(x_vals &lt; x[1]),\n                     c(x[1],0),\n                     c(x[2],0),\n                     df_for_graph |&gt;\n                        filter(x_vals &gt; x[2]))\n  }\n  if(section == \"lower\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_left,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(pnorm(x,mu,sigma), 4)\n  }\n  if(section == \"upper\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_right,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- 1 - round(pnorm(x,mu,sigma), 4)\n  }\n  if(section == \"between\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_between,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(diff(pnorm(x,mu,sigma)), 4)\n  }\n  if(section == \"tails\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_tails,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(1 - diff(pnorm(x,mu,sigma)), 4)\n  }\n  \n  # plot bell curve\n  bell_curve + \n    labs(subtitle = paste0(\"Probability: \", prob_val),\n         caption = \"SML 201\", y = \"\") +\n    theme_minimal()\n}\n\n\n\n\n\n\n\n\n\nGoal: Estimate unknown population statistics\nObjective: Deploy and interpret confidence intervals\n\n\n\n\n\n\n\nGhostbusters (1984)"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#start",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#start",
    "title": "15: Confidence Intervals",
    "section": "",
    "text": "Goal: Estimate unknown population statistics\nObjective: Deploy and interpret confidence intervals\n\n\n\n\n\n\n\nGhostbusters (1984)"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#scenario-believe-in-ghosts",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#scenario-believe-in-ghosts",
    "title": "15: Confidence Intervals",
    "section": "Scenario: Believe in Ghosts?",
    "text": "Scenario: Believe in Ghosts?\n\nQueryData\n\n\n\n\n\n\n\nRay Stantz\n\n\n\n\n\nWhat proportion of people believe that ghosts exist?\n\n\n\n\n\n\n\nsource: Pulse of the Nation survey by Cards Against Humanity\nPoll 1: September 2017\n1000 observations\n15 variables\n\n\n\n\n\n\n\nPulse of the Nation"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#normal-distribution",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#normal-distribution",
    "title": "15: Confidence Intervals",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\n\n\nWalter Peck wants estimates to have at least 95 percent confidence!\n\n# 2.5 and 97.5 percentiles\nqnorm(c(0.025, 0.975))\n\n[1] -1.959964  1.959964\n\n\n\n\n\n\nvnorm(qnorm(c(0.025, 0.975)), section = \"between\") +\n  annotate(\"text\", x = 0, y = 0.2, label = \"95%\", \n           color = \"white\", size = 15) +\n  labs(title = \"Extracting a 95 Percent Interval\",\n       x = \"z\") +\n  scale_x_continuous(breaks = c(-1.96, 1.96),\n                     labels = c(-1.96, 1.96))"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#sample-proportion",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#sample-proportion",
    "title": "15: Confidence Intervals",
    "section": "Sample Proportion",
    "text": "Sample Proportion\n\nDisplayCode\n\n\n\n\n\n\n ghosts    n percent\n     No  621   62.1%\n    Yes  379   37.9%\n  Total 1000  100.0%\n\n\n\\[\\hat{p} = 0.379\\] \\[n = 1000\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npulse_df |&gt;\n  tabyl(ghosts) |&gt;\n  adorn_totals(\"row\") |&gt;\n  adorn_pct_formatting()\n\ntitle_string &lt;- \"&lt;span style = 'color:#23b63c'&gt;Believes that Ghosts Exist&lt;/span&gt;&lt;br&gt;versus&lt;br&gt;&lt;span style = 'color:#ea0000'&gt;Doesn't Believe that Ghosts Exist&lt;/span&gt;\"\n\npulse_df |&gt;\n  ggplot() +\n  geom_bar(aes(x = ghosts, fill = ghosts)) +\n  annotate(\"text\", x = c(\"No\", \"Yes\"), y = c(310, 170), \n           label = c(\"62%\", \"38%\"), \n           color = \"white\", size = 15) +\n  labs(title = title_string,\n       caption = \"SML 201\") +\n  scale_fill_manual(values = c(\"#ea0000\", \"#23b63c\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_markdown(hjust = 0.5,\n                                      face = \"bold\",\n                                      size = 20))"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#confidence-interval-for-a-proportion",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#confidence-interval-for-a-proportion",
    "title": "15: Confidence Intervals",
    "section": "Confidence Interval for a Proportion",
    "text": "Confidence Interval for a Proportion\n\nMathR\n\n\n\\[\\hat{p} \\pm E, \\quad\\text{where } E = z_{\\alpha/2}*\\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}} \\text{ and } z_{\\alpha/2} \\approx 1.96\\]\n\\[\\left(0.3489, 0.4091\\right)\\]\nWe are 95 percent confident that the true population proportion of Americans that believe in ghosts is in between 34.89 and 40.91 percent.\n\n\n\n# sample statistics\nphat &lt;- mean(pulse_df$ghosts == \"Yes\")\nn &lt;- sum(!is.na(pulse_df$ghosts))\n\n# margin of error\nE &lt;- qnorm(0.975)*sqrt((phat*(1-phat))/n)\n\n# confidence interval\nphat + c(-1,1)*E\n\n[1] 0.3489314 0.4090686"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#scenario-how-old-are-you",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#scenario-how-old-are-you",
    "title": "15: Confidence Intervals",
    "section": "Scenario: How Old Are You?",
    "text": "Scenario: How Old Are You?\n\n\n\n\n\nDana Barrett\n\n\n\n\n\nAmong the people that believe in ghosts, how old are you?\n\n\n\nDisplayCode\n\n\n\n\n\n\n# A tibble: 2 × 3\n  ghosts  xbar     n\n  &lt;fct&gt;  &lt;dbl&gt; &lt;int&gt;\n1 No      50.3   621\n2 Yes     48.0   379\n\n\n\\[\\bar{x} \\approx 47.9525\\] \\[n = 379\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npulse_df |&gt;\n  group_by(ghosts) |&gt;\n  summarize(xbar = mean(age, na.rm = TRUE),\n            n = n())\n\ntitle_string &lt;- \"&lt;span style = 'color:#23b63c'&gt;Believes that Ghosts Exist&lt;/span&gt;&lt;br&gt;versus&lt;br&gt;&lt;span style = 'color:#ea0000'&gt;Doesn't Believe that Ghosts Exist&lt;/span&gt;\"\n\npulse_df |&gt;\n  ggplot() +\n  geom_density(aes(x = age, fill = ghosts),\n               alpha = 0.5) +\n  labs(title = title_string,\n       caption = \"SML 201\") +\n  scale_fill_manual(values = c(\"#ea0000\", \"#23b63c\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_markdown(hjust = 0.5,\n                                      face = \"bold\",\n                                      size = 20))"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#confidence-interval-for-a-mean",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#confidence-interval-for-a-mean",
    "title": "15: Confidence Intervals",
    "section": "Confidence Interval for a Mean",
    "text": "Confidence Interval for a Mean\n\nMathR\n\n\n\\[\\bar{x} \\pm E, \\quad\\text{where } E = t_{\\alpha/2}*\\frac{s}{n}\\]\n\\[\\left(46.3651, 49.5399\\right)\\]\nWe are 95 percent confident that the true population mean age for people that believe ghosts exist is in between 46.37 and 49.54 years old.\n\n\n\n# subset\nghosts_yes &lt;- pulse_df |&gt; filter(ghosts == \"Yes\")\n\n# sample statistics\nxbar &lt;- mean(ghosts_yes$age, na.rm = TRUE)\ns &lt;- sd(ghosts_yes$age, na.rm = TRUE)\nn &lt;- sum(!is.na(ghosts_yes$age))\n\n# margin of error (here: \"df\" are degrees of freedom)\nE &lt;- qt(0.975, df = n - 1)*s/sqrt(n)\n\n# confidence interval\nxbar + c(-1,1)*E\n\n[1] 46.36511 49.53991"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#student-t-distribution",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#student-t-distribution",
    "title": "15: Confidence Intervals",
    "section": "Student t Distribution",
    "text": "Student t Distribution\n\nIdeaGossetdf\n\n\nThe Student t distribution is an abstraction of the standard normal distribution that adjusts with wider tails to allow for more probability in the tails\n\n\n\nt distribution\n\n\n\n\n\n\n\nWilliam Sealy Gosset\n\n\n\n\nIn advanced statistics, such as the adjusted \\(R^{2}\\) calculation for the coefficient of determination, the degrees of freedom are the difference between\n\nnumber of observations in the data\nnumber of independent variables being modeled\n\nFor this setting—estimating the true population mean—the degrees of freedom are simply\n\\[df = n - 1\\]\n\n\n\n\n\n\n\n\n\nLeaving the t distribution behind\n\n\n\nFor these calculations\n\\[\\bar{x} \\pm E, \\quad\\text{where } E = t_{\\alpha/2}*\\frac{s}{n}\\]\n\nrely more on summary statistics rather than all of the gathered data\n“degrees of freedom” is a rather convoluted notion\nt-distribution is itself an approximation\nleads to more reliance on abstract probability distributions\ndeparts from frequentist probability philosophy\nmore useful before calculators and computers"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#scenario-princeton-politics",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#scenario-princeton-politics",
    "title": "15: Confidence Intervals",
    "section": "Scenario: Princeton Politics",
    "text": "Scenario: Princeton Politics\nThe survey question was\n\nOn a scale of 0 to 100—with 0 = Democrat and 100 = Republican—where are your political leanings?"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#bootstrap-distribution",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#bootstrap-distribution",
    "title": "15: Confidence Intervals",
    "section": "Bootstrap Distribution",
    "text": "Bootstrap Distribution\n\nset.seed(201)\nbootstrap_distribution &lt;- demo_df |&gt;\n  specify(response = politics) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"mean\")\n\nWarning: Removed 16 rows containing missing values."
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#endpoints-standard-error",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#endpoints-standard-error",
    "title": "15: Confidence Intervals",
    "section": "Endpoints (Standard Error)",
    "text": "Endpoints (Standard Error)\n\nxbar &lt;- mean(demo_df$politics, na.rm = TRUE)\nse_ci &lt;- bootstrap_distribution |&gt;\n  get_confidence_interval(level = 0.95,\n                          type = \"se\", \n                          point_estimate = xbar)"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#visualization",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#visualization",
    "title": "15: Confidence Intervals",
    "section": "Visualization",
    "text": "Visualization\n\nbootstrap_distribution |&gt;\n  visualize() +\n  shade_confidence_interval(endpoints = se_ci,\n                            color = princeton_black,\n                            fill = princeton_orange) +\n  labs(title = \"Political Leanings of Princeton Students\",\n       subtitle = \"Fall 2024\",\n       caption = \"SML 201\",\n       x = \"0: Democrat ... 100: Republican\") +\n  theme_minimal() +\n  xlim(0, 100)"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#inference",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#inference",
    "title": "15: Confidence Intervals",
    "section": "Inference",
    "text": "Inference\n\nprint(round(se_ci))\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1       30       38\n\n\nIn this survey question, “On a scale of 0 to 100—with 0 = Democrat and 100 = Republican—where are your political leanings?”, we are 95 percent confident that the true population mean for Princeton students is in between 30 and 38.\n\n\n\n\n\n\nDiscussion\n\n\n\nThis was a survey among SML 201 students\n\nnot a representative or random sample of Princeton students\nself-reported data"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#scenario-pineapple-on-pizza",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#scenario-pineapple-on-pizza",
    "title": "15: Confidence Intervals",
    "section": "Scenario: Pineapple on Pizza",
    "text": "Scenario: Pineapple on Pizza\nThe sample proportions (among those who were adamant) were\n\ndemo_df |&gt;\n  filter(pineapplePizza %in% c(\"No!\", \"Yes!\")) |&gt;\n  tabyl(pineapplePizza) |&gt;\n  adorn_totals(\"row\") |&gt;\n  adorn_pct_formatting()\n\n pineapplePizza  n percent\n            No! 32   43.8%\n           Yes! 41   56.2%\n          Total 73  100.0%"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#bootstrap-distribution-1",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#bootstrap-distribution-1",
    "title": "15: Confidence Intervals",
    "section": "Bootstrap Distribution",
    "text": "Bootstrap Distribution\n\nset.seed(201)\nbootstrap_distribution &lt;- demo_df |&gt;\n  filter(pineapplePizza %in% c(\"No!\", \"Yes!\")) |&gt;\n  specify(response = pineapplePizza, success= \"Yes!\") |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"prop\")"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#endpoints-percentile",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#endpoints-percentile",
    "title": "15: Confidence Intervals",
    "section": "Endpoints (Percentile)",
    "text": "Endpoints (Percentile)\nBuilding the confidence interval from percentiles is perhaps more reasonable than using the standard errors (and less code too).\n\nper_ci &lt;- bootstrap_distribution |&gt;\n  get_ci(level = 0.95, type = \"percentile\")"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#visualization-1",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#visualization-1",
    "title": "15: Confidence Intervals",
    "section": "Visualization",
    "text": "Visualization\n\nbootstrap_distribution |&gt;\n  visualize() +\n  shade_ci(per_ci, \n           color = princeton_black, fill = princeton_orange)"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#inference-1",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#inference-1",
    "title": "15: Confidence Intervals",
    "section": "Inference",
    "text": "Inference\n\nbootstrap_distribution |&gt; get_ci()\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.438    0.671\n\n\nAmong the Princeton students who have a strong opinion on whether or not pineapple is a good pizza topping, we are 95 percent confident that the true population proportion of students who like pineapple on pizza is in between 44 and 67 percent.\n\n\n\n\n\n\nDiscussion\n\n\n\nThis was a survey among SML 201 students\n\nrelatively smaller sample size (n = 73 students)\nSince 0.5 is contained within the confidence interval, this result is not significally different than simply flipping a fair coin."
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#scenario-20-ghosts",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#scenario-20-ghosts",
    "title": "15: Confidence Intervals",
    "section": "Scenario: 20 Ghosts",
    "text": "Scenario: 20 Ghosts\n\n\n\n\n\nLouis Tully\n\n\n\n\n\nRemember when we faced the Dungeon Master and their army of 20 ghosts? Under the equal probabilities of a uniform distribution, we knew that we would face off against \\(\\mu = 10.5\\) ghosts, on average.\n\n\n\n\n\n\nd20"
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#experiment",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#experiment",
    "title": "15: Confidence Intervals",
    "section": "Experiment",
    "text": "Experiment\n\n\n\n\n\nEgon Spengler\n\n\n\n\n\nWe could resample the outcome space and see what proportion of confidence intervals capture the [population mean] ghost."
  },
  {
    "objectID": "posts/15_confidence_intervals/15_confidence_intervals.html#simulation",
    "href": "posts/15_confidence_intervals/15_confidence_intervals.html#simulation",
    "title": "15: Confidence Intervals",
    "section": "Simulation",
    "text": "Simulation\n\nAnimationCode\n\n\n\n\n\nCI animation\n\n\n\n\n\nset.seed(201)\n# d20\nd20_df &lt;- data.frame(d20_outcomes = 1:20)\n\n# create data frame and allocate space\ndf_for_graph &lt;- data.frame(\n  id = 1:26,\n  a = rep(NA, 26),\n  b = rep(NA, 26),\n  result = rep(NA, 26),\n  result_color = rep(NA, 26)\n)\n\nfor(i in 1:26){\n  # bootstrap_distribution &lt;- d20_df |&gt;\n  #   specify(response = d20_outcomes) |&gt;\n  #   generate(reps = 50, type = \"bootstrap\") |&gt;\n  #   calculate(stat = \"mean\")\n  # CI &lt;- bootstrap_distribution |&gt; get_ci()\n  \n  this_sample &lt;- sample(1:20, size = 10, replace = TRUE)\n  xbar &lt;- mean(this_sample)\n  s &lt;- sd(this_sample)\n  n &lt;- length(this_sample)\n  E &lt;- qt(0.975, df = n-1)*s/sqrt(n)\n  \n  \n  # df_for_graph$a[i] &lt;- unlist(CI[1])\n  # df_for_graph$b[i] &lt;- unlist(CI[2])\n  df_for_graph$a[i] &lt;- xbar - E\n  df_for_graph$b[i] &lt;- xbar + E\n  df_for_graph$result[i] &lt;- ifelse(\n    df_for_graph$a[i] &lt; 10.5 & 10.5 &lt; df_for_graph$b[i],\n    \"captured\",\n    \"not captured\"\n  )\n  df_for_graph$result_color[i] &lt;- ifelse(\n    df_for_graph$a[i] &lt; 10.5 & 10.5 &lt; df_for_graph$b[i],\n    \"#23b63c\",\n    \"#ea0000\"\n  )\n  \n  capture_rate &lt;- mean(df_for_graph$result == \"captured\",\n                       na.rm = TRUE)\n  \n  this_plot &lt;- df_for_graph |&gt;\n    filter(id %in% 1:i) |&gt;\n    ggplot() +\n    # geom_vline(aes(xintercept = 10.5), \n    #            color = \"#ab9f8f\", linewidth = 2) +\n    geom_segment(aes(x = a, y = id, \n                     xend = b, yend = id,\n                     color = result_color)) +\n    geom_point(aes(x = a, y = id, color = result_color),\n               size = 3) +\n    geom_point(aes(x = b, y = id, color = result_color),\n               size = 3) +\n    geom_image(aes(x = 10.5, y = id),\n               image = \"ghostbusters_ghost.png\") +\n    labs(title = \"Confidence Intervals\",\n         subtitle = paste0(\"capture rate: \",\n                           round(100*capture_rate, 2),\n                           \" percent\"),\n         caption = \"SML 201\",\n         x = \"ghosts\", y = \"interval number\") +\n    scale_color_manual(values = c(\"#23b63c\", \"#ea0000\")) +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n  \n  ggsave(paste0(\"images/CI_plot\", LETTERS[i], \".png\"), this_plot)\n}\n\npng_files &lt;- Sys.glob(\"images/CI_plot*.png\")\n\ngifski::gifski(\n  png_files,\n  \"CI_animation.gif\",    #output file name\n  height = 1600, width = 1600, #you may change the resolution\n  delay = 1/2                #seconds\n)"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html",
    "title": "16: Hypothesis Testing (1)",
    "section": "",
    "text": "Libraries and Helper Functions\n\n\n\n\n\n\nlibrary(\"infer\")         #pipeline workflow for hypothesis testing\nlibrary(\"janitor\")       #compute proportions easily\nlibrary(\"moderndive\")    #textbook's package and data\nlibrary(\"patchwork\")     #easily let's me show graphs side-by-side\nlibrary(\"tidyverse\")     #the overall programming style universe\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n# data set: SML 201 demographics survey\ndemo_df &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/04_categories/sml201survey.csv\")\n\n# helper function\nvnorm &lt;- function(x, mu = 0, sigma = 1, section = \"lower\"){\n  \n  # bell curve\n  x_vals &lt;- seq(mu - 4*sigma, mu + 4*sigma, length.out = 201)\n  y_vals &lt;- dnorm(x_vals, mu, sigma)\n  df_for_graph &lt;- data.frame(x_vals, y_vals)\n\n  # outline shaded regions\n  if(length(x) == 1){\n    shade_left &lt;- rbind(c(x[1],0), df_for_graph |&gt;\n                        filter(x_vals &lt; x[1]))\n    shade_right &lt;- rbind(c(x[1],0), df_for_graph |&gt;\n                        filter(x_vals &gt; x[1]))\n  }\n  if(length(x) == 2){\n    shade_between &lt;- rbind(c(x[1],0),\n                       df_for_graph |&gt;\n                         filter(x_vals &gt; x[1] &\n                                  x_vals &lt; x[2]),\n                       c(x[2],0))\n    shade_tails &lt;- rbind(df_for_graph |&gt;\n                        filter(x_vals &lt; x[1]),\n                     c(x[1],0),\n                     c(x[2],0),\n                     df_for_graph |&gt;\n                        filter(x_vals &gt; x[2]))\n  }\n  if(section == \"lower\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_left,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(pnorm(x,mu,sigma), 4)\n  }\n  if(section == \"upper\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_right,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- 1 - round(pnorm(x,mu,sigma), 4)\n  }\n  if(section == \"between\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_between,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(diff(pnorm(x,mu,sigma)), 4)\n  }\n  if(section == \"tails\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_tails,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(1 - diff(pnorm(x,mu,sigma)), 4)\n  }\n  \n  # plot bell curve\n  bell_curve + \n    labs(subtitle = paste0(\"Probability: \", prob_val),\n         caption = \"SML 201\", y = \"\") +\n    theme_minimal()\n}\n\n\n\n\n\n\n\n\n\nGoal: Explore hypothesis testing and one-sided tests\nObjective: Deploy t-tests and null distributions"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#start",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#start",
    "title": "16: Hypothesis Testing (1)",
    "section": "",
    "text": "Goal: Explore hypothesis testing and one-sided tests\nObjective: Deploy t-tests and null distributions"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#stacked-bar-chart",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#stacked-bar-chart",
    "title": "16: Hypothesis Testing (1)",
    "section": "Stacked Bar Chart",
    "text": "Stacked Bar Chart\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npromotions |&gt;\n  ggplot(aes(x = gender)) +\n  geom_bar(aes(fill = decision),\n           stat = \"count\") +\n  labs(title = \"Bank Promotions Study\",\n       subtitle = \"Identical resumes except for applicant name\",\n       caption = \"Source: Journal of Applied Psychology\",\n       x = \"Gender of name on resume\") +\n  theme_minimal(base_size = 16)"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#observed-proportions",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#observed-proportions",
    "title": "16: Hypothesis Testing (1)",
    "section": "Observed Proportions",
    "text": "Observed Proportions\n\nCross Tabulationpercentagescode\n\n\n\n\n gender not promoted\n   male   3       21\n female  10       14\n\n\n\n\n\n\n gender    not promoted\n   male 12.50%   87.50%\n female 41.67%   58.33%\n\n\n\n\n\npromotions |&gt;\n  tabyl(gender, decision) |&gt;\n  adorn_percentages(\"row\") |&gt;\n  adorn_pct_formatting(digits = 2)\n\n\n\n\n\nmale promotion rate: 21/24 = 0.875\nfemale promotion rate: 14/24 = 0.583\ndifference in rates: 0.875 - 0.583 = 0.292\n\n\\[\\hat{p}_{m} - \\hat{p}_{f} = 0.292\\]"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#permutation-test",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#permutation-test",
    "title": "16: Hypothesis Testing (1)",
    "section": "Permutation Test",
    "text": "Permutation Test\n\n\n\n\n\n\nKey Observation\n\n\n\nIf gender did not matter when it comes to these job promotions, then it should not matter if we shuffle the gender labels in the data."
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#shuffling",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#shuffling",
    "title": "16: Hypothesis Testing (1)",
    "section": "Shuffling",
    "text": "Shuffling\n\npatchworkcode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noriginal_bar_graph &lt;- ggplot(promotions, aes(x = gender, fill = decision)) +\n  geom_bar() +\n  labs(title = \"original\",\n       x = \"Gender of name on resume\") +\n  theme(legend.position = \"none\")\n\ngender_shuffled &lt;- promotions\n\nset.seed(20241104)\ngender_shuffled$gender &lt;- sample(promotions$gender) \n#sampled without replacement\n\nshuffled_bar_graph &lt;- gender_shuffled %&gt;%\n  ggplot(aes(x = gender, fill = decision)) +\n  geom_bar() +\n  labs(title = \"shuffled\",\n       x = \"Gender of name on resume\")\n\n# patchwork\noriginal_bar_graph + shuffled_bar_graph"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#hypothesis-test-of-proportions",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#hypothesis-test-of-proportions",
    "title": "16: Hypothesis Testing (1)",
    "section": "Hypothesis Test of Proportions",
    "text": "Hypothesis Test of Proportions\n\n123456\n\n\n“First, a hypothesis is a statement about the value of an unknown population parameter. In our resume activity, our population parameter is the difference in population proportions \\(p_{m} - p_{f}\\)”\n\n\n“Second, a hypothesis test consists of a test between two competing hypotheses … Generally the null hypothesis is a claim that there really is ‘no effect’ or ‘no difference.’” Here our null hypothesis is\n\n\\(H_{0}\\): men and women are promoted at the same rate\n\n“Generally the alternative hypothesis is the claim the experimenter or researcher wants to establish or find evidence for and is viewed as a ‘challenger’ hypothesis to the null hypothesis”. Here our alternative hypothesis is\n\n\\(H_{a}\\): men are promoted at a higher rate than women\n\nIn math symbols, we have\n\\[\\begin{array}{rrcl}\n  H_{o}: p_{m} - p_{f} & = & 0 \\\\\n  H_{a}: p_{m} - p_{f} & &gt; & 0 \\\\\n\\end{array}\\]\n\n\n“Third, a test statistic is a point estimate/sample statistic formula used for hypothesis testing, where a sample statistic is merely a summary statistic based on a sample of observations.” Here, our test statistic \\(\\hat{p}_{m} - \\hat{p}_{f}\\) estimates the parameter of interest: the difference in population proportions \\(p_{m} - p_{f}\\)\n\n\n“Fourth, the observed test statistic is the value of the test statistic that we observed in real-life.” In this example the observed difference was\n\\[\\hat{p}_{m} - \\hat{p}_{f} = 0.875 - 0.583 = 0.292\\]\n\n\n“Fifth, the null distribution is the sampling distribution of the test statistic assuming the null hypothesis \\(H_0\\) is true.”\n\n\n\n\n\n\n\n\np-value\n\n\n\n“The p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true”"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#null-distribution",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#null-distribution",
    "title": "16: Hypothesis Testing (1)",
    "section": "Null Distribution",
    "text": "Null Distribution\n\nnull_distribution &lt;- promotions %&gt;% \n  specify(formula = decision ~ gender, success = \"promoted\") %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\") %&gt;% \n  calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))\n\n\nnull_distribution |&gt;\n  visualize(bins = 10)"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#observed-difference",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#observed-difference",
    "title": "16: Hypothesis Testing (1)",
    "section": "Observed Difference",
    "text": "Observed Difference\n\n# observed difference in proportions\nobs_diff_prop &lt;- promotions %&gt;% \n  specify(decision ~ gender, success = \"promoted\") %&gt;% \n  calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))\n#print(obs_diff_prop) #0.292\n\n\nnull_distribution |&gt;\n  visualize(bins = 10) +\n  \n  # choices for direction are \"right\", \"left\", and \"both\"\n  shade_p_value(obs_stat = obs_diff_prop,\n                direction = \"right\")"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#p-value-1",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#p-value-1",
    "title": "16: Hypothesis Testing (1)",
    "section": "p-value",
    "text": "p-value\n\nnull_distribution %&gt;% \n  get_p_value(obs_stat = obs_diff_prop, \n              direction = \"right\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.029\n\n\n\n\n\n\n\n\nWhen p-value &lt; 0.05\n\n\n\nFor NHST (null hypothesis significance testing), many scientists compare the p-value to a significance level of \\(\\alpha = 0.05\\). Since the p-value &lt; 0.05, we reject the null hypothesis of equal proportions of promotions among men and women."
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#data-cleaning",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#data-cleaning",
    "title": "16: Hypothesis Testing (1)",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nsummary(demo_df$SAT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      0    1500    1540    1617    1560   15560      24 \n\n\n\nSAT_df &lt;- demo_df |&gt;\n  select(SAT) |&gt;\n  filter(SAT &gt;= 400 & SAT &lt;= 1600)\n\n\nsummary(SAT_df$SAT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1100    1500    1540    1516    1560    1600 \n\n# observed sample mean: xbar = 1516"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#bootstrap-method",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#bootstrap-method",
    "title": "16: Hypothesis Testing (1)",
    "section": "Bootstrap Method",
    "text": "Bootstrap Method\n\nset.seed(20241104)\nbootstrap_distribution &lt;- SAT_df |&gt;\n  specify(response = SAT) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"mean\")\n\n\nbootstrap_distribution |&gt;\n  visualize() +\n  shade_ci(endpoints = bootstrap_distribution |&gt;\n             get_ci(level = 0.95))\n\n\n\n\n\n\n\n\n\nbootstrap_distribution |&gt; get_ci(level = 0.95)\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    1499.    1530.\n\n\nWe are 95 percent confident that the true average SAT score for a Princeton student is in between 1499 and 1530."
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#design",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#design",
    "title": "16: Hypothesis Testing (1)",
    "section": "Design",
    "text": "Design\n\nColloquially: The average SAT score for a Princeton student is above 1510.\nNull hypothesis: The average SAT score for a Princeton student is 1510.\nAlternative hypothesis: The average SAT score for a Princeton student is above 1510.\n\n\\[H_{0}: \\mu = 1510\\] \\[H_{a}: \\mu &gt; 1510\\]"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#boxplot",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#boxplot",
    "title": "16: Hypothesis Testing (1)",
    "section": "Boxplot",
    "text": "Boxplot\n\nMedianMeancode\n\n\nUsually, the middle line in a boxplot corresponds to the sample median.\n\n\n\n\n\n\n\n\n\n\n\nThere is an option to change the middle line in a boxplot to refer to the sample mean (but this is rarely done in practice)\n\n\n\n\n\n\n\n\n\n\n\n\nSAT_df |&gt;\n  ggplot() +\n  geom_hline(yintercept = 1510, color = \"red\", linewidth = 3) +\n  geom_boxplot(aes(y = SAT),\n               middle = mean(SAT_df$SAT),\n               alpha = 0.5,\n               color = princeton_black,\n               fill = princeton_orange) +\n  labs(title = \"SAT Scores for Princeton Students\",\n       subtitle = \"middle line: mean\",\n       caption = \"Sample among SML 201 students\", \n       x = \"\", y = \"SAT score\") +\n  scale_y_continuous(limits = c(1400, 1600)) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#t-statistic",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#t-statistic",
    "title": "16: Hypothesis Testing (1)",
    "section": "t statistic",
    "text": "t statistic\n\\[t = \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}}\\]\n\nn      &lt;- sum(!is.na(SAT_df$SAT))\nxbar   &lt;- mean(SAT_df$SAT, na.rm = TRUE)\ns      &lt;- sd(SAT_df$SAT, na.rm = TRUE)\nmu     &lt;- 1510\nt_stat &lt;- (xbar - mu) / (s/sqrt(n))"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#critical-region",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#critical-region",
    "title": "16: Hypothesis Testing (1)",
    "section": "Critical Region",
    "text": "Critical Region\n\nplotcodedisclaimer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# significance level: alpha = 0.05 ==&gt; top 5 percentile\n# df: degrees of freedom\ncrit_value &lt;- qt(0.95, df = n-1)\n\nvnorm(crit_value, section = \"upper\") +\n  labs(title = \"Critical Region\",\n       subtitle = \"One-sided hypothesis test\",\n       caption = \"SML 201\",\n       x = \"t distribution\")\n\n\n\n\n\n\n\n\n\nNormal Distribution Uusage\n\n\n\nWe used the normal distribution here in this example since\n\n\\(n &gt; 30\\)\nThe Central Limit Theorem said that sample distributions of the mean converge toward the normal distribution\nTeacher Derek didn’t have a t-distribution analogue for the vnorm helper function at this time."
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#comparison",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#comparison",
    "title": "16: Hypothesis Testing (1)",
    "section": "Comparison",
    "text": "Comparison\n\nplotscode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCritical Region\n\n\n\nThat is, in order to reject the null hypothesis, we wanted the \\(t\\) statistic to be inside of the critical region.\n\n\n\n\n\np1 &lt;- vnorm(crit_value, section = \"upper\") +\n  geom_vline(aes(xintercept = t_stat),\n             color = \"red\", linewidth = 2) +\n  labs(title = \"Critical Region\",\n       subtitle = \"One-sided hypothesis test\",\n       caption = \"SML 201\",\n       x = \"t distribution\") +\n  scale_x_continuous(breaks = t_stat,\n                     labels = \"t statistic\")\n\np2 &lt;- vnorm(t_stat, section = \"upper\") +\n  geom_vline(aes(xintercept = t_stat),\n             color = \"red\", linewidth = 2) +\n  labs(title = \"p-value\",\n       subtitle = \"One-sided hypothesis test\",\n       caption = \"SML 201\",\n       x = \"t distribution\") +\n  scale_x_continuous(breaks = t_stat,\n                     labels = \"t statistic\")\n\n# patchwork\np1 + p2"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#t.test",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#t.test",
    "title": "16: Hypothesis Testing (1)",
    "section": "t.test",
    "text": "t.test\n\nt.test(SAT_df$SAT, mu = 1510, alterative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  SAT_df$SAT\nt = 0.74335, df = 105, p-value = 0.4589\nalternative hypothesis: true mean is not equal to 1510\n95 percent confidence interval:\n 1500.719 1530.413\nsample estimates:\nmean of x \n 1515.566 \n\n\n\n\n\n\n\n\nWhen p-value &gt; 0.05\n\n\n\nFor NHST (null hypothesis significance testing), many scientists compare the p-value to a significance level of \\(\\alpha = 0.05\\). Since the p-value &gt; 0.05, we fail to reject the null hypothesis that the average SAT score of Princeton students is 1510.\n\n\n\n\n\n\n\n\nLeaving the t distribution behind\n\n\n\nFor these calculations\n\\[t = \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}}\\]\n\nrely more on summary statistics rather than all of the gathered data\n“degrees of freedom” is a rather convoluted notion\nt-distribution is itself an approximation\nleads to more reliance on abstract probability distributions\ndeparts from frequentist probability philosophy\nmore useful before calculators and computers"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#design-1",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#design-1",
    "title": "16: Hypothesis Testing (1)",
    "section": "Design",
    "text": "Design\n\nColloquially: The average SAT score for a Princeton student is above 1500.\nNull hypothesis: The average SAT score for a Princeton student is at most 1500.\nAlternative hypothesis: The average SAT score for a Princeton student is above 1500.\n\n\\[H_{0}: \\mu \\leq 1500\\] \\[H_{a}: \\mu &gt; 1500\\]"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#bootstrap-distribution",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#bootstrap-distribution",
    "title": "16: Hypothesis Testing (1)",
    "section": "Bootstrap Distribution",
    "text": "Bootstrap Distribution\nFor one-sided hypothesis tests, we still employ a bootstrap distribution.\n\nbootstrap_distribution &lt;- SAT_df |&gt;\n  specify(response = SAT) |&gt;\n  hypothesize(null = \"point\", mu = 1500) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"mean\")"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#observed-stat",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#observed-stat",
    "title": "16: Hypothesis Testing (1)",
    "section": "Observed Stat",
    "text": "Observed Stat\n\nobs_mean &lt;- SAT_df |&gt;\n  specify(response = SAT) |&gt;\n  # hypothesize(null = \"point\", mu = 40) |&gt;\n  # generate(reps = 1000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"mean\")"
  },
  {
    "objectID": "posts/16_nhst_one_sided/16_NHST_one_sided.html#p-value-2",
    "href": "posts/16_nhst_one_sided/16_NHST_one_sided.html#p-value-2",
    "title": "16: Hypothesis Testing (1)",
    "section": "p-value",
    "text": "p-value\n\nbootstrap_distribution |&gt;\n  visualize() +\n  shade_p_value(obs_mean, direction = \"right\")\n\n\n\n\n\n\n\n\n\nbootstrap_distribution |&gt;\n  get_p_value(obs_mean, direction = \"right\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.012\n\n\n\n\n\n\n\n\nWhen p-value &lt; 0.05\n\n\n\nSince the p-value &lt; 0.05, we reject the null hypothesis that the average SAT score of Princeton students is at most 1500."
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html",
    "title": "17: Hypothesis Testing (2)",
    "section": "",
    "text": "Libraries and Helper Functions\n\n\n\n\n\n\nlibrary(\"ggsignif\")   #put significance \"stars\" on boxplots\nlibrary(\"infer\")      #pipeline workflow for hypothesis testing\nlibrary(\"janitor\")    #compute proportions easily\nlibrary(\"moderndive\") #textbook's package and data\nlibrary(\"patchwork\")  #easily let's me show graphs side-by-side\nlibrary(\"tidyverse\")  #the overall programming style universe\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n# data set: SML 201 demographics survey\n# demo_df &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/04_categories/sml201survey.csv\")\ndemo_df &lt;- readr::read_csv(\"sml201survey.csv\")\n\n# helper function\nvnorm &lt;- function(x, mu = 0, sigma = 1, section = \"lower\"){\n  \n  # bell curve\n  x_vals &lt;- seq(mu - 4*sigma, mu + 4*sigma, length.out = 201)\n  y_vals &lt;- dnorm(x_vals, mu, sigma)\n  df_for_graph &lt;- data.frame(x_vals, y_vals)\n\n  # outline shaded regions\n  if(length(x) == 1){\n    shade_left &lt;- rbind(c(x[1],0), df_for_graph |&gt;\n                        filter(x_vals &lt; x[1]))\n    shade_right &lt;- rbind(c(x[1],0), df_for_graph |&gt;\n                        filter(x_vals &gt; x[1]))\n  }\n  if(length(x) == 2){\n    shade_between &lt;- rbind(c(x[1],0),\n                       df_for_graph |&gt;\n                         filter(x_vals &gt; x[1] &\n                                  x_vals &lt; x[2]),\n                       c(x[2],0))\n    shade_tails &lt;- rbind(df_for_graph |&gt;\n                        filter(x_vals &lt; x[1]),\n                     c(x[1],0),\n                     c(x[2],0),\n                     df_for_graph |&gt;\n                        filter(x_vals &gt; x[2]))\n  }\n  if(section == \"lower\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_left,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(pnorm(x,mu,sigma), 4)\n  }\n  if(section == \"upper\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_right,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- 1 - round(pnorm(x,mu,sigma), 4)\n  }\n  if(section == \"between\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_between,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(diff(pnorm(x,mu,sigma)), 4)\n  }\n  if(section == \"tails\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_tails,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(1 - diff(pnorm(x,mu,sigma)), 4)\n  }\n  \n  # plot bell curve\n  bell_curve + \n    labs(subtitle = paste0(\"Probability: \", prob_val),\n         caption = \"SML 201\", y = \"\") +\n    theme_minimal()\n}\n\n\n\n\n\n\n\n\n\nGoal: Continue to explore hypothesis testing and two-sided tests\nObjective: Get “stars” and significance levels\n\n\n\n\n\n\n\nFry"
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#start",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#start",
    "title": "17: Hypothesis Testing (2)",
    "section": "",
    "text": "Goal: Continue to explore hypothesis testing and two-sided tests\nObjective: Get “stars” and significance levels\n\n\n\n\n\n\n\nFry"
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#survey-data",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#survey-data",
    "title": "17: Hypothesis Testing (2)",
    "section": "Survey Data",
    "text": "Survey Data\n\nsummary(demo_df$GPA)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  0.000   3.406   3.600   3.547   3.800   4.650      13"
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#clean-data",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#clean-data",
    "title": "17: Hypothesis Testing (2)",
    "section": "Clean Data",
    "text": "Clean Data\n\ndemo_long &lt;- demo_df |&gt;\n  select(GPA, coffeeTea) |&gt;\n  filter(GPA &gt; 0 & GPA &lt;= 4.0) |&gt;\n  drop_na()\n\n\ndemo_long |&gt;\n  group_by(coffeeTea) |&gt;\n  summarize(xbar = mean(GPA))\n\n# A tibble: 3 × 2\n  coffeeTea  xbar\n  &lt;chr&gt;     &lt;dbl&gt;\n1 (neither)  3.63\n2 coffee     3.57\n3 tea        3.55"
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#boxplot",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#boxplot",
    "title": "17: Hypothesis Testing (2)",
    "section": "Boxplot",
    "text": "Boxplot\n\ndemo_long |&gt;\n  filter(coffeeTea %in% c(\"coffee\", \"tea\")) |&gt;\n  ggplot(aes(x = coffeeTea, y = GPA)) +\n  geom_boxplot(color = princeton_black, fill = princeton_orange) +\n  labs(title = \"Does Beverage Choice affect GPA?\",\n       subtitle = \"(toward two-means hypothesis testing)\",\n       caption = \"SML 201\",\n       x = \"beverage\", y = \"grade point average\") +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntuition\n\n\n\n\n\nThe boxplot visualization lets us guess at whether or not the means for the two groups are significantly different (or not)."
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#null-distribution",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#null-distribution",
    "title": "17: Hypothesis Testing (2)",
    "section": "Null Distribution",
    "text": "Null Distribution\nIn the modern approach, with the infer code package, we build a null distribution (i.e. assuming the same GPA for coffee and tea drinkers).\n\n\n\n\n\n\nPermutation Test\n\n\n\n\n\nIf the beverage choice here did not affect GPA, then it shouldn’t matter if we permute the “coffee” or “tea” labels.\n\n\n\n\nnull_distribution &lt;- demo_long |&gt;\n  filter(coffeeTea %in% c(\"coffee\", \"tea\")) |&gt;\n  specify(formula = GPA ~ coffeeTea) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  calculate(stat = \"diff in means\", order = c(\"coffee\", \"tea\"))\n\n\nobs_diff &lt;- demo_long |&gt;\n  filter(coffeeTea %in% c(\"coffee\", \"tea\")) |&gt;\n  specify(formula = GPA ~ coffeeTea) |&gt;\n  # hypothesize(null = \"independence\") |&gt;\n  # generate(reps = 1000, type = \"permute\") |&gt;\n  calculate(stat = \"diff in means\", order = c(\"coffee\", \"tea\"))"
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#visualization",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#visualization",
    "title": "17: Hypothesis Testing (2)",
    "section": "Visualization",
    "text": "Visualization\n\n\n\n\n\n\np-value\n\n\n\n“The p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true”\n\n\n\nnull_distribution |&gt;\n  visualize() +\n  labs(title = \"Does Beverage Affect GPA?\",\n       subtitle = \"Two-Sided Hypothesis Test\",\n       caption = \"SML 201\",\n       x = \"difference between coffee and tea drinkers\") +\n  shade_p_value(obs_diff, direction = \"two-sided\",\n                color = princeton_black, fill = princeton_orange) +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo-Sided\n\n\n\nSince our p-value search looks at both tails of the null distribution, we call this a two-sided test."
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#conclusion",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#conclusion",
    "title": "17: Hypothesis Testing (2)",
    "section": "Conclusion",
    "text": "Conclusion\n\nnull_distribution |&gt; get_p_value(obs_diff, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.676\n\n\n\n\n\n\n\n\nInconclusive\n\n\n\nSince the p-value &gt; 0.05, we have failed to reject the null hypothesis that the GPA for coffee and tea drinkers are the same (at the \\(\\alpha = 0.05\\) significance level).\nWe may treat this result as inconclusive or note that we have not found evidence here toward showing that beverage choice affects GPA attainment."
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#boxplot-1",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#boxplot-1",
    "title": "17: Hypothesis Testing (2)",
    "section": "Boxplot",
    "text": "Boxplot\n\ndemo_long &lt;- demo_df |&gt;\n  select(residentialCollege, flossing) |&gt;\n  drop_na()\n\ndemo_long |&gt;\n  filter(residentialCollege %in% c(\"New College West\", \"Rockefeller\")) |&gt;\n  ggplot(aes(x = residentialCollege, y = flossing)) +\n  geom_boxplot(color = princeton_black, fill = princeton_orange) +\n  labs(title = \"Does Residential College affect Flossing Frequency?\",\n       subtitle = \"(toward two-means hypothesis testing)\",\n       caption = \"SML 201\",\n       x = \"\", y = \"times flossing per week\") +\n  theme_minimal(base_size = 16)"
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#permutation-test-1",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#permutation-test-1",
    "title": "17: Hypothesis Testing (2)",
    "section": "Permutation Test",
    "text": "Permutation Test\n\nnull_distribution &lt;- demo_long |&gt;\n  filter(residentialCollege %in% c(\"New College West\", \"Rockefeller\")) |&gt;\n  specify(formula = flossing ~ residentialCollege) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  calculate(stat = \"diff in means\", order = c(\"New College West\", \"Rockefeller\"))\n\nobs_diff &lt;- demo_long |&gt;\n  filter(residentialCollege %in% c(\"New College West\", \"Rockefeller\")) |&gt;\n  specify(formula = flossing ~ residentialCollege) |&gt;\n  # hypothesize(null = \"independence\") |&gt;\n  # generate(reps = 1000, type = \"permute\") |&gt;\n  calculate(stat = \"diff in means\", order = c(\"New College West\", \"Rockefeller\"))\n\nnull_distribution |&gt;\n  visualize() +\n  labs(title = \"Does Living Location Affect Flossing Rate?\",\n       subtitle = \"Two-Sided Hypothesis Test\",\n       caption = \"SML 201\",\n       x = \"difference between New College West and Rockefeller\") +\n  shade_p_value(obs_diff, direction = \"two-sided\",\n                color = princeton_black, fill = princeton_orange) +\n  theme_minimal(base_size = 16)"
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#conclusion-1",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#conclusion-1",
    "title": "17: Hypothesis Testing (2)",
    "section": "Conclusion",
    "text": "Conclusion\n\nnull_distribution |&gt; get_p_value(obs_diff, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.018\n\n\n\n\n\n\n\n\nSome Evidence\n\n\n\nSince the p-value &lt; 0.05, we have reject the null hypothesis that the flossing frequency is the same for students at New College West and Rockefeller (at the \\(\\alpha = 0.05\\) significance level).\nWe may treat this result as having found some evidence that there may be a relationship between these two residential colleges and flossing frequency."
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#pooled-variance",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#pooled-variance",
    "title": "17: Hypothesis Testing (2)",
    "section": "Pooled Variance",
    "text": "Pooled Variance\nThe old-fashioned way of computing a two-means hypothesis test was conducted through the \\(t\\)-distribution through the pooled variance\n\\[t = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{\\sqrt{\\frac{s_{p}^{2}}{n_{1}} + \\frac{s_{p}^{2}}{n_{2}}}} \\text{ with } s_{p}^{2} = \\frac{(n_{1}-1)s_{1}^{2} + (n_{2}-1)^{2}s_{2}^{2}}{n_{1} + n_{2} - 2}\\]\n\ndemo_wide &lt;- demo_long |&gt;\n  pivot_wider(names_from = residentialCollege,\n              values_from = flossing,\n              values_fn = list)\n\n\nNCW &lt;- unlist(demo_wide$`New College West`)\nRocky &lt;- unlist(demo_wide$Rockefeller)\nxbar1 &lt;- mean(NCW)\nxbar2 &lt;- mean(Rocky)\ns1 &lt;- sd(NCW)\ns2 &lt;- sd(Rocky)\nn1 &lt;- length(NCW)\nn2 &lt;- length(Rocky)\npooled_var &lt;- ((n1 - 1)*s1^2 + (n2-1)*s2^2) / (n1 + n2 - 2)\nt_stat &lt;- (xbar1 - xbar2) / sqrt(pooled_var/n1 + pooled_var/n2)\ncrit_val &lt;- qt(c(0.025, 0.975), df = n1 + n2 - 2)"
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#comparison",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#comparison",
    "title": "17: Hypothesis Testing (2)",
    "section": "Comparison",
    "text": "Comparison\n\nplotscode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCritical Region\n\n\n\nThat is, in order to reject the null hypothesis, we wanted the \\(t\\) statistic to be inside of the critical region.\n\n\n\n\n\np1 &lt;- vnorm(crit_val, section = \"tails\") +\n  geom_vline(aes(xintercept = t_stat),\n             color = \"red\", linewidth = 2) +\n  labs(title = \"Critical Region\",\n       subtitle = \"Two-sided hypothesis test\",\n       caption = \"SML 201\",\n       x = \"t distribution\") +\n  scale_x_continuous(breaks = t_stat,\n                     labels = \"t statistic\")\n\np2 &lt;- vnorm(t_stat, section = \"lower\") +\n  geom_vline(aes(xintercept = t_stat),\n             color = \"red\", linewidth = 2) +\n  labs(title = \"p-value\",\n       subtitle = \"Two-sided hypothesis test\",\n       caption = \"SML 201\",\n       x = \"t distribution\") +\n  scale_x_continuous(breaks = t_stat,\n                     labels = \"t statistic\")\n\n# patchwork\np1 / p2"
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#t.test",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#t.test",
    "title": "17: Hypothesis Testing (2)",
    "section": "t.test",
    "text": "t.test\n\nt.test(NCW, Rocky, alternative = \"two.sided\")\n\n\n    Welch Two Sample t-test\n\ndata:  NCW and Rocky\nt = -2.5613, df = 21.1, p-value = 0.01815\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -5.8556895 -0.6085962\nsample estimates:\nmean of x mean of y \n 3.125000  6.357143 \n\n\n\n\n\n\n\n\nSome Evidence\n\n\n\nSince the p-value &lt; 0.05, we have reject the null hypothesis that the flossing frequency is the same for students at New College West and Rockefeller (at the \\(\\alpha = 0.05\\) significance level).\nWe may treat this result as having found some evidence that there may be a relationship between these two residential colleges and flossing frequency."
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#number-of-courses-versus-age",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#number-of-courses-versus-age",
    "title": "17: Hypothesis Testing (2)",
    "section": "Number of Courses versus Age",
    "text": "Number of Courses versus Age\n\nsetupplotcode\n\n\n\nresponse variable: numCourses (i.e. number of courses taken before this semester)\nexplanatory variable: age\n\n\\[\\hat{y} = \\beta_{0} + \\beta_{1}X_{1}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndemo_long &lt;- demo_df |&gt;\n  filter(age &gt;= 18 & age &lt;= 21) |&gt;\n  filter(!is.na(numCourses))\n\ndemo_long |&gt;\n  ggplot(aes(x = age, y = numCourses)) +\n  geom_point() +\n  geom_smooth(formula = \"y ~ x\",\n              method = \"lm\",\n              se = TRUE) +\n  labs(title = \"Number of Courses versus Age\",\n       subtitle = \"Is this slope significant?\",\n       caption = \"SML 201\",\n       y = \"number of courses taken\\nbefore this semester\") +\n  theme_minimal(base_size = 14)"
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#model-statistics",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#model-statistics",
    "title": "17: Hypothesis Testing (2)",
    "section": "Model Statistics",
    "text": "Model Statistics\n\nlin_fit &lt;- lm(numCourses ~ age, data = demo_long)\nsummary(lin_fit)\n\n\nCall:\nlm(formula = numCourses ~ age, data = demo_long)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.6380  -2.7257  -0.7257   2.8181  14.8181 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -92.9408     8.6081  -10.80   &lt;2e-16 ***\nage           5.4561     0.4408   12.38   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.105 on 121 degrees of freedom\nMultiple R-squared:  0.5588,    Adjusted R-squared:  0.5551 \nF-statistic: 153.2 on 1 and 121 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#nhst",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#nhst",
    "title": "17: Hypothesis Testing (2)",
    "section": "NHST",
    "text": "NHST\nFor a regression model, each coefficent is treated as a hypothesis test.\n\nnull hypothesis: coefficient \\(\\beta_{1}\\) is zero\nalternative hypothesis: coefficient \\(\\beta_{1}\\) is nonzero\n\n\\[H_{o}: \\beta_{1} = 0\\] \\[H_{a}: \\beta_{1} \\neq 0\\]\n\n\n\n\n\n\nSome Evidence\n\n\n\nSince the p-value &lt; 0.05, we have reject the null hypothesis that the \\(\\beta_{1}\\) coefficient is zero (at the \\(\\alpha = 0.05\\) significance level).\nWe may treat this result as having found some evidence that there may be a relationship between age and and the number of courses taken."
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#gpa-versus-shoe-size",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#gpa-versus-shoe-size",
    "title": "17: Hypothesis Testing (2)",
    "section": "GPA versus Shoe Size",
    "text": "GPA versus Shoe Size\n\nsetupplotcode\n\n\n\nresponse variable: GPA\nexplanatory variable: shoeSize\n\n\\[\\hat{y} = \\beta_{0} + \\beta_{1}X_{1}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndemo_long &lt;- demo_df |&gt;\n  filter(GPA &gt; 0 & GPA &lt;= 4.0) |&gt;\n  filter(shoeSize &gt;= 5 & shoeSize &lt;= 20)\n\ndemo_long |&gt;\n  ggplot(aes(x = shoeSize, y = GPA)) +\n  geom_point() +\n  geom_smooth(formula = \"y ~ x\",\n              method = \"lm\",\n              se = TRUE) +\n  labs(title = \"Number of Courses versus Age\",\n       subtitle = \"Is this slope significant?\",\n       caption = \"SML 201\",\n       y = \"number of courses taken\\nbefore this semester\") +\n  theme_minimal(base_size = 14)"
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#model-statistics-1",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#model-statistics-1",
    "title": "17: Hypothesis Testing (2)",
    "section": "Model Statistics",
    "text": "Model Statistics\n\nlin_fit &lt;- lm(GPA ~ shoeSize, data = demo_long)\nsummary(lin_fit)\n\n\nCall:\nlm(formula = GPA ~ shoeSize, data = demo_long)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.97540 -0.15362  0.03751  0.21608  0.47696 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.34304    0.11967  27.935   &lt;2e-16 ***\nshoeSize     0.02582    0.01354   1.907    0.059 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2943 on 114 degrees of freedom\nMultiple R-squared:  0.03093,   Adjusted R-squared:  0.02243 \nF-statistic: 3.638 on 1 and 114 DF,  p-value: 0.05898\n\n\n\n\n\n\n\n\nInconclusive\n\n\n\nSince the p-value &gt; 0.05, we have failed to reject the null hypothesis that the \\(\\beta_{1}\\) coefficient is zero (at the \\(\\alpha = 0.05\\) significance level).\nWe may treat this result as inconclusive or note that we have not found evidence here toward showing a relationship between GPA and shoe size."
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#significant",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#significant",
    "title": "17: Hypothesis Testing (2)",
    "section": "Significant",
    "text": "Significant\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndemo_long &lt;- demo_df |&gt;\n  select(residentialCollege, flossing) |&gt;\n  drop_na()\n\ndemo_long |&gt;\n  ggplot(aes(x = residentialCollege, y = flossing)) +\n  geom_boxplot(color = princeton_black, fill = princeton_orange) +\n  geom_signif(\n    comparisons = list(c(\"New College West\", \"Rockefeller\")),\n    map_signif_level = TRUE,\n    y_position = 10\n  ) +\n  labs(title = \"Does Residential College affect Flossing Frequency?\",\n       subtitle = \"(stars of significance)\",\n       caption = \"SML 201\",\n       x = \"\", y = \"times flossing per week\") +\n  theme_minimal(base_size = 14) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))"
  },
  {
    "objectID": "posts/17_nhst_two_sided/17_NHST_two_sided.html#not-significant",
    "href": "posts/17_nhst_two_sided/17_NHST_two_sided.html#not-significant",
    "title": "17: Hypothesis Testing (2)",
    "section": "Not Significant",
    "text": "Not Significant\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndemo_long &lt;- demo_df |&gt;\n  select(GPA, coffeeTea) |&gt;\n  filter(GPA &gt; 0 & GPA &lt;= 4.0) |&gt;\n  drop_na()\n\ndemo_long |&gt;\n  ggplot(aes(x = coffeeTea, y = GPA)) +\n  geom_boxplot(color = princeton_black, fill = princeton_orange) +\n  geom_signif(\n    comparisons = list(c(\"coffee\", \"tea\")),\n    map_signif_level = TRUE,\n    y_position = 4.2\n    \n  ) +\n  labs(title = \"Does Beverage Choice affect GPA?\",\n       subtitle = \"(stars of significance)\",\n       caption = \"SML 201\",\n       x = \"beverage\", y = \"grade point average\") +\n  scale_y_continuous(limits = c(2.0, 4.5)) +\n  theme_minimal(base_size = 14)"
  },
  {
    "objectID": "posts/18_intro_ml/18_intro_ml.html",
    "href": "posts/18_intro_ml/18_intro_ml.html",
    "title": "18: Introduction to Machine Learning",
    "section": "",
    "text": "Libraries and Helper Functions\n\n\n\n\n\n\nlibrary(\"ggtext\")\nlibrary(\"tidymodels\")\nlibrary(\"tidyverse\")\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n# data set: Tour de France\ntdf_winners &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-04-07/tdf_winners.csv')\n\n\n\n\n\n\n\nGoal: Introduce machine learning (ideas and terminology)\n\n\n\n\n\nObjectives:\nintroduce tidymodels package\npractice with a TidyTuesday data set\n\n\n\n\n\n\n\nstructurecolnames\n\n\n\nstr(tdf_winners, give.attr = FALSE)\n\nspc_tbl_ [106 × 19] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ edition      : num [1:106] 1 2 3 4 5 6 7 8 9 10 ...\n $ start_date   : Date[1:106], format: \"1903-07-01\" \"1904-07-02\" ...\n $ winner_name  : chr [1:106] \"Maurice Garin\" \"Henri Cornet\" \"Louis Trousselier\" \"René Pottier\" ...\n $ winner_team  : chr [1:106] \"La Française\" \"Conte\" \"Peugeot–Wolber\" \"Peugeot–Wolber\" ...\n $ distance     : num [1:106] 2428 2428 2994 4637 4488 ...\n $ time_overall : num [1:106] 94.6 96.1 NA NA NA ...\n $ time_margin  : num [1:106] 2.99 2.27 NA NA NA ...\n $ stage_wins   : num [1:106] 3 1 5 5 2 5 6 4 2 3 ...\n $ stages_led   : num [1:106] 6 3 10 12 5 13 13 3 13 13 ...\n $ height       : num [1:106] 1.62 NA NA NA NA NA 1.78 NA NA NA ...\n $ weight       : num [1:106] 60 NA NA NA NA NA 88 NA NA NA ...\n $ age          : num [1:106] 32 19 24 27 24 25 22 22 26 23 ...\n $ born         : Date[1:106], format: \"1871-03-03\" \"1884-08-04\" ...\n $ died         : Date[1:106], format: \"1957-02-19\" \"1941-03-18\" ...\n $ full_name    : chr [1:106] NA NA NA NA ...\n $ nickname     : chr [1:106] \"The Little Chimney-sweep\" \"Le rigolo (The joker)\" \"Levaloy / Trou-trou\" NA ...\n $ birth_town   : chr [1:106] \"Arvier\" \"Desvres\" \"Paris\" \"Moret-sur-Loing\" ...\n $ birth_country: chr [1:106] \"Italy\" \"France\" \"France\" \"France\" ...\n $ nationality  : chr [1:106] \" France\" \" France\" \" France\" \" France\" ...\n\n\n\n\n\ncolnames(tdf_winners)\n\n [1] \"edition\"       \"start_date\"    \"winner_name\"   \"winner_team\"  \n [5] \"distance\"      \"time_overall\"  \"time_margin\"   \"stage_wins\"   \n [9] \"stages_led\"    \"height\"        \"weight\"        \"age\"          \n[13] \"born\"          \"died\"          \"full_name\"     \"nickname\"     \n[17] \"birth_town\"    \"birth_country\" \"nationality\"  \n\n\n\n\n\n\nplotcode\n\n\n\n\nWarning: Removed 41 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\n\n\n\ntdf_winners |&gt;\n  mutate(year = edition + 1904) |&gt;\n  ggplot() +\n  # geom_point(aes(x = height, y = time_overall),\n  #            color = \"blue\") +\n  geom_text(aes(x = height, y = time_overall,\n                label = year), color = \"blue\") +\n  labs(title = \"Are taller bicyclists faster?\",\n       subtitle = \"featuring Tour de France winners\",\n       caption = \"Source: TidyTuesday\",\n       x = \"height (meters)\",\n       y = \"time (hours)\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/18_intro_ml/18_intro_ml.html#start",
    "href": "posts/18_intro_ml/18_intro_ml.html#start",
    "title": "18: Introduction to Machine Learning",
    "section": "",
    "text": "Libraries and Helper Functions\n\n\n\n\n\n\nlibrary(\"ggtext\")\nlibrary(\"tidymodels\")\nlibrary(\"tidyverse\")\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n# data set: Tour de France\ntdf_winners &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-04-07/tdf_winners.csv')\n\n\n\n\n\n\n\nGoal: Introduce machine learning (ideas and terminology)\n\n\n\n\n\nObjectives:\nintroduce tidymodels package\npractice with a TidyTuesday data set"
  },
  {
    "objectID": "posts/18_intro_ml/18_intro_ml.html#exploratory-data-analyses",
    "href": "posts/18_intro_ml/18_intro_ml.html#exploratory-data-analyses",
    "title": "18: Introduction to Machine Learning",
    "section": "",
    "text": "structurecolnames\n\n\n\nstr(tdf_winners, give.attr = FALSE)\n\nspc_tbl_ [106 × 19] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ edition      : num [1:106] 1 2 3 4 5 6 7 8 9 10 ...\n $ start_date   : Date[1:106], format: \"1903-07-01\" \"1904-07-02\" ...\n $ winner_name  : chr [1:106] \"Maurice Garin\" \"Henri Cornet\" \"Louis Trousselier\" \"René Pottier\" ...\n $ winner_team  : chr [1:106] \"La Française\" \"Conte\" \"Peugeot–Wolber\" \"Peugeot–Wolber\" ...\n $ distance     : num [1:106] 2428 2428 2994 4637 4488 ...\n $ time_overall : num [1:106] 94.6 96.1 NA NA NA ...\n $ time_margin  : num [1:106] 2.99 2.27 NA NA NA ...\n $ stage_wins   : num [1:106] 3 1 5 5 2 5 6 4 2 3 ...\n $ stages_led   : num [1:106] 6 3 10 12 5 13 13 3 13 13 ...\n $ height       : num [1:106] 1.62 NA NA NA NA NA 1.78 NA NA NA ...\n $ weight       : num [1:106] 60 NA NA NA NA NA 88 NA NA NA ...\n $ age          : num [1:106] 32 19 24 27 24 25 22 22 26 23 ...\n $ born         : Date[1:106], format: \"1871-03-03\" \"1884-08-04\" ...\n $ died         : Date[1:106], format: \"1957-02-19\" \"1941-03-18\" ...\n $ full_name    : chr [1:106] NA NA NA NA ...\n $ nickname     : chr [1:106] \"The Little Chimney-sweep\" \"Le rigolo (The joker)\" \"Levaloy / Trou-trou\" NA ...\n $ birth_town   : chr [1:106] \"Arvier\" \"Desvres\" \"Paris\" \"Moret-sur-Loing\" ...\n $ birth_country: chr [1:106] \"Italy\" \"France\" \"France\" \"France\" ...\n $ nationality  : chr [1:106] \" France\" \" France\" \" France\" \" France\" ...\n\n\n\n\n\ncolnames(tdf_winners)\n\n [1] \"edition\"       \"start_date\"    \"winner_name\"   \"winner_team\"  \n [5] \"distance\"      \"time_overall\"  \"time_margin\"   \"stage_wins\"   \n [9] \"stages_led\"    \"height\"        \"weight\"        \"age\"          \n[13] \"born\"          \"died\"          \"full_name\"     \"nickname\"     \n[17] \"birth_town\"    \"birth_country\" \"nationality\"  \n\n\n\n\n\n\nplotcode\n\n\n\n\nWarning: Removed 41 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\n\n\n\ntdf_winners |&gt;\n  mutate(year = edition + 1904) |&gt;\n  ggplot() +\n  # geom_point(aes(x = height, y = time_overall),\n  #            color = \"blue\") +\n  geom_text(aes(x = height, y = time_overall,\n                label = year), color = \"blue\") +\n  labs(title = \"Are taller bicyclists faster?\",\n       subtitle = \"featuring Tour de France winners\",\n       caption = \"Source: TidyTuesday\",\n       x = \"height (meters)\",\n       y = \"time (hours)\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/18_intro_ml/18_intro_ml.html#split",
    "href": "posts/18_intro_ml/18_intro_ml.html#split",
    "title": "18: Introduction to Machine Learning",
    "section": "Split",
    "text": "Split\n\ndata_split &lt;- initial_split(bike_df)\ntrain_df &lt;- training(data_split)\ntest_df &lt;- testing(data_split)\n\n\nprint(paste(\"The number of observations in the training set is:\", \n            nrow(train_df)))\n\n[1] \"The number of observations in the training set is: 46\"\n\nprint(paste(\"The number of observations in the testing set is:\", \n            nrow(test_df)))\n\n[1] \"The number of observations in the testing set is: 16\""
  },
  {
    "objectID": "posts/18_intro_ml/18_intro_ml.html#one-split",
    "href": "posts/18_intro_ml/18_intro_ml.html#one-split",
    "title": "18: Introduction to Machine Learning",
    "section": "One Split",
    "text": "One Split\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitle_string &lt;- \"&lt;span style='color:#000000'&gt;&lt;b&gt;Training Sets&lt;/b&gt;&lt;/span&gt; &lt;span style='color:#0000FF'&gt;and&lt;/span&gt; \n&lt;span style='color:#FF0000'&gt;&lt;b&gt;Testing Sets&lt;/b&gt;&lt;/span&gt;\"\n\ntrain_df |&gt;\n  ggplot(aes(x = height, y = pace)) +\n  geom_point(aes(color = \"training set\"), \n             # color = \"black\"\n             ) +\n  geom_smooth(method = \"lm\",\n              aes(x = height, y = pace),\n              color = \"black\",\n              data = train_df,\n              formula = \"y ~ x\",\n              se = FALSE) +\n  geom_point(aes(x = height, y = pace, color = \"testing set\"),\n             # color = \"red\",\n             data = test_df,\n             size = 3) +\n  labs(title = title_string,\n       subtitle = \"approx 75-25 percent split\",\n       caption = \"Math 32\",\n       x = \"height (meters)\",\n       y = \"pace (km/hr)\") +\n  scale_color_manual(name = \"Data Split\",\n                     breaks = c(\"training set\", \"testing set\"),\n                     values = c(\"training set\" = \"black\",\n                                \"testing set\" = \"red\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_markdown(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))"
  },
  {
    "objectID": "posts/18_intro_ml/18_intro_ml.html#many-splits",
    "href": "posts/18_intro_ml/18_intro_ml.html#many-splits",
    "title": "18: Introduction to Machine Learning",
    "section": "Many Splits",
    "text": "Many Splits\n\nplotcode\n\n\n\n\n\n\ntitle_string &lt;- \"&lt;span style='color:#000000'&gt;&lt;b&gt;Training Sets&lt;/b&gt;&lt;/span&gt; &lt;span style='color:#0000FF'&gt;and&lt;/span&gt; \n&lt;span style='color:#FF0000'&gt;&lt;b&gt;Testing Sets&lt;/b&gt;&lt;/span&gt;\"\n\nfor(i in 1:10){\n  \n  data_split &lt;- initial_split(df)\n  train_df &lt;- training(data_split)\n  test_df &lt;- testing(data_split)\n  \n  this_plot &lt;- train_df |&gt;\n    ggplot(aes(x = height, y = pace)) +\n    geom_point(aes(color = \"training set\"), \n               # color = \"black\"\n    ) +\n    geom_smooth(method = \"lm\",\n                aes(x = height, y = pace),\n                color = \"black\",\n                data = train_df,\n                formula = \"y ~ x\",\n              se = FALSE) +\n  geom_point(aes(x = height, y = pace, color = \"testing set\"),\n             # color = \"red\",\n             data = test_df,\n             size = 3) +\n  labs(title = title_string,\n       subtitle = \"approx 75-25 percent split\",\n       caption = \"Math 32\",\n       x = \"height (meters)\",\n       y = \"pace (km/hr)\") +\n  scale_color_manual(name = \"Data Split\",\n                     breaks = c(\"training set\", \"testing set\"),\n                     values = c(\"training set\" = \"black\",\n                                \"testing set\" = \"red\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_markdown(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n  \n  ggsave(paste0(\"images/plot\", i, \".png\"),\n         plot = this_plot,\n         device = \"png\")\n}"
  },
  {
    "objectID": "posts/18_intro_ml/18_intro_ml.html#mse",
    "href": "posts/18_intro_ml/18_intro_ml.html#mse",
    "title": "18: Introduction to Machine Learning",
    "section": "MSE",
    "text": "MSE\nOne metric for evaluating a regression model (i.e. numerical predictions for the response variable) is mean square error of the test set.\n\\[\\text{MSE} = \\ds\\frac{1}{n_{\\text{test}}}\\sum_{j = 1}^{n_{\\text{test}}} (y_{j} - \\hat{y}_{j})^{2}\\]\n\ndata_split &lt;- initial_split(bike_df)\ntrain_df &lt;- training(data_split)\ntest_df &lt;- testing(data_split)\n\nlm_train &lt;- linear_reg() |&gt; \n  set_engine(\"lm\") |&gt;\n  fit(pace ~ height + weight + age, data = train_df)\n\n\nn_test &lt;- nrow(test_df)\ntrue_values &lt;- test_df$pace\npredictions &lt;- predict(lm_train, new_data = test_df |&gt; select(-pace))\n\nMSE &lt;- (1/n_test)*sum((true_values - predictions)^2)\nprint(MSE)\n\n[1] 3.342595"
  },
  {
    "objectID": "posts/18_intro_ml/18_intro_ml.html#second-model",
    "href": "posts/18_intro_ml/18_intro_ml.html#second-model",
    "title": "18: Introduction to Machine Learning",
    "section": "Second Model",
    "text": "Second Model\n\nset.seed(201)\nN &lt;- 25 #number of replicates\nMSE_vec &lt;- rep(NA, N)\n\nfor(i in 1:N){\n  data_split &lt;- initial_split(bike_df)\n  train_df &lt;- training(data_split)\n  test_df &lt;- testing(data_split)\n  \n  lm_train &lt;- linear_reg() |&gt; \n    set_engine(\"lm\") |&gt;\n    # fit(pace ~ height + weight + age, data = train_df)\n    fit(pace ~ height + weight + age + height:weight + height:age + weight:age +\n        height:weight:age,\n      data = train_df)\n  \n  n_test &lt;- nrow(test_df)\n  true_values &lt;- test_df$pace\n  predictions &lt;- predict(lm_train, new_data = test_df |&gt; select(-pace))\n  \n  MSE_vec[i] &lt;- (1/n_test)*sum((true_values - predictions)^2)\n}\n\n# vector of MSE\nMSE_vec\n\n [1]  3.381904  6.009615  2.841596  9.032613  5.648347  5.244057 14.174292\n [8]  7.890369  6.835132  7.788564 13.704889  5.324071  6.282300  2.811976\n[15]  5.136457  5.586255  5.446546  4.333529 12.666760  7.113682  8.068847\n[22]  6.562516 10.239050  6.873517  6.590273\n\n\n\n# cross-validation error\ncv_error &lt;- mean(MSE_vec)\ncv_error\n\n[1] 7.023486"
  },
  {
    "objectID": "posts/18_intro_ml/18_intro_ml.html#misbehaviorx",
    "href": "posts/18_intro_ml/18_intro_ml.html#misbehaviorx",
    "title": "18: Introduction to Machine Learning",
    "section": "MisbehaviorX",
    "text": "MisbehaviorX\n\nVehiGAN AI algorithm to instantaneouly detect malicious signal attacks\n\nMd Hasan Shahriar, Mohammad Raashid Ansari, Jean-Philippe Monteuuis, Cong Chen, Jonathan Petit, Y. Thomas Hou, Wenjing Lou\npresented their work at the ICDCS 2024 conference in Jersey City in the summer of 2024\n\nrealms: machine learning, artificial intelligence, cybersecurity\n\n\n\n\nVehiGAN diagrams\n\n\n\nprovided 0.1% stratified sample of the MisbehaviorX data set\n\nvery new data set (originally 8 GB)\n\nconduct over 100 hypothesis tests!"
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html",
    "href": "posts/19_power_analysis/19_power.html",
    "title": "19: Power Analysis",
    "section": "",
    "text": "Libraries and Helper Functions\n\n\n\n\n\n\nlibrary(\"infer\")\nlibrary(\"janitor\")\nlibrary(\"pwr\")\nlibrary(\"tidyverse\")\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n# data set: GPT detectors\nGPT_detectors &lt;- readr::read_csv('detectors.csv')\n# GPT_detectors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-07-18/detectors.csv')\n\n\n\n\n\n\n\nGoal: Introduce power analysis\n\n\n\n\n\nObjectives:\ncontingency tables\np-hacking\neffect size (Cohen’s h)\npower analyses\n\n\n\n\n\n\n\n\n\nGPT Detectors for academic dishonesty\ndata hosted at TidyTuesday\nPaper: GPT Detectors Are Biased Against Non-Native English Writers\n\n\n\n\n\n\n\nMr Belding\n\n\n\n\n\n\n\n\nChatbotsDetectorsCountscode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  kind   AI Human Total\n    AI 1158  2559  3717\n Human  449  2019  2468\n Total 1607  4578  6185\n\n\n\n\n\nGPT_detectors |&gt;\n  ggplot(aes(x = model)) +\n  geom_bar(aes(fill = .pred_class), \n           position = \"fill\", stat = \"count\") +\n  labs(title = \"GPT Chatbots\",\n       subtitle = \"Proportions of essays that were deemed AI or human\",\n       caption = \"SML 201\",\n       x = \"chatbot\",\n       y = \"proportions\") +\n  theme_minimal(base_size = 14)\n\n\nGPT_detectors |&gt;\n  ggplot(aes(x = detector)) +\n  geom_bar(aes(fill = .pred_class), \n           position = \"fill\", stat = \"count\") +\n  labs(title = \"GPT Detectors\",\n       subtitle = \"Proportions of essays that were deemed AI or human\",\n       caption = \"SML 201\",\n       x = \"detector\",\n       y = \"proportions\") +\n  theme_minimal(base_size = 14) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\nGPT_detectors |&gt;\n  tabyl(kind, .pred_class) |&gt;\n  adorn_totals(c(\"row\", \"col\"))"
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#start",
    "href": "posts/19_power_analysis/19_power.html#start",
    "title": "19: Power Analysis",
    "section": "",
    "text": "Libraries and Helper Functions\n\n\n\n\n\n\nlibrary(\"infer\")\nlibrary(\"janitor\")\nlibrary(\"pwr\")\nlibrary(\"tidyverse\")\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n# data set: GPT detectors\nGPT_detectors &lt;- readr::read_csv('detectors.csv')\n# GPT_detectors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-07-18/detectors.csv')\n\n\n\n\n\n\n\nGoal: Introduce power analysis\n\n\n\n\n\nObjectives:\ncontingency tables\np-hacking\neffect size (Cohen’s h)\npower analyses"
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#data-gpt-detectors",
    "href": "posts/19_power_analysis/19_power.html#data-gpt-detectors",
    "title": "19: Power Analysis",
    "section": "",
    "text": "GPT Detectors for academic dishonesty\ndata hosted at TidyTuesday\nPaper: GPT Detectors Are Biased Against Non-Native English Writers\n\n\n\n\n\n\n\nMr Belding"
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#exploratory-data-analysis",
    "href": "posts/19_power_analysis/19_power.html#exploratory-data-analysis",
    "title": "19: Power Analysis",
    "section": "",
    "text": "ChatbotsDetectorsCountscode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  kind   AI Human Total\n    AI 1158  2559  3717\n Human  449  2019  2468\n Total 1607  4578  6185\n\n\n\n\n\nGPT_detectors |&gt;\n  ggplot(aes(x = model)) +\n  geom_bar(aes(fill = .pred_class), \n           position = \"fill\", stat = \"count\") +\n  labs(title = \"GPT Chatbots\",\n       subtitle = \"Proportions of essays that were deemed AI or human\",\n       caption = \"SML 201\",\n       x = \"chatbot\",\n       y = \"proportions\") +\n  theme_minimal(base_size = 14)\n\n\nGPT_detectors |&gt;\n  ggplot(aes(x = detector)) +\n  geom_bar(aes(fill = .pred_class), \n           position = \"fill\", stat = \"count\") +\n  labs(title = \"GPT Detectors\",\n       subtitle = \"Proportions of essays that were deemed AI or human\",\n       caption = \"SML 201\",\n       x = \"detector\",\n       y = \"proportions\") +\n  theme_minimal(base_size = 14) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\nGPT_detectors |&gt;\n  tabyl(kind, .pred_class) |&gt;\n  adorn_totals(c(\"row\", \"col\"))"
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#quartet",
    "href": "posts/19_power_analysis/19_power.html#quartet",
    "title": "19: Power Analysis",
    "section": "Quartet",
    "text": "Quartet\nAddressing: Are the detectors correctly identifying AI-generated work as works of AI?\n\npositive: AI\nnegative: human\n\n\n# True positive: AI classification of AI work\nTP &lt;- cross_tab[1,2]\n# False negative: Human classification of AI work\nFN &lt;- cross_tab[1,3]\n# False positive: AI classification of human work\nFP &lt;- cross_tab[2,2]\n# True negative: Human classification of human work\nTN &lt;- cross_tab[2,3]"
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#type-i-and-type-ii",
    "href": "posts/19_power_analysis/19_power.html#type-i-and-type-ii",
    "title": "19: Power Analysis",
    "section": "Type I and Type II",
    "text": "Type I and Type II\n\nType I errors: false positive\nType II errors: false negative\n\n\nprint(paste0(\"The number of Type I errors is: \", FP))\n\n[1] \"The number of Type I errors is: 449\"\n\nprint(paste0(\"The number of Type II errors is: \", FN))\n\n[1] \"The number of Type II errors is: 2559\""
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#metrics",
    "href": "posts/19_power_analysis/19_power.html#metrics",
    "title": "19: Power Analysis",
    "section": "Metrics",
    "text": "Metrics\nThere are many metrics for confusion matrices.\n\nFalse Positive Rate (Fallout)\n\\[FPR = \\frac{FP}{FP + TN}\\]\n\nFPR &lt;- FP / (FP + TN)\nprint(paste0(\"The false positive rate is \", round(100*FPR, 2), \" percent\"))\n\n[1] \"The false positive rate is 18.19 percent\"\n\n\n\n\nAccuracy\n\\[\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}\\]\n\nacc &lt;- (TP + TN) / (TP + FP + FN + TN)\nprint(paste0(\"The accuracy is \", round(100*acc, 2), \" percent\"))\n\n[1] \"The accuracy is 51.37 percent\""
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#catalyst",
    "href": "posts/19_power_analysis/19_power.html#catalyst",
    "title": "19: Power Analysis",
    "section": "Catalyst",
    "text": "Catalyst\n\n\n\nProfessor Daryl Bem, Cornell University\n2011 peer-reviewed and publised paper\ndemonstrated that college students have ESP (extra sensory perception)\n\nachieved p-value &lt; 0.05\nachieved some effect size\n\n\n\n\n\n\n\n\n(not a picture of Daryl Bem)"
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#p-hacking",
    "href": "posts/19_power_analysis/19_power.html#p-hacking",
    "title": "19: Power Analysis",
    "section": "p-hacking",
    "text": "p-hacking\n\n\n\nThe more statistical analyses a researcher runs—the more hypotheses they “test”—the more likely that at least one of these analyses will produce a result that appears to be “significant”, simply by chance. — Sean Trott\n\n\nXKCD: Significant\np-hacking in R by Sean Trott\n\n\n\n\n\n\n\nProject 3\n\n\n\nDisclosure: the requested tasks in Project 3 may constitute “p-hacking”\n\n\n\n\n\n\n\n\nsignificant beans!"
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#gaussian-white-noise",
    "href": "posts/19_power_analysis/19_power.html#gaussian-white-noise",
    "title": "19: Power Analysis",
    "section": "Gaussian White Noise",
    "text": "Gaussian White Noise\nIn this exploration, we are going to try to extract “significance” (p-value &lt; 0.05) from Gaussian white noise\n\\[X, Y \\sim N(\\mu = 201, \\sigma^{2} = 25^{2})\\]\n\nx_vals &lt;- rnorm(100, 201, 25)\ny_vals &lt;- rnorm(100, 201, 25) #that is, same mean\n\n\ncor_val &lt;- cor(x_vals, y_vals)\n\ndata.frame(x_vals, y_vals) |&gt;\n  ggplot(aes(x = x_vals, y = y_vals)) +\n  coord_equal() +\n  geom_point(color = \"gray50\") +\n  geom_smooth(formula = \"y ~ x\",\n              method = \"lm\") +\n  labs(title = \"Gaussian White Noise\",\n       subtitle = paste0(\"Two normally distributed vectors\\nwith the same mean\\nr: \", round(cor_val, 4)),\n       caption = \"SML 201\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nN &lt;- 1000 #number of replicates\nslopes &lt;- rep(NA, N)\np_vals &lt;- rep(NA, N)\n\nfor(i in 1:N){\n  this_x &lt;- sample(x_vals) #permutation\n  this_lm &lt;- summary(lm(y_vals ~ this_x))\n  slopes[i] &lt;- this_lm$coefficients[2]\n  p_vals[i] &lt;- this_lm$coefficients[8]\n}\n\n\ndf_for_graph &lt;- data.frame(slopes, p_vals) |&gt;\n  mutate(result = ifelse(p_vals &lt; 0.05, \"significant\", \"not significant\"))\n\n\ndf_for_graph |&gt;\n  ggplot(aes(x = slopes, y = p_vals, color = result)) +\n  geom_point() +\n  labs(title = \"The Search for Significance\",\n       subtitle = \"After resampling the noise, we may have found significance\",\n       caption = \"Source: Sean Trott\",\n       y = \"p-value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndetection_rate &lt;- mean(p_vals &lt; 0.05)\n\nWe have found “significance” in 4.5 percent of our replicates.\n\n\n\n\n\n\np-hacking\n\n\n\nWhat if a research team reported findings of “significance” from the 4.5 percent and discarded results from the other 95.5 percent?\n\n\n\n\n\n\n\n\nMitigating p-Hacking\n\n\n\n\n\n\nResearchers can pre-register their studies (usually around the same time as the ethics approval) to show what they are trying to measure first.\nReport confidence intervals too\nReport effect sizes too"
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#cohen",
    "href": "posts/19_power_analysis/19_power.html#cohen",
    "title": "19: Power Analysis",
    "section": "Cohen",
    "text": "Cohen\n\n\n\nProfessor Jacob Cohen, NYU\nadvocate of power analysis and effect size\ncritic of NHST\nknown for\n\nCohen’s kappa\nCohen’s h\nCohen’s d\n\n\n\n\n\n\n\n\n(not a picture of Jacob Cohen)"
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#cohens-h",
    "href": "posts/19_power_analysis/19_power.html#cohens-h",
    "title": "19: Power Analysis",
    "section": "Cohens h",
    "text": "Cohens h\nCohen’s h is a measure of distance between two proportions\n\\[h = |2\\text{arcsin}\\sqrt{p_{1}} - 2\\text{arcsin}\\sqrt{p_{2}}|\\]\n\nsuggested interpretation:\n\n\\(h = 0.20\\): “small effect size”\n\\(h = 0.50\\): “medium effect size”\n\\(h = 0.80\\): “large effect size”\n\n\n\n# sensitivity: probability of detection\np1 &lt;- TP / (TP + FN)\n# false positive rate\np2 &lt;- FP / (TN + FP)\npwr::ES.h(p1, p2)\n\n[1] 0.3030231\n\n\nWe have found a small effect size in GPT detection rate moving from human-generated essays to AI-generated essays."
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#counts-1",
    "href": "posts/19_power_analysis/19_power.html#counts-1",
    "title": "19: Power Analysis",
    "section": "Counts",
    "text": "Counts\n\nGPT_detectors |&gt;\n  filter(!is.na(native)) |&gt;\n  tabyl(native, .pred_class) |&gt;\n  adorn_totals(c(\"row\", \"col\"))\n\n native  AI Human Total\n     No 390   247   637\n    Yes  59  1772  1831\n  Total 449  2019  2468"
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#confusion-matrix-1",
    "href": "posts/19_power_analysis/19_power.html#confusion-matrix-1",
    "title": "19: Power Analysis",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\ncross_tab_2 &lt;- GPT_detectors |&gt;\n  filter(!is.na(native)) |&gt;\n  tabyl(native, .pred_class)\nTP_2 &lt;- cross_tab_2[1,2]\nFN_2 &lt;- cross_tab_2[1,3]\nFP_2 &lt;- cross_tab_2[2,2]\nTN_2 &lt;- cross_tab_2[2,3]"
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#nhst-1",
    "href": "posts/19_power_analysis/19_power.html#nhst-1",
    "title": "19: Power Analysis",
    "section": "NHST",
    "text": "NHST\n\nnull_distribution &lt;- GPT_detectors |&gt;\n  filter(!is.na(native)) |&gt;\n  specify(formula = .pred_class ~ native, success = \"AI\") |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  calculate(stat = \"diff in props\", order = c(\"No\", \"Yes\"))\n\nobs_diff &lt;- GPT_detectors |&gt;\n  filter(!is.na(native)) |&gt;\n  specify(formula = .pred_class ~ native, success = \"AI\") |&gt;\n  # hypothesize(null = \"independence\") |&gt;\n  # generate(reps = 1000, type = \"permute\") |&gt;\n  calculate(stat = \"diff in props\", order = c(\"No\", \"Yes\"))\n\nnull_distribution |&gt;\n  get_p_value(obs_diff, direction = \"greater\") |&gt; \n  pull()\n\n[1] 0\n\n\nSince the p-value &lt; 0.05, we reject the null hypothesis that the AI classification rate is the same for essays written by both native and non-native English speakers (at the \\(\\alpha = 0.05\\) significance level)."
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#effect-size-1",
    "href": "posts/19_power_analysis/19_power.html#effect-size-1",
    "title": "19: Power Analysis",
    "section": "Effect Size",
    "text": "Effect Size\n\n# sensitivity: probability of detection\np1_2 &lt;- TP_2 / (TP_2 + FN_2)\n# false positive rate\np2_2 &lt;- FP_2 / (TN_2 + FP_2)\npwr::ES.h(p1_2, p2_2)\n\n[1] 1.436245\n\n\nWe have found a large effect size in GPT detection rate moving from native-speaker writers to non-native writers."
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#seeking-power",
    "href": "posts/19_power_analysis/19_power.html#seeking-power",
    "title": "19: Power Analysis",
    "section": "Seeking Power",
    "text": "Seeking Power\nFor the first comparison between the AI-generated essays and the human-generated essays, what was the power of the NHST for GPT detection?\n\npwr.p.test(alternative = \"greater\",\n           h = ES.h(p1,p2),\n           n = sum(!is.na(GPT_detectors$kind)),\n           # power = ? #i.e. this is what we are trying to find\n           sig.level = 0.05\n           )\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.3030231\n              n = 6185\n      sig.level = 0.05\n          power = 1\n    alternative = greater\n\n\nFor the second comparison between the native speaker written essays and the non-native written essays, what was the power of the NHST for GPT detection?\n\npwr.p.test(alternative = \"greater\",\n           h = ES.h(p1_2,p2_2),\n           n = sum(!is.na(GPT_detectors$native)),\n           # power = ? #i.e. this is what we are trying to find\n           sig.level = 0.05\n           )\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 1.436245\n              n = 2468\n      sig.level = 0.05\n          power = 1\n    alternative = greater"
  },
  {
    "objectID": "posts/19_power_analysis/19_power.html#seeking-sample-size",
    "href": "posts/19_power_analysis/19_power.html#seeking-sample-size",
    "title": "19: Power Analysis",
    "section": "Seeking Sample Size",
    "text": "Seeking Sample Size\nFor the first comparison between the AI-generated essays and the human-generated essays, how large should the sample size be (at least) to achieve (at least) 0.80 power?\n\npwr.p.test(alternative = \"greater\",\n           h = ES.h(p1,p2),\n           #n = ? #i.e. this is what we are trying to find\n           power = 0.80, \n           sig.level = 0.05\n           )\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.3030231\n              n = 67.33124\n      sig.level = 0.05\n          power = 0.8\n    alternative = greater\n\n\n\nThis experiment needed at least 68 essays."
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html",
    "href": "posts/20_supervised_learning/20_supervised_learning.html",
    "title": "20: Supervised Learning",
    "section": "",
    "text": "Libraries and Loading the Data\n\n\n\n\n\n\nlibrary(\"corrplot\")\nlibrary(\"tidyclust\")\nlibrary(\"tidymodels\")\nlibrary(\"tidyverse\")\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n# data set: GPT detectors\noffice_raw &lt;- readr::read_csv('office_sentiment.csv')\n# office_raw &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/data/office_sentiment.csv\")\n\n\n\n\n\n\n\nGoal: Explore topics in supervised learning\n\n\n\n\n\nObjectives:\ntidymodels\nridge regression\nLASSO regression"
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html#start",
    "href": "posts/20_supervised_learning/20_supervised_learning.html#start",
    "title": "20: Supervised Learning",
    "section": "",
    "text": "Libraries and Loading the Data\n\n\n\n\n\n\nlibrary(\"corrplot\")\nlibrary(\"tidyclust\")\nlibrary(\"tidymodels\")\nlibrary(\"tidyverse\")\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n# data set: GPT detectors\noffice_raw &lt;- readr::read_csv('office_sentiment.csv')\n# office_raw &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/data/office_sentiment.csv\")\n\n\n\n\n\n\n\nGoal: Explore topics in supervised learning\n\n\n\n\n\nObjectives:\ntidymodels\nridge regression\nLASSO regression"
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html#filtering",
    "href": "posts/20_supervised_learning/20_supervised_learning.html#filtering",
    "title": "20: Supervised Learning",
    "section": "Filtering",
    "text": "Filtering\n\noffice_df &lt;- office_raw |&gt;\n  select(imdb_rating,\n         season, episode, total_votes,\n         sentimentAnalysis_score, sentimentr_score, syuzhet_score) |&gt;\n  rename(ep_in_season = episode,\n         sent1 = syuzhet_score,\n         sent2 = sentimentr_score,\n         sent3 = sentimentAnalysis_score)"
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html#derived-variables",
    "href": "posts/20_supervised_learning/20_supervised_learning.html#derived-variables",
    "title": "20: Supervised Learning",
    "section": "Derived Variables",
    "text": "Derived Variables\n\noffice_df &lt;- office_df |&gt;\n  group_by(season, ep_in_season) |&gt;\n  mutate(sent1_mean = mean(sent1, na.rm = TRUE),\n         sent2_mean = mean(sent2, na.rm = TRUE),\n         sent3_mean = mean(sent3, na.rm = TRUE),\n         sent1_dev = sd(sent1, na.rm = TRUE),\n         sent2_dev = sd(sent2, na.rm = TRUE),\n         sent3_dev = sd(sent3, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(imdb_rating,\n         sent1_mean, sent2_mean, sent3_mean,\n         sent1_dev, sent2_dev, sent3_dev,\n         season, ep_in_season, total_votes) |&gt;\n  distinct() |&gt;\n  mutate(episode_num = 1:n())"
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html#correlation-plot",
    "href": "posts/20_supervised_learning/20_supervised_learning.html#correlation-plot",
    "title": "20: Supervised Learning",
    "section": "Correlation Plot",
    "text": "Correlation Plot\n\noffice_df |&gt;\n  cor(use = \"pairwise.complete.obs\") |&gt;\n  corrplot(method = \"number\",\n           order = \"FPC\",\n           type = \"upper\")"
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html#penalties",
    "href": "posts/20_supervised_learning/20_supervised_learning.html#penalties",
    "title": "20: Supervised Learning",
    "section": "Penalties",
    "text": "Penalties\n\ntidy(ridge_fit, penalty = 0)\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\n\n# A tibble: 11 × 3\n   term          estimate penalty\n   &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)   7.48           0\n 2 sent1_mean   -0.336          0\n 3 sent2_mean   -0.911          0\n 4 sent3_mean   -0.247          0\n 5 sent1_dev    -0.684          0\n 6 sent2_dev     1.32           0\n 7 sent3_dev     0.846          0\n 8 season        0.00749        0\n 9 ep_in_season  0.0144         0\n10 total_votes   0.000371       0\n11 episode_num  -0.00222        0\n\n\n\ntidy(ridge_fit, penalty = 201)\n\n# A tibble: 11 × 3\n   term            estimate penalty\n   &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)   8.25           201\n 2 sent1_mean    0.0000141      201\n 3 sent2_mean    0.00537        201\n 4 sent3_mean   -0.000168       201\n 5 sent1_dev    -0.000994       201\n 6 sent2_dev     0.0118         201\n 7 sent3_dev     0.00446        201\n 8 season       -0.000234       201\n 9 ep_in_season  0.0000353      201\n10 total_votes   0.00000117     201\n11 episode_num  -0.0000102      201\n\n\n\nridge_fit |&gt; autoplot()"
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html#predictions",
    "href": "posts/20_supervised_learning/20_supervised_learning.html#predictions",
    "title": "20: Supervised Learning",
    "section": "Predictions",
    "text": "Predictions\n\npredict(ridge_fit,\n        new_data = office_df |&gt; select(-imdb_rating),\n        penalty = 0) |&gt;\n  head()\n\n# A tibble: 6 × 1\n  .pred\n  &lt;dbl&gt;\n1  8.75\n2  8.68\n3  8.50\n4  8.36\n5  8.67\n6  8.65\n\n\n\npredict(ridge_fit,\n        new_data = office_df |&gt; select(-imdb_rating),\n        penalty = 201) |&gt;\n  head()\n\n# A tibble: 6 × 1\n  .pred\n  &lt;dbl&gt;\n1  8.25\n2  8.25\n3  8.25\n4  8.25\n5  8.25\n6  8.25"
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html#lambda-grid",
    "href": "posts/20_supervised_learning/20_supervised_learning.html#lambda-grid",
    "title": "20: Supervised Learning",
    "section": "Lambda Grid",
    "text": "Lambda Grid\n\nlambda_vals &lt;- 10^seq(-3,3, length.out = 10)\nformat(lambda_vals, scientific=FALSE)\n\n [1] \"   0.001000000\" \"   0.004641589\" \"   0.021544347\" \"   0.100000000\"\n [5] \"   0.464158883\" \"   2.154434690\" \"  10.000000000\" \"  46.415888336\"\n [9] \" 215.443469003\" \"1000.000000000\"\n\nlambda_grid &lt;- grid_regular(penalty(range = c(-3, 3)),\n                            levels = 50)\n\n\nData Split\n\noffice_split &lt;- initial_split(office_df, strata = \"imdb_rating\")\noffice_train &lt;- training(office_split)\noffice_test  &lt;- testing(office_split)\noffice_fold  &lt;- vfold_cv(office_train, v = 10)\n\n\n\nPreprocessing\n\nridge_recipe &lt;- recipe(formula = imdb_rating ~ ., \n                       data = office_train) |&gt;\n  step_novel(all_nominal_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_zv(all_predictors())\n\n\n\nSpecification\n\nridge_spec &lt;- linear_reg(mixture = 0, penalty = tune()) |&gt;\n  set_engine(\"glmnet\")\n\n\n\nWorkflow\n\nridge_workflow &lt;- workflow() |&gt;\n  add_recipe(ridge_recipe) |&gt;\n  add_model(ridge_spec)\n\n\n\nTuning\n\ntune_results &lt;- tune_grid(\n  ridge_workflow,\n  resamples = office_fold,\n  grid = lambda_grid\n)\n\n\ntune_results |&gt; autoplot()\n\n\n\n\n\n\n\n\n\ntune_results |&gt; collect_metrics() |&gt; head()\n\n# A tibble: 6 × 7\n  penalty .metric .estimator  mean     n std_err .config              \n    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.001   rmse    standard   0.375    10  0.0358 Preprocessor1_Model01\n2 0.001   rsq     standard   0.498    10  0.0956 Preprocessor1_Model01\n3 0.00133 rmse    standard   0.375    10  0.0358 Preprocessor1_Model02\n4 0.00133 rsq     standard   0.498    10  0.0956 Preprocessor1_Model02\n5 0.00176 rmse    standard   0.375    10  0.0358 Preprocessor1_Model03\n6 0.00176 rsq     standard   0.498    10  0.0956 Preprocessor1_Model03\n\n\n\n# tune_results |&gt;\n#   collect_metrics() |&gt;\n#   filter(.metric == \"rmse\") |&gt;\n#   filter(mean == min(mean))"
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html#predictions-refined",
    "href": "posts/20_supervised_learning/20_supervised_learning.html#predictions-refined",
    "title": "20: Supervised Learning",
    "section": "Predictions Refined",
    "text": "Predictions Refined\n\n# best_lambda &lt;- tune_results |&gt;\n#   collect_metrics() |&gt;\n#   filter(.metric == \"rmse\") |&gt;\n#   filter(mean == min(mean)) |&gt;\n#   distinct() |&gt;\n#   pull(penalty)\n\nbest_lambda &lt;- select_best(tune_results, metric = \"rmse\") |&gt;\n  pull(penalty)\n\npredict(ridge_fit,\n        new_data = office_df |&gt; select(-imdb_rating),\n        penalty = best_lambda) |&gt;\n  head()\n\n# A tibble: 6 × 1\n  .pred\n  &lt;dbl&gt;\n1  8.74\n2  8.67\n3  8.52\n4  8.37\n5  8.66\n6  8.65"
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html#coefficients",
    "href": "posts/20_supervised_learning/20_supervised_learning.html#coefficients",
    "title": "20: Supervised Learning",
    "section": "Coefficients",
    "text": "Coefficients\n\ntidy(lasso_fit, penalty = 1e-2)\n\n# A tibble: 11 × 3\n   term          estimate penalty\n   &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)   7.62        0.01\n 2 sent1_mean   -0.308       0.01\n 3 sent2_mean   -0.453       0.01\n 4 sent3_mean    0           0.01\n 5 sent1_dev    -0.650       0.01\n 6 sent2_dev     0.852       0.01\n 7 sent3_dev     0.449       0.01\n 8 season        0           0.01\n 9 ep_in_season  0.0135      0.01\n10 total_votes   0.000387    0.01\n11 episode_num  -0.00172     0.01\n\n\n\ntidy(lasso_fit, penalty = 1e-1)\n\n# A tibble: 11 × 3\n   term           estimate penalty\n   &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)   7.59          0.1\n 2 sent1_mean    0             0.1\n 3 sent2_mean    0             0.1\n 4 sent3_mean    0             0.1\n 5 sent1_dev     0             0.1\n 6 sent2_dev     0             0.1\n 7 sent3_dev     0             0.1\n 8 season        0             0.1\n 9 ep_in_season  0             0.1\n10 total_votes   0.000314      0.1\n11 episode_num  -0.0000509     0.1"
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html#lambda-grid-1",
    "href": "posts/20_supervised_learning/20_supervised_learning.html#lambda-grid-1",
    "title": "20: Supervised Learning",
    "section": "Lambda Grid",
    "text": "Lambda Grid\n\nPreprocessing\n\nlasso_recipe &lt;- recipe(formula = imdb_rating ~ ., \n                       data = office_train) |&gt;\n  step_novel(all_nominal_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_zv(all_predictors()) |&gt;\n  step_normalize()\n\n\n\nSpecification\n\nlasso_spec &lt;- linear_reg(mixture = 1, penalty = tune()) |&gt;\n  set_engine(\"glmnet\")\n\n\n\nWorkflow\n\nlasso_workflow &lt;- workflow() |&gt;\n  add_recipe(lasso_recipe) |&gt;\n  add_model(lasso_spec)\n\n\n\nTuning\n\nlambda_grid &lt;- grid_regular(penalty(range = c(-2, -1)),\n                            levels = 25)\n\ntune_results &lt;- tune_grid(\n  lasso_workflow,\n  resamples = office_fold,\n  grid = lambda_grid\n)\n\n\ntune_results |&gt; autoplot()"
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html#explanatory-variables",
    "href": "posts/20_supervised_learning/20_supervised_learning.html#explanatory-variables",
    "title": "20: Supervised Learning",
    "section": "Explanatory Variables",
    "text": "Explanatory Variables\n\ntune_results |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  filter(mean == min(mean))\n\n# A tibble: 1 × 7\n  penalty .metric .estimator  mean     n std_err .config              \n    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1  0.0215 rmse    standard   0.371    10  0.0392 Preprocessor1_Model09\n\n\n\nbest_lambda &lt;- select_best(tune_results, metric = \"rmse\") |&gt;\n  pull(penalty)\n\ntidy(lasso_fit, penalty = best_lambda)\n\n# A tibble: 11 × 3\n   term          estimate penalty\n   &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)   7.74      0.0215\n 2 sent1_mean   -0.162     0.0215\n 3 sent2_mean    0         0.0215\n 4 sent3_mean    0         0.0215\n 5 sent1_dev    -0.547     0.0215\n 6 sent2_dev     0.444     0.0215\n 7 sent3_dev     0.0297    0.0215\n 8 season        0         0.0215\n 9 ep_in_season  0.0117    0.0215\n10 total_votes   0.000376  0.0215\n11 episode_num  -0.00149   0.0215"
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html#preprocessing-2",
    "href": "posts/20_supervised_learning/20_supervised_learning.html#preprocessing-2",
    "title": "20: Supervised Learning",
    "section": "Preprocessing",
    "text": "Preprocessing\n\npca_recipe &lt;- \n  recipe(formula = imdb_rating ~ ., data = office_train) |&gt; \n  step_normalize(all_predictors()) |&gt;\n  step_pca(all_predictors(), threshold = tune())"
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html#specification-2",
    "href": "posts/20_supervised_learning/20_supervised_learning.html#specification-2",
    "title": "20: Supervised Learning",
    "section": "Specification",
    "text": "Specification\n\nlm_spec &lt;- \n  linear_reg() |&gt;\n  set_engine(\"lm\")"
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html#workflow-2",
    "href": "posts/20_supervised_learning/20_supervised_learning.html#workflow-2",
    "title": "20: Supervised Learning",
    "section": "Workflow",
    "text": "Workflow\n\npca_workflow &lt;- \n  workflow() |&gt;\n  add_recipe(pca_recipe) |&gt;\n  add_model(lm_spec)"
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html#tuning-2",
    "href": "posts/20_supervised_learning/20_supervised_learning.html#tuning-2",
    "title": "20: Supervised Learning",
    "section": "Tuning",
    "text": "Tuning\n\nlambda_grid &lt;- grid_regular(threshold(),\n                            levels = 20)\n\ntune_results &lt;- tune_grid(\n  pca_workflow,\n  resamples = office_fold,\n  grid = lambda_grid\n)\n\n\ntune_results |&gt; autoplot()\n\n\n\n\n\n\n\n\n\nbest_threshold &lt;- select_best(tune_results, metric = \"rmse\")\npca_refined &lt;- finalize_workflow(pca_workflow, best_threshold)\npca_fit &lt;- fit(pca_refined, data = office_train)"
  },
  {
    "objectID": "posts/20_supervised_learning/20_supervised_learning.html#visualization",
    "href": "posts/20_supervised_learning/20_supervised_learning.html#visualization",
    "title": "20: Supervised Learning",
    "section": "Visualization",
    "text": "Visualization\n\nflat_data &lt;- pca_recipe &lt;- \n  recipe(formula = imdb_rating ~ ., data = office_train) |&gt; \n  step_normalize(all_predictors()) |&gt;\n  step_pca(all_predictors()) |&gt; \n  prep() |&gt;\n  juice() |&gt;\n  select(imdb_rating, PC1, PC2)\n\n\nkmeans_fit &lt;- k_means(num_clusters = 5) |&gt;\n  set_engine(\"stats\") |&gt;\n  fit(imdb_rating ~ PC1 + PC2, data = flat_data)\n\n\nflat_data |&gt;\n  mutate(cluster_num = extract_cluster_assignment(kmeans_fit) |&gt;\n           pull()) |&gt;\n  ggplot(aes(x = PC1, y = PC2, color = cluster_num)) +\n  geom_point() +\n  labs(title = \"The Office\",\n       subtitle = \"inputs projected onto first two principal components\",\n       caption = \"Source: Schrute package\",\n       x = \"principal component 1\", y = \"principal component 2\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/21_unsupervised_learning/21_unsupervised_learning.html",
    "href": "posts/21_unsupervised_learning/21_unsupervised_learning.html",
    "title": "21: Unsupervised Learning",
    "section": "",
    "text": "Libraries and Loading the Data\n\n\n\n\n\n\nlibrary(\"corrplot\")\nlibrary(\"DiagrammeR\")#easy coding of flowcharts\nlibrary(\"GGally\")    #extensions for ggplot\nlibrary(\"gt\")\nlibrary(\"sf\")\nlibrary(\"tidyclust\")\nlibrary(\"tidymodels\")\nlibrary(\"tidyverse\") #general framework for data wrangling\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n# data set: College Scorecard\n# college_raw &lt;- readr::read_csv('Most-Recent-Cohorts-Institution.csv')\n# college_df &lt;- college_raw |&gt;\n#   select(INSTNM, UGDS,\n#          STABBR, LATITUDE, LONGITUDE,\n#          ADM_RATE,\n#          # SATMT75, SATWR75,\n#          C150_4, RET_FT4,\n#          MD_EARN_WNE_P10, ENDOWBEGIN) |&gt;\n#   drop_na() |&gt;\n#   filter(LONGITUDE &gt; -125 & LONGITUDE &lt; -67) |&gt;\n#   filter(LATITUDE &gt; 25 & LATITUDE &lt; 49)\n# readr::write_csv(college_df, \"college_data.csv\")\ncollege_df &lt;- readr::read_csv(\"college_data.csv\")\n# college_df &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/refs/heads/main/data/college_data.csv\")\nstates_shp &lt;- readr::read_rds(\"us_states_shp.rds\")\n\n\n\n\n\n\n\nGoal: Explore topics in unsupervised learning\n\n\n\n\n\nObjectives:\nclustering\nworkflows\n\n\n\n\n\n\nToday’s data comes from College Scorecard.\n\nINSTNM: institution name\nUGDS: undergraduate enrollment (number of students)\nLATITUDE\nLONGITUDE\nADM_RATE: admission rate\nC150_4: completion rate (at 150% of expected time for 4-year degrees)\nRET_FT4: retention rate (full-time students, 4-year institutions)\nMD_EARN_WNE_P10: median earnings, 10 years after graduation (USD)\nENDOWBEGIN: endowment (beginning of school year)\n\n\n\n\n\ncollege_df |&gt;\n  select_if(is.numeric) |&gt;\n  cor(use = \"pairwise.complete.obs\") |&gt;\n  corrplot(method = \"ellipse\",\n           order = \"FPC\",\n           type = \"upper\")"
  },
  {
    "objectID": "posts/21_unsupervised_learning/21_unsupervised_learning.html#start",
    "href": "posts/21_unsupervised_learning/21_unsupervised_learning.html#start",
    "title": "21: Unsupervised Learning",
    "section": "",
    "text": "Libraries and Loading the Data\n\n\n\n\n\n\nlibrary(\"corrplot\")\nlibrary(\"DiagrammeR\")#easy coding of flowcharts\nlibrary(\"GGally\")    #extensions for ggplot\nlibrary(\"gt\")\nlibrary(\"sf\")\nlibrary(\"tidyclust\")\nlibrary(\"tidymodels\")\nlibrary(\"tidyverse\") #general framework for data wrangling\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n# data set: College Scorecard\n# college_raw &lt;- readr::read_csv('Most-Recent-Cohorts-Institution.csv')\n# college_df &lt;- college_raw |&gt;\n#   select(INSTNM, UGDS,\n#          STABBR, LATITUDE, LONGITUDE,\n#          ADM_RATE,\n#          # SATMT75, SATWR75,\n#          C150_4, RET_FT4,\n#          MD_EARN_WNE_P10, ENDOWBEGIN) |&gt;\n#   drop_na() |&gt;\n#   filter(LONGITUDE &gt; -125 & LONGITUDE &lt; -67) |&gt;\n#   filter(LATITUDE &gt; 25 & LATITUDE &lt; 49)\n# readr::write_csv(college_df, \"college_data.csv\")\ncollege_df &lt;- readr::read_csv(\"college_data.csv\")\n# college_df &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/refs/heads/main/data/college_data.csv\")\nstates_shp &lt;- readr::read_rds(\"us_states_shp.rds\")\n\n\n\n\n\n\n\nGoal: Explore topics in unsupervised learning\n\n\n\n\n\nObjectives:\nclustering\nworkflows"
  },
  {
    "objectID": "posts/21_unsupervised_learning/21_unsupervised_learning.html#data-college-scorecard",
    "href": "posts/21_unsupervised_learning/21_unsupervised_learning.html#data-college-scorecard",
    "title": "21: Unsupervised Learning",
    "section": "",
    "text": "Today’s data comes from College Scorecard.\n\nINSTNM: institution name\nUGDS: undergraduate enrollment (number of students)\nLATITUDE\nLONGITUDE\nADM_RATE: admission rate\nC150_4: completion rate (at 150% of expected time for 4-year degrees)\nRET_FT4: retention rate (full-time students, 4-year institutions)\nMD_EARN_WNE_P10: median earnings, 10 years after graduation (USD)\nENDOWBEGIN: endowment (beginning of school year)"
  },
  {
    "objectID": "posts/21_unsupervised_learning/21_unsupervised_learning.html#correlation-plot",
    "href": "posts/21_unsupervised_learning/21_unsupervised_learning.html#correlation-plot",
    "title": "21: Unsupervised Learning",
    "section": "",
    "text": "college_df |&gt;\n  select_if(is.numeric) |&gt;\n  cor(use = \"pairwise.complete.obs\") |&gt;\n  corrplot(method = \"ellipse\",\n           order = \"FPC\",\n           type = \"upper\")"
  },
  {
    "objectID": "posts/21_unsupervised_learning/21_unsupervised_learning.html#scene",
    "href": "posts/21_unsupervised_learning/21_unsupervised_learning.html#scene",
    "title": "21: Unsupervised Learning",
    "section": "Scene",
    "text": "Scene\n\nstates_shp |&gt;\n  ggplot() +\n  geom_sf(color = \"gray80\", fill = \"white\") +\n  geom_point(aes(x = LONGITUDE, y = LATITUDE, \n                 color = STABBR),\n             data = college_df) +\n  labs(title = \"American Universities\",\n       subtitle = \"A selection of insitutions in the continental USA\",\n       caption = \"Source: College Scorecard\",\n       x = \"\", y = \"\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/21_unsupervised_learning/21_unsupervised_learning.html#a-first-foray",
    "href": "posts/21_unsupervised_learning/21_unsupervised_learning.html#a-first-foray",
    "title": "21: Unsupervised Learning",
    "section": "A first foray",
    "text": "A first foray\n\nset.seed(20241121)\nk4_fit &lt;- k_means(num_clusters = 4) |&gt;\n  set_engine(\"stats\") |&gt;\n  fit(~ LATITUDE + LONGITUDE, data = college_df)"
  },
  {
    "objectID": "posts/21_unsupervised_learning/21_unsupervised_learning.html#centroids",
    "href": "posts/21_unsupervised_learning/21_unsupervised_learning.html#centroids",
    "title": "21: Unsupervised Learning",
    "section": "Centroids",
    "text": "Centroids\n\nstates_shp |&gt;\n  ggplot() +\n  geom_sf(color = \"gray80\", fill = \"white\") +\n  geom_point(aes(x = LONGITUDE, y = LATITUDE),\n             color = \"gray60\",\n             data = college_df) +\n  geom_point(aes(x = LONGITUDE, y = LATITUDE, \n                 color = .cluster),\n             data = extract_centroids(k4_fit),\n             size = 7) +\n  labs(title = \"Centroids\",\n       subtitle = \"The clusters will be picked by distance to the nearest centroid\",\n       caption = \"Source: College Scorecard\",\n       x = \"\", y = \"\") +\n  scale_color_brewer(palette = \"Dark2\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/21_unsupervised_learning/21_unsupervised_learning.html#clusters",
    "href": "posts/21_unsupervised_learning/21_unsupervised_learning.html#clusters",
    "title": "21: Unsupervised Learning",
    "section": "Clusters",
    "text": "Clusters\n\ncluster_df &lt;- college_df |&gt;\n  mutate(cluster_num = extract_cluster_assignment(k4_fit) |&gt;\n           pull())\n\nstates_shp |&gt;\n  ggplot() +\n  geom_sf(color = \"gray80\", fill = \"white\") +\n  geom_point(aes(x = LONGITUDE, y = LATITUDE, \n                 color = cluster_num),\n             data = cluster_df) +\n  labs(title = \"Clusters\",\n       subtitle = \"k = 4\",\n       caption = \"Source: College Scorecard\",\n       x = \"\", y = \"\") +\n  scale_color_brewer(palette = \"Dark2\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsiderations\n\n\n\n\nHow do we place the centroids? Randomly?\nHow do we pick the number of clusters?"
  },
  {
    "objectID": "posts/21_unsupervised_learning/21_unsupervised_learning.html#try-several-arrangements",
    "href": "posts/21_unsupervised_learning/21_unsupervised_learning.html#try-several-arrangements",
    "title": "21: Unsupervised Learning",
    "section": "Try several arrangements",
    "text": "Try several arrangements\n\nN &lt;- 10 #max possible number of clusters\nk_vals &lt;- 1:N\nerr_ratios &lt;- rep(NA, N)\n\nfor(i in 1:N){\n  kmeans_fit &lt;- k_means(num_clusters = k_vals[i]) |&gt;\n    set_engine(\"stats\") |&gt;\n    fit(~ LATITUDE + LONGITUDE, data = college_df)\n  err_ratios[i] &lt;- kmeans_fit$fit$tot.withinss / kmeans_fit$fit$totss\n}"
  },
  {
    "objectID": "posts/21_unsupervised_learning/21_unsupervised_learning.html#scree-plot",
    "href": "posts/21_unsupervised_learning/21_unsupervised_learning.html#scree-plot",
    "title": "21: Unsupervised Learning",
    "section": "Scree Plot",
    "text": "Scree Plot\n\ndf_for_graph &lt;- data.frame(num_clusters = 1:N,\n                           err_ratios)\n\ndf_for_graph |&gt;\n  ggplot() +\n  geom_point(aes(x = factor(num_clusters), y = err_ratios),\n             size = 3) +\n  geom_line(aes(x = num_clusters, y = err_ratios)) +\n  labs(title = \"Scree Plot\",\n       subtitle = \"Aiming to choose the number of clusters\",\n       caption = \"SML 201\",\n       x = \"number of clusters\",\n       y = \"within/total SSE ratio\") +\n  theme_minimal(base_size = 14)"
  },
  {
    "objectID": "posts/21_unsupervised_learning/21_unsupervised_learning.html#tidymodels",
    "href": "posts/21_unsupervised_learning/21_unsupervised_learning.html#tidymodels",
    "title": "21: Unsupervised Learning",
    "section": "Tidymodels",
    "text": "Tidymodels\n\n# cross-validation folds\ncollege_cv &lt;- vfold_cv(college_df, v = 10)\n\n# specification\ncollege_spec &lt;- k_means(num_clusters = tune())\n\n# recipe\ncollege_recipe &lt;- recipe(~., data = college_df |&gt;\n                         select_if(is.numeric)) |&gt;\n  step_normalize()\n\n# workflow\ncollege_workflow &lt;- workflow(college_recipe, college_spec)\n\n# parameter grid\nkvals_grid &lt;- grid_regular(num_clusters(), levels = 10)\n\n# tuning\ntune_results &lt;- tune_cluster(\n  college_workflow,\n  resamples = college_cv,\n  grid = kvals_grid,\n  metrics = cluster_metric_set(sse_within_total, sse_total, sse_ratio)\n)"
  },
  {
    "objectID": "posts/21_unsupervised_learning/21_unsupervised_learning.html#scree-plot-1",
    "href": "posts/21_unsupervised_learning/21_unsupervised_learning.html#scree-plot-1",
    "title": "21: Unsupervised Learning",
    "section": "Scree Plot",
    "text": "Scree Plot\n\ntune_results |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"sse_ratio\") |&gt;\n  ggplot(aes(x = num_clusters, y = mean)) +\n  geom_point(aes(x = factor(num_clusters), y = mean),\n             size = 3) +\n  geom_line() +\n  labs(title = \"Scree Plot\",\n       subtitle = \"Aiming to choose the number of clusters\",\n       caption = \"SML 201\",\n       x = \"number of clusters\",\n       y = \"within/total SSE ratio\") +\n  theme_minimal(base_size = 14)"
  },
  {
    "objectID": "posts/22_parallelization/22_parallelization.html",
    "href": "posts/22_parallelization/22_parallelization.html",
    "title": "22: Parallelization",
    "section": "",
    "text": "Libraries and Loading the Data\n\n\n\n\n\n\nlibrary(\"janitor\")\nlibrary(\"future\")\nlibrary(\"parallel\")\nlibrary(\"tidymodels\")\nlibrary(\"tidyverse\") #general framework for data wrangling\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\nloan_raw &lt;- readr::read_csv(\"loan_data_set.csv\") |&gt;\n  janitor::clean_names()\n\n\n\n\n\n\n\nGoal: “Take out the Cheese” Day\n\n\n\n\n\nObjectives:\nShapiro-Wilk test\nLogistic regression revisited\nParallelization\n\n\n\n\n\n\n“Dream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.”\n\nSource: Kaggle\n\n\n\n\nGender (of primary applicant)\nMarital status (of primary applicant)\nDependents\nEducation\nSelf-employed\nApplicant income (monthly, in dollars)\nCo-applicant income (monthly, in dollars)\nloan amount terms (in months)\nCredit history\nProperty area\n\n\n\n\n\nremove rows that have missing values in the response variable (loan_amount)\nconvert dependents to a numerical variable\n\nhere, replace “+” with nothing\n\ncombine “income” columns\n\nensure all dollar amounts are in the same units (thousands of dollars)\n\nconvert credit_history to a factor variable (i.e. categorical)\nretain relevant columns\n\n\nloan_df &lt;- loan_raw |&gt;\n  filter(!is.na(loan_amount)) |&gt;\n  mutate(dependents_num = as.numeric(\n    str_replace(dependents, \"\\\\+\", \"\")\n  )) |&gt;\n  mutate(income = applicant_income/1000 + coapplicant_income/1000) |&gt;\n  mutate(credit_history = factor(credit_history)) |&gt;\n  select(loan_amount, income, dependents_num, gender, married, education, self_employed, credit_history, property_area, loan_status) |&gt;\n  mutate(approved = ifelse(loan_status == \"Y\", 1, 0),\n         approved_fac = factor(approved,\n                               levels = c(0,1))) |&gt;\n  select(loan_amount, income, dependents_num, gender, married, education, self_employed, credit_history, property_area, approved_fac)"
  },
  {
    "objectID": "posts/22_parallelization/22_parallelization.html#start",
    "href": "posts/22_parallelization/22_parallelization.html#start",
    "title": "22: Parallelization",
    "section": "",
    "text": "Libraries and Loading the Data\n\n\n\n\n\n\nlibrary(\"janitor\")\nlibrary(\"future\")\nlibrary(\"parallel\")\nlibrary(\"tidymodels\")\nlibrary(\"tidyverse\") #general framework for data wrangling\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\nloan_raw &lt;- readr::read_csv(\"loan_data_set.csv\") |&gt;\n  janitor::clean_names()\n\n\n\n\n\n\n\nGoal: “Take out the Cheese” Day\n\n\n\n\n\nObjectives:\nShapiro-Wilk test\nLogistic regression revisited\nParallelization"
  },
  {
    "objectID": "posts/22_parallelization/22_parallelization.html#data-home-loans",
    "href": "posts/22_parallelization/22_parallelization.html#data-home-loans",
    "title": "22: Parallelization",
    "section": "",
    "text": "“Dream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.”\n\nSource: Kaggle\n\n\n\n\nGender (of primary applicant)\nMarital status (of primary applicant)\nDependents\nEducation\nSelf-employed\nApplicant income (monthly, in dollars)\nCo-applicant income (monthly, in dollars)\nloan amount terms (in months)\nCredit history\nProperty area\n\n\n\n\n\nremove rows that have missing values in the response variable (loan_amount)\nconvert dependents to a numerical variable\n\nhere, replace “+” with nothing\n\ncombine “income” columns\n\nensure all dollar amounts are in the same units (thousands of dollars)\n\nconvert credit_history to a factor variable (i.e. categorical)\nretain relevant columns\n\n\nloan_df &lt;- loan_raw |&gt;\n  filter(!is.na(loan_amount)) |&gt;\n  mutate(dependents_num = as.numeric(\n    str_replace(dependents, \"\\\\+\", \"\")\n  )) |&gt;\n  mutate(income = applicant_income/1000 + coapplicant_income/1000) |&gt;\n  mutate(credit_history = factor(credit_history)) |&gt;\n  select(loan_amount, income, dependents_num, gender, married, education, self_employed, credit_history, property_area, loan_status) |&gt;\n  mutate(approved = ifelse(loan_status == \"Y\", 1, 0),\n         approved_fac = factor(approved,\n                               levels = c(0,1))) |&gt;\n  select(loan_amount, income, dependents_num, gender, married, education, self_employed, credit_history, property_area, approved_fac)"
  },
  {
    "objectID": "posts/22_parallelization/22_parallelization.html#qq-plots",
    "href": "posts/22_parallelization/22_parallelization.html#qq-plots",
    "title": "22: Parallelization",
    "section": "QQ Plots",
    "text": "QQ Plots\nOne way is to construct quantile-quantile plots. Let us proceed with some more [ggplot] tech.\n\nloan_df |&gt;\n  ggplot(aes(sample = loan_amount)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(title = \"QQ Plot\",\n       subtitle = \"Loan Amount looks to be normally distributed\\nexcept for higher level amounts\",\n       caption = \"Source: Dream House Finance\",\n       x = \"quantile 1\", y = \"quantile 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nloan_df |&gt;\n  ggplot(aes(sample = log(loan_amount))) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(title = \"QQ Plot\",\n       subtitle = \"Log loan amount looks to be normally distributed\\nexcept for the tails\",\n       caption = \"Source: Dream House Finance\",\n       x = \"quantile 1\", y = \"quantile 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis could also be shown in groups.\n\nloan_df |&gt;\n  ggplot(aes(sample = log(loan_amount),\n             color = factor(approved_fac))) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(title = \"QQ Plot\",\n       subtitle = \"Log loan amount looks to be normally distributed\\nexcept for the tails\",\n       caption = \"Source: Dream House Finance\",\n       x = \"quantile 1\", y = \"quantile 2\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/22_parallelization/22_parallelization.html#shapiro-wilk-test",
    "href": "posts/22_parallelization/22_parallelization.html#shapiro-wilk-test",
    "title": "22: Parallelization",
    "section": "Shapiro-Wilk Test",
    "text": "Shapiro-Wilk Test\n\nnull hypothesis: data is normally distributed\nalternative hypothesis: data is not normally distributed\n\n\nstats::shapiro.test(loan_df$loan_amount)\n\n\n    Shapiro-Wilk normality test\n\ndata:  loan_df$loan_amount\nW = 0.76752, p-value &lt; 2.2e-16\n\n\n\nstats::shapiro.test(log(loan_df$loan_amount))\n\n\n    Shapiro-Wilk normality test\n\ndata:  log(loan_df$loan_amount)\nW = 0.9633, p-value = 5.357e-11"
  },
  {
    "objectID": "posts/22_parallelization/22_parallelization.html#parallelization",
    "href": "posts/22_parallelization/22_parallelization.html#parallelization",
    "title": "22: Parallelization",
    "section": "Parallelization",
    "text": "Parallelization\n\nparallel::detectCores()\n\n[1] 16\n\n\n\nstart_time &lt;- Sys.time()\n\nfuture::plan(multisession, workers = 5)\n\ntune_results &lt;- tune_grid(\n  loan_workflow,\n  resamples = loan_cv,\n  grid = lambda_grid\n)\n\nparallelization_time &lt;- Sys.time() - start_time\n\n\nprint(\"The brute force time was\")\n\n[1] \"The brute force time was\"\n\nprint(brute_force_time)\n\nTime difference of 2.280213 secs\n\n\n\nprint(\"The parallelized processing time was\")\n\n[1] \"The parallelized processing time was\"\n\nprint(parallelization_time)\n\nTime difference of 10.5041 secs"
  },
  {
    "objectID": "posts/23_case_study/23_case_study.html",
    "href": "posts/23_case_study/23_case_study.html",
    "title": "23: One More Case Study",
    "section": "",
    "text": "Libraries and Loading the Data\n\n\n\n\n\n\nlibrary(\"ggsignif\")  #show significance levels on boxplots\nlibrary(\"patchwork\") #arrange plots\nlibrary(\"tidyclust\") #tidymodels clustering\nlibrary(\"tidymodels\")#framework for machine learning in R\nlibrary(\"tidyverse\") #general framework for data wrangling\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\nset.seed(201)\ncontrol_data &lt;- data.frame(\n  age = sample(18:65, size = 100, replace = TRUE),\n  height = round(rnorm(100, 65, 8)),\n  sex = sample(c(\"female\", \"male\"), size = 100, replace = TRUE),\n  group = \"control\",\n  weight_before = round(rnorm(100, 150, 15))\n) |&gt;\n  #coded like Gaussian white noise\n  mutate(weight_after = round(weight_before - rnorm(100, 0, 6)))\n\ntreatment_data &lt;- data.frame(\n  age = sample(18:65, size = 100, replace = TRUE),\n  height = round(rnorm(100, 65, 8)),\n  sex = sample(c(\"female\", \"male\"), size = 100, replace = TRUE),\n  group = \"treatment\",\n  weight_before = round(rnorm(100, 150, 15))\n) |&gt;\n  #coded like Gaussian white noise\n  mutate(weight_after = round(weight_before - rnorm(100, 15, 6)))\n\nGLP_raw &lt;- rbind(control_data, treatment_data) |&gt;\n  sample_frac() |&gt;\n  mutate(patient_id = 1:200, .before = \"age\")\n\n# helper function\nvnorm &lt;- function(x, mu = 0, sigma = 1, section = \"lower\"){\n  \n  # bell curve\n  x_vals &lt;- seq(mu - 4*sigma, mu + 4*sigma, length.out = 201)\n  y_vals &lt;- dnorm(x_vals, mu, sigma)\n  df_for_graph &lt;- data.frame(x_vals, y_vals)\n\n  # outline shaded regions\n  if(length(x) == 1){\n    shade_left &lt;- rbind(c(x[1],0), df_for_graph |&gt;\n                        filter(x_vals &lt; x[1]))\n    shade_right &lt;- rbind(c(x[1],0), df_for_graph |&gt;\n                        filter(x_vals &gt; x[1]))\n  }\n  if(length(x) == 2){\n    shade_between &lt;- rbind(c(x[1],0),\n                       df_for_graph |&gt;\n                         filter(x_vals &gt; x[1] &\n                                  x_vals &lt; x[2]),\n                       c(x[2],0))\n    shade_tails &lt;- rbind(df_for_graph |&gt;\n                        filter(x_vals &lt; x[1]),\n                     c(x[1],0),\n                     c(x[2],0),\n                     df_for_graph |&gt;\n                        filter(x_vals &gt; x[2]))\n  }\n  if(section == \"lower\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_left,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(pnorm(x,mu,sigma), 4)\n  }\n  if(section == \"upper\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_right,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- 1 - round(pnorm(x,mu,sigma), 4)\n  }\n  if(section == \"between\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_between,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(diff(pnorm(x,mu,sigma)), 4)\n  }\n  if(section == \"tails\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_tails,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(1 - diff(pnorm(x,mu,sigma)), 4)\n  }\n  \n  # plot bell curve\n  bell_curve + \n    labs(subtitle = paste0(\"Probability: \", prob_val),\n         caption = \"SML 201\", y = \"\") +\n    theme_minimal()\n}\n\n\n\n\n\n\n\nGoal: Take on one more case study\n\n\n\n\n\nObjectives:\nprobability\nconfidence intervals\nhypothesis testing\nmachine learning\n\n\n\n\n\n\n\n“GLP [Glucagon-like peptide-1] agonists are medications that help lower blood sugar levels and promote weight loss.” — Cleveland Clinic\n\nWe are going to take hypothetical data and pretend that we are analyzing results from clinical trials in hopes of obtaining approval from the Food and Drug Administration."
  },
  {
    "objectID": "posts/23_case_study/23_case_study.html#start",
    "href": "posts/23_case_study/23_case_study.html#start",
    "title": "23: One More Case Study",
    "section": "",
    "text": "Libraries and Loading the Data\n\n\n\n\n\n\nlibrary(\"ggsignif\")  #show significance levels on boxplots\nlibrary(\"patchwork\") #arrange plots\nlibrary(\"tidyclust\") #tidymodels clustering\nlibrary(\"tidymodels\")#framework for machine learning in R\nlibrary(\"tidyverse\") #general framework for data wrangling\n\n# school colors\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\nset.seed(201)\ncontrol_data &lt;- data.frame(\n  age = sample(18:65, size = 100, replace = TRUE),\n  height = round(rnorm(100, 65, 8)),\n  sex = sample(c(\"female\", \"male\"), size = 100, replace = TRUE),\n  group = \"control\",\n  weight_before = round(rnorm(100, 150, 15))\n) |&gt;\n  #coded like Gaussian white noise\n  mutate(weight_after = round(weight_before - rnorm(100, 0, 6)))\n\ntreatment_data &lt;- data.frame(\n  age = sample(18:65, size = 100, replace = TRUE),\n  height = round(rnorm(100, 65, 8)),\n  sex = sample(c(\"female\", \"male\"), size = 100, replace = TRUE),\n  group = \"treatment\",\n  weight_before = round(rnorm(100, 150, 15))\n) |&gt;\n  #coded like Gaussian white noise\n  mutate(weight_after = round(weight_before - rnorm(100, 15, 6)))\n\nGLP_raw &lt;- rbind(control_data, treatment_data) |&gt;\n  sample_frac() |&gt;\n  mutate(patient_id = 1:200, .before = \"age\")\n\n# helper function\nvnorm &lt;- function(x, mu = 0, sigma = 1, section = \"lower\"){\n  \n  # bell curve\n  x_vals &lt;- seq(mu - 4*sigma, mu + 4*sigma, length.out = 201)\n  y_vals &lt;- dnorm(x_vals, mu, sigma)\n  df_for_graph &lt;- data.frame(x_vals, y_vals)\n\n  # outline shaded regions\n  if(length(x) == 1){\n    shade_left &lt;- rbind(c(x[1],0), df_for_graph |&gt;\n                        filter(x_vals &lt; x[1]))\n    shade_right &lt;- rbind(c(x[1],0), df_for_graph |&gt;\n                        filter(x_vals &gt; x[1]))\n  }\n  if(length(x) == 2){\n    shade_between &lt;- rbind(c(x[1],0),\n                       df_for_graph |&gt;\n                         filter(x_vals &gt; x[1] &\n                                  x_vals &lt; x[2]),\n                       c(x[2],0))\n    shade_tails &lt;- rbind(df_for_graph |&gt;\n                        filter(x_vals &lt; x[1]),\n                     c(x[1],0),\n                     c(x[2],0),\n                     df_for_graph |&gt;\n                        filter(x_vals &gt; x[2]))\n  }\n  if(section == \"lower\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_left,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(pnorm(x,mu,sigma), 4)\n  }\n  if(section == \"upper\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_right,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- 1 - round(pnorm(x,mu,sigma), 4)\n  }\n  if(section == \"between\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_between,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(diff(pnorm(x,mu,sigma)), 4)\n  }\n  if(section == \"tails\"){\n    bell_curve &lt;- df_for_graph |&gt;\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_tails,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val &lt;- round(1 - diff(pnorm(x,mu,sigma)), 4)\n  }\n  \n  # plot bell curve\n  bell_curve + \n    labs(subtitle = paste0(\"Probability: \", prob_val),\n         caption = \"SML 201\", y = \"\") +\n    theme_minimal()\n}\n\n\n\n\n\n\n\nGoal: Take on one more case study\n\n\n\n\n\nObjectives:\nprobability\nconfidence intervals\nhypothesis testing\nmachine learning"
  },
  {
    "objectID": "posts/23_case_study/23_case_study.html#data",
    "href": "posts/23_case_study/23_case_study.html#data",
    "title": "23: One More Case Study",
    "section": "",
    "text": "“GLP [Glucagon-like peptide-1] agonists are medications that help lower blood sugar levels and promote weight loss.” — Cleveland Clinic\n\nWe are going to take hypothetical data and pretend that we are analyzing results from clinical trials in hopes of obtaining approval from the Food and Drug Administration."
  },
  {
    "objectID": "posts/23_case_study/23_case_study.html#null-distribution",
    "href": "posts/23_case_study/23_case_study.html#null-distribution",
    "title": "23: One More Case Study",
    "section": "Null Distribution",
    "text": "Null Distribution\nUse infer package code to perform NHST to test the claim (among those in the treatment group)\n\nnull hypothesis: male patients have at least as much weight loss than female patients\nalternative hypothesis: male patients have less weight loss than female patients\n\n\n# build a null distribution\nnull_distribution &lt;- GLP_df |&gt;\n  filter(group == \"treatment\") |&gt;\n  specify(formula = weight_loss ~ sex) |&gt; \n  hypothesize(null = \"independence\") |&gt; \n  generate(reps = 1000, type = \"permute\") |&gt; \n  calculate(stat = \"diff in means\", order = c(\"male\", \"female\"))\n\n\n# compute the observed difference in means\nobs_diff_means &lt;- GLP_df |&gt;\n  filter(group == \"treatment\") |&gt;\n  specify(formula = weight_loss ~ sex) |&gt; \n  calculate(stat = \"diff in means\", order = c(\"male\", \"female\"))\n  \n# visualize the null distribution and shade in the p-value\nnull_distribution |&gt;\nvisualize(bins = 10) + \n  shade_p_value(obs_stat = obs_diff_means, direction = \"less\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nnull_distribution |&gt;\n  get_p_value(obs_stat = obs_diff_means, direction = \"less\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.118"
  },
  {
    "objectID": "posts/23_case_study/23_case_study.html#boxplots",
    "href": "posts/23_case_study/23_case_study.html#boxplots",
    "title": "23: One More Case Study",
    "section": "Boxplots",
    "text": "Boxplots\nCreate a side-by-side boxplot to conduct a two-sided NHST for weight loss between the control and treatment groups.\n\nGLP_df |&gt;\n  ggplot(aes(x = group, y = weight_loss,\n             fill = group)) +\n  geom_boxplot() +\n  geom_signif(\n    comparisons = list(c(\"control\", \"treatment\")),\n    map_signif_level = TRUE\n  ) +\n  labs(title = \"GLP Clinical Trials\",\n       subtitle = \"(toward publisable results)\",\n       caption = \"(hypothetical data)\",\n       x = \"\", y = \"weight loss (pounds)\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/23_case_study/23_case_study.html#ridge-regression",
    "href": "posts/23_case_study/23_case_study.html#ridge-regression",
    "title": "23: One More Case Study",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nml_pipeline_fit &lt;- linear_reg(mixture = 0, penalty = 0) |&gt; \n  set_engine(\"glmnet\") |&gt;\n  fit(weight_loss ~ ., data = GLP_ml)\nml_pipeline_recipe &lt;- recipe(weight_loss ~ ., data = GLP_ml) |&gt;\n  step_novel(all_nominal_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt;\n  step_normalize(all_numeric_predictors())\nml_pipeline_spec &lt;- linear_reg(mixture = 0, penalty = tune()) |&gt;\n  set_engine(\"glmnet\")\nml_pipeline_workflow &lt;- workflow() |&gt; \n  add_recipe(ml_pipeline_recipe) |&gt;\n  add_model(ml_pipeline_spec)\n\nml_pipeline_grid &lt;- tune_grid(\n  ml_pipeline_workflow,\n  resamples = data_fold, \n  grid = lambda_grid\n)\n\n\nbest_penalty &lt;- select_best(ml_pipeline_grid, \n                            metric = \"rmse\") |&gt;\n  pull(penalty)\npredictions &lt;- predict(ml_pipeline_fit, \n                       new_data = GLP_df |&gt;\n                         select(-weight_loss),\n                       penalty = best_penalty)\ntrue_values &lt;- GLP_df  |&gt; select(weight_loss)\nn &lt;- GLP_df  |&gt; drop_na(weight_loss) |&gt; nrow()\nRMSE &lt;- sqrt((1/n)*sum((true_values - predictions)^2))\nprint(RMSE)\n\n[1] 6.090254"
  },
  {
    "objectID": "posts/23_case_study/23_case_study.html#lasso-regression",
    "href": "posts/23_case_study/23_case_study.html#lasso-regression",
    "title": "23: One More Case Study",
    "section": "Lasso Regression",
    "text": "Lasso Regression\n\nml_pipeline_fit &lt;- linear_reg(mixture = 1, penalty = 0) |&gt; \n  set_engine(\"glmnet\") |&gt;\n  fit(weight_loss ~ ., data = GLP_ml)\nml_pipeline_recipe &lt;- recipe(weight_loss ~ ., data = GLP_ml) |&gt;\n  step_novel(all_nominal_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt;\n  step_normalize(all_numeric_predictors())\nml_pipeline_spec &lt;- linear_reg(mixture = 1, penalty = tune()) |&gt;\n  set_engine(\"glmnet\")\nml_pipeline_workflow &lt;- workflow() |&gt; \n  add_recipe(ml_pipeline_recipe) |&gt;\n  add_model(ml_pipeline_spec)\n\nml_pipeline_grid &lt;- tune_grid(\n  ml_pipeline_workflow,\n  resamples = data_fold, \n  grid = lambda_grid\n)\n\n\nbest_penalty &lt;- select_best(ml_pipeline_grid, \n                            metric = \"rmse\") |&gt;\n  pull(penalty)\npredictions &lt;- predict(ml_pipeline_fit, \n                       new_data = GLP_df |&gt;\n                         select(-weight_loss),\n                       penalty = best_penalty)\ntrue_values &lt;- GLP_df  |&gt; select(weight_loss)\nn &lt;- GLP_df  |&gt; drop_na(weight_loss) |&gt; nrow()\nRMSE &lt;- sqrt((1/n)*sum((true_values - predictions)^2))\nprint(RMSE)\n\n[1] 6.063561"
  },
  {
    "objectID": "posts/23_case_study/23_case_study.html#clustering",
    "href": "posts/23_case_study/23_case_study.html#clustering",
    "title": "23: One More Case Study",
    "section": "Clustering",
    "text": "Clustering\n\n# cross-validation folds\nml_pipeline_cv &lt;- vfold_cv(GLP_ml, v = 5)\n\n# specification\nml_pipeline_spec &lt;- k_means(num_clusters = tune())\n\n# recipe\nml_pipeline_recipe &lt;- recipe(~weight_loss, data = GLP_ml) |&gt;\n  step_zv() |&gt;\n  step_normalize()\n\n# workflow\nml_pipeline_workflow &lt;- workflow(ml_pipeline_recipe, ml_pipeline_spec)\n\n# parameter grid\nkvals_grid &lt;- grid_regular(num_clusters(), levels = 10)\n\n# tuning\ntune_results &lt;- tune_cluster(\n  ml_pipeline_workflow,\n  resamples = ml_pipeline_cv,\n  grid = kvals_grid,\n  metrics = cluster_metric_set(sse_within_total, sse_total, sse_ratio)\n)\n\n\ntune_results |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"sse_ratio\") |&gt;\n  ggplot(aes(x = num_clusters, y = mean)) +\n  geom_point(aes(x = factor(num_clusters), y = mean),\n             size = 3) +\n  geom_line() +\n  labs(title = \"Scree Plot\",\n       subtitle = \"Aiming to choose the number of clusters\",\n       caption = \"SML 201\",\n       x = \"number of clusters\",\n       y = \"within/total SSE ratio\") +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\nVisualization\nCreate a side-by-side arrangements of scatterplots\n\nhorizontal: weight_before\nvertical: weight_after\n\n\ncolor coded by group\ncolor coded by cluster assignment\n\n\ncluster_fit &lt;- k_means(num_clusters = 2) |&gt;\n  set_engine(\"stats\") |&gt;\n  fit(~ weight_loss, data = GLP_df)\nGLP_clust &lt;- GLP_df |&gt;\n  mutate(cluster_num = extract_cluster_assignment(cluster_fit) |&gt;\n           pull())\n\n\np1 &lt;- GLP_clust |&gt;\n  ggplot(aes(x = weight_before, y = weight_after)) +\n  geom_point(aes(color = group),\n             alpha = 0.5, size = 3) +\n  labs(title = \"GLP Clinical Trials\",\n       subtitle = \"experiment groups\",\n       x = \"weight before (pounds)\",\n       y = \"weight after (pounds)\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\np2 &lt;- GLP_clust |&gt;\n  ggplot(aes(x = weight_before, y = weight_after)) +\n  geom_point(aes(color = cluster_num),\n             alpha = 0.5, size = 3) +\n  labs(title = \"GLP Clinical Trials\",\n       subtitle = \"k = 2 clusters\",\n       x = \"weight before (pounds)\",\n       y = \"weight after (pounds)\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n#patchwork\np1 + p2"
  },
  {
    "objectID": "posts/teacher_notes/teacher_notes.html",
    "href": "posts/teacher_notes/teacher_notes.html",
    "title": "Thoughts After the Semester",
    "section": "",
    "text": "This past semester, I had the honor of teaching SML 201, Introduction to Data Science, at Princeton University, and here I felt self-motivated to type up a lot of notes and thoughts I had over the semester to continue to build a great course. In particular, I am proud of\n\ndeploying a variety of real-world data sets (most of them less than 10 years old)\nscaffolding tougher statistics concepts\ncreating 4 intense take-home group projects\npushing the students higher on Bloom’s Taxonomy\n\n\n\n\n\n\n\nOn Comparisons\n\n\n\nIn this blog post, whenever I use words like “more” or “higher”, I am simply comparing my recent performance to iterations of my classes that I delivered at a different university in years past. I mean no implications of my current colleagues at my current university as I haven’t combed through all of their materials.\n\n\nNumerous post-semester thoughts will be organized into several sections, which will be listed alphabetically (as seen in the table of contents)"
  },
  {
    "objectID": "posts/teacher_notes/teacher_notes.html#attendance",
    "href": "posts/teacher_notes/teacher_notes.html#attendance",
    "title": "teacher notes",
    "section": "",
    "text": "started with 120 students\nwanted to increase to 144 students (i.e. the seating capacity of the classroom times the two lecture sessions)\nended up with 134 students\n\nToward the end of the semester, I observed that about 60 percent of the students attended classes regularly. Some people might think that recording the lecture sessions correlates with the lower attendance, but upon looking at the Panopto Video statistics, only about 10 students were using the recordings. More pertinent is probably these Quarto blog posts (I call them lecture slides” at work) which include nearly all of the R code that we use."
  },
  {
    "objectID": "posts/teacher_notes/teacher_notes.html#clo-1",
    "href": "posts/teacher_notes/teacher_notes.html#clo-1",
    "title": "Thoughts After the Semester",
    "section": "CLO 1",
    "text": "CLO 1\n\nImplement data science and statistical concepts and methods to data sets and real-world scenarios"
  },
  {
    "objectID": "posts/teacher_notes/teacher_notes.html#clo-2",
    "href": "posts/teacher_notes/teacher_notes.html#clo-2",
    "title": "Thoughts After the Semester",
    "section": "CLO 2",
    "text": "CLO 2\n\nUtilize software to load files, perform data wrangling, and produce visualizations"
  },
  {
    "objectID": "posts/teacher_notes/teacher_notes.html#clo-3",
    "href": "posts/teacher_notes/teacher_notes.html#clo-3",
    "title": "Thoughts After the Semester",
    "section": "CLO 3",
    "text": "CLO 3\n\nCompute results from data analyses and express the findings to academic and scientific communication audiences"
  },
  {
    "objectID": "posts/teacher_notes/teacher_notes.html#clo-4",
    "href": "posts/teacher_notes/teacher_notes.html#clo-4",
    "title": "Thoughts After the Semester",
    "section": "CLO 4",
    "text": "CLO 4\n\nDiscuss ethical and societal impact of data science projects and equitable practices\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn Data Ethics\n\n\n\n\n\nI had only one assignment that emphasized data ethics, and we talked about data ethics for a small amount of time during the semester. If students are reporting high familiarity with data ethics on these Likert scales, their knowledge probably came from other courses."
  },
  {
    "objectID": "posts/teacher_notes/teacher_notes.html#project-1",
    "href": "posts/teacher_notes/teacher_notes.html#project-1",
    "title": "Thoughts After the Semester",
    "section": "Project 1",
    "text": "Project 1\n\ntheme: Electoral College\nconcept: exploring data\nareas: politics, management\nmain hurdle: merging data\n\nInitially, my plan was for students to redo the 2020 USA Presidential election, but for the Green and Libertarian parties. Unfortunately, it was trivial because between those two parties, the Libertarian candidate had earned more votes in nearly every county. Furthermore, the Green party did not have a candidate in some counties, or sometimes multiple candidates.\nInstead, I had students explore statistics concepts like error, logarithmic transformation, and z-score standardization. Some student groups did a great job in understanding why these data analyses could be useful to a political campaign."
  },
  {
    "objectID": "posts/teacher_notes/teacher_notes.html#project-2",
    "href": "posts/teacher_notes/teacher_notes.html#project-2",
    "title": "Thoughts After the Semester",
    "section": "Project 2",
    "text": "Project 2\n\ntheme: Covid-19 Wastewater\nconcepts: correlation and linear regression\nareas: biology, epidemiology, public health\nmain hurdle: pivoting data\n\nInitially, my plan was for students to seek lagged correlation between spikes in wastewater findings of viral levels and hospitalization rates. Unfortunately, very few states (of the United States) kept records of hospitalizations or mortality that could be connected back to the coronaviruses.\nInstead, I had the students replicate the data visualizations that were found on the National Wastewater Surveillance System."
  },
  {
    "objectID": "posts/teacher_notes/teacher_notes.html#project-3",
    "href": "posts/teacher_notes/teacher_notes.html#project-3",
    "title": "Thoughts After the Semester",
    "section": "Project 3",
    "text": "Project 3\n\ntheme: Cybersecurity\nconcepts: confidence intervals and hypothesis testing\nareas: mechanical engineering, physics, computer science\nmain hurdle: connecting the data with the associated research paper\n\nI found a wonderful data set called MisbehaviorX that was created by a somewhat local group of scientists. That data set was created to allow future scientists to judge the quality of their machine learning algorithms for autonomous cars. The data simulated various possible cyber attacks. I had the students at least use our statistics techniques to measure the signals of the attacks."
  },
  {
    "objectID": "posts/teacher_notes/teacher_notes.html#project-4",
    "href": "posts/teacher_notes/teacher_notes.html#project-4",
    "title": "Thoughts After the Semester",
    "section": "Project 4",
    "text": "Project 4\n\ntheme: Ridesharing\nconcepts: machine learning\nareas: business, economics, finance\nmain hurdle: variable selection\n\nAt some data science conferences, I have seen economists talk about exploring Uber data sets (i.e. the ridesharing company). I was able to find a similarly extensive data set through Chicago Open Portal. I had the students explore the data in a way that reviewed all of the major concepts from the semester. Unfortunately, the anonymized data was rather homongenous (for example, there was little separation between morning and evening commutes), so we didn’t get impressive results at the time."
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html",
    "href": "posts/301_01_introduction/01_introduction.html",
    "title": "1: Introductions",
    "section": "",
    "text": "Goal: Introduce course\nObjective: Explore some Python codes\n\n\n\n\n\ntextbooks"
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#start",
    "href": "posts/301_01_introduction/01_introduction.html#start",
    "title": "1: Introductions",
    "section": "",
    "text": "Goal: Introduce course\nObjective: Explore some Python codes\n\n\n\n\n\ntextbooks"
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#data-intelligence-modern-data-science-methods",
    "href": "posts/301_01_introduction/01_introduction.html#data-intelligence-modern-data-science-methods",
    "title": "1: Introductions",
    "section": "Data Intelligence: Modern Data Science Methods",
    "text": "Data Intelligence: Modern Data Science Methods\n\nSpring 2025\nMonday, Wednesday, 11 AM to 1250 PM\nLecturer: Derek\n\nI go by “Derek” or “teacher”\n\n\n\n\n\n\n\n\nCourse Description\n\n\n\n\n\nThis course provides the training for students to be independent in modern data analysis. The course emphasizes the rigorous treatment of data and the programming skills and conceptual understanding required for dealing with modern datasets. The course examines data analysis through the lens of statistics and machine learning methods. Students verify their understanding by working with real datasets. The course also covers supporting topics such as experiment design, ethical data use, best practices for statistical and machine learning methods, reproducible research, writing a quantitative research paper, and presenting research results."
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#lecturer",
    "href": "posts/301_01_introduction/01_introduction.html#lecturer",
    "title": "1: Introductions",
    "section": "Lecturer",
    "text": "Lecturer"
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#current-research-in-pedagogy",
    "href": "posts/301_01_introduction/01_introduction.html#current-research-in-pedagogy",
    "title": "1: Introductions",
    "section": "Current Research in Pedagogy",
    "text": "Current Research in Pedagogy\n\n\n\n\n\nactive learning\ncomputer programming\nflipped classrooms"
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#textbooks",
    "href": "posts/301_01_introduction/01_introduction.html#textbooks",
    "title": "1: Introductions",
    "section": "Textbooks",
    "text": "Textbooks\n\nList1234\n\n\n\nAn Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani, Taylor\nDeep Learning Illustrated by Jon Krohn\nHow AI Works by Ronald T Kneusel\nProbabilistic Machine Learning by Kevin Patrick Murphy\n\n\n\n\n\n\nISLP\n\n\n\nAn Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani, Taylor\n\n\n\n\n\n\nDeep Learning\n\n\n\nDeep Learning Illustrated by Jon Krohn\n\n\n\n\n\n\nHow AI Works\n\n\n\nHow AI Works by Ronald T Kneusel\n\n\n\n\n\n\nProbabilistic Machine Learning\n\n\n\nProbabilistic Machine Learning by Kevin Patrick Murphy"
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#cooperative-classroom",
    "href": "posts/301_01_introduction/01_introduction.html#cooperative-classroom",
    "title": "1: Introductions",
    "section": "Cooperative Classroom",
    "text": "Cooperative Classroom\nLearning in a cooperative environment should be stimulating, demanding, and fair. Because this approach to learning is different from the competitive classroom structure that many other courses used to be based on, it is important for us to be clear about mutual expectations. Below are my expectations for students in this class. This set of expectations is intended to maximize debate and exchange of ideas in an atmosphere of mutual respect while preserving individual ownership of ideas and written words. If you feel you do not understand or cannot agree to these expectations, you should discuss this with your instructor and classmates.\n\nStudents are expected to work cooperatively with other members of the class and show respect for the ideas and contributions of other people.\nWhen working as part of a group, students should strive to be good contributors to the group, listen to others, not dominate, and recognize the contributions of others. Students should try to ensure that everyone in the group is welcome to contribute and recognize that everyone contributes in different ways to a group process.\nStudents should explore data, make observations, and develop inferences as part of a group. If you use material from published sources, you must provide appropriate attribution.\n\n\n\n(Students will be asked to acknowledge this document in an online form.)\nThis document has been adapted from Scientific Teaching by Jo Handelsman, Sarah Miller, and Christine Pfund\n\n\n\n\nScientific Teaching"
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#pep-talk",
    "href": "posts/301_01_introduction/01_introduction.html#pep-talk",
    "title": "1: Introductions",
    "section": "Pep Talk",
    "text": "Pep Talk\nLearning R can be difficult at first—it is like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you will be using like ggplot2—made this wise observation:\n\n\n\n\n\n\nWisdom from Hadley Wickham\n\n\n\n\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\n\n\nIf you are finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, ask questions … e-mail [Derek], etc. I promise you can do this.\n—Andrew Heiss, Georgia State University"
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#inclusion-statement",
    "href": "posts/301_01_introduction/01_introduction.html#inclusion-statement",
    "title": "1: Introductions",
    "section": "Inclusion Statement",
    "text": "Inclusion Statement\nI value all students regardless of their background, country of origin, race, religion, ethnicity, gender, sexual orientation, disability status, etc. and am committed to providing a climate of excellence and inclusiveness within all aspects of the course. If there are aspects of your culture or identity that you would like to share with me as they relate to your success in this class, I am happy to meet to discuss. Likewise, if you have any concerns in this area or facing any special issues or challenges, you are encouraged to discuss the matter with me (set up a meeting by e-mail) with an assurance of full confidentiality (only exception being mandatory reporting of academic integrity code violations or sexual harassment)."
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#tensorflow-playground",
    "href": "posts/301_01_introduction/01_introduction.html#tensorflow-playground",
    "title": "1: Introductions",
    "section": "TensorFlow Playground",
    "text": "TensorFlow Playground\n\nlink: https://playground.tensorflow.org\nexplore the various menus and buttons\nfeel free to run a simulation"
  },
  {
    "objectID": "posts/301_02_convergence/02_convergence.html",
    "href": "posts/301_02_convergence/02_convergence.html",
    "title": "2: Convergence",
    "section": "",
    "text": "Goal: Discuss convergence\nObjective: Explore some Python codes about root finding and stochastic processes\n\n\nAs we get started, try to load a session in Google Colab"
  },
  {
    "objectID": "posts/301_02_convergence/02_convergence.html#start",
    "href": "posts/301_02_convergence/02_convergence.html#start",
    "title": "2: Convergence",
    "section": "",
    "text": "Goal: Discuss convergence\nObjective: Explore some Python codes about root finding and stochastic processes\n\n\nAs we get started, try to load a session in Google Colab"
  },
  {
    "objectID": "posts/301_02_convergence/02_convergence.html#tensorflow-playground",
    "href": "posts/301_02_convergence/02_convergence.html#tensorflow-playground",
    "title": "2: Convergence",
    "section": "TensorFlow Playground",
    "text": "TensorFlow Playground\n\nlink: https://playground.tensorflow.org\nexplore the various menus and buttons\nfeel free to run a simulation"
  },
  {
    "objectID": "posts/301_02_convergence/02_convergence.html#sequences",
    "href": "posts/301_02_convergence/02_convergence.html#sequences",
    "title": "2: Convergence",
    "section": "Sequences",
    "text": "Sequences\n\nindex: \\(n \\in \\mathbb{N} = \\{1, 2, 3, 4, 5, ...\\}\\)\nformulaic: f(n) = 2n - 1\n\n\\[1, 3, 5, 7, 9, ...\\]\n\nconstructive:\n\n\\[3, 3.1, 3.14, 3.141, 3.1415, 3.14159, ...\\]\n\nshapes:\n\n\n\n\ntriangular numbers\n\n\n\nimage source: BYJUs"
  },
  {
    "objectID": "posts/301_02_convergence/02_convergence.html#random-variables",
    "href": "posts/301_02_convergence/02_convergence.html#random-variables",
    "title": "2: Convergence",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable has no set value, but rather represents an element of chance. We can better understand a random variable through statistics like\n\nmean\nvariance\ndistribution\n\n\n\n\n\n\n\nStochastic Process\n\n\n\n\n\nA stochastic process is a sequence of random variables"
  },
  {
    "objectID": "posts/301_02_convergence/02_convergence.html#application-dinner-choices",
    "href": "posts/301_02_convergence/02_convergence.html#application-dinner-choices",
    "title": "2: Convergence",
    "section": "Application: Dinner Choices",
    "text": "Application: Dinner Choices\nSuppose that we have a Princeton student whose behavior includes eating only three types of dinner:\n\\[S = \\{\\text{ramen}, \\text{pizza}, \\text{sushi}\\}\\]\nwith transition matrix\n\\[P = \\left(\\begin{array}{ccc}\n0.2 & 0.4 & 0.4 \\\\\n0.3 & 0.4 & 0.3 \\\\\n0.2 & 0.2 & 0.6\n\\end{array}\\right)\\]\n\n\n\ndinner choices network\n\n\n\n\n\n\n\n\nNetwork terminology\n\n\n\n\n\n\ndirected versus undirected graphs\ncyclic versus acyclic graphs\n\nLater studies focus on DAGs: directed, acyclic network graphs\n\n\n\nSuppose that, on a Monday, the student’s preferences are\n\\[x_0 = \\left(\\begin{array}{ccc} 0.5 & 0.25 & 0.25 \\end{array}\\right)\\]\n\nWhat is the probability that the student will eat ramen on Tuesday (i.e. the next day)?\nWhat is the probability that the student will eat pizza on Wednesday (i.e. two days later)?\nWhat is the long-term dinner-choice behavior of this student?"
  },
  {
    "objectID": "posts/highcharter_example/highcharter_example.html",
    "href": "posts/highcharter_example/highcharter_example.html",
    "title": "Highcharter Example",
    "section": "",
    "text": "colnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\""
  },
  {
    "objectID": "posts/highcharter_example/highcharter_example.html#data",
    "href": "posts/highcharter_example/highcharter_example.html#data",
    "title": "Highcharter Example",
    "section": "",
    "text": "colnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\""
  },
  {
    "objectID": "posts/highcharter_example/highcharter_example.html#ggplot",
    "href": "posts/highcharter_example/highcharter_example.html#ggplot",
    "title": "Highcharter Example",
    "section": "ggplot",
    "text": "ggplot\n\npenguins |&gt;\n  ggplot() +\n  geom_point(aes(x = flipper_length_mm, y = bill_length_mm),\n             color = \"blue\") +\n  geom_point(aes(x = flipper_length_mm, y = bill_depth_mm),\n             color = \"red\") +\n  labs(title = \"Simultaneous Graphs\",\n       subtitle = \"secondary y axis\",\n       caption = \"Data Source: Palmer Penguins\") +\n  scale_y_continuous(\n    name = 'bill length (mm)',\n    sec.axis = sec_axis(~ . * 1.0, 'bill depth (mm)')\n  ) +\n  theme_minimal() +\n  theme(\n    axis.title.y = element_text(color = 'blue'),\n    axis.title.y.right = element_text(color = 'red')\n  )\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "posts/highcharter_example/highcharter_example.html#highcharter",
    "href": "posts/highcharter_example/highcharter_example.html#highcharter",
    "title": "Highcharter Example",
    "section": "highcharter",
    "text": "highcharter\n\nhchart(\n  object = penguins,\n  mapping = hcaes(x = flipper_length_mm, y = bill_length_mm),\n  type = \"scatter\", color = \"blue\", yaxis = 1\n) |&gt;\n  hc_add_series(\n    data = penguins,\n    mapping = hcaes(x = flipper_length_mm, y = bill_depth_mm),\n    type = \"scatter\", color = \"red\", yaxis = 2\n  ) |&gt;\n  hc_yAxis_multiples(\n    list(title = list(text = \"Bill Length (mm)\", style = list(color = \"blue\")),\n         labels = list(style = list(color = \"blue\"))),\n    list(opposite = TRUE, title = list(text = \"Bill Depth (mm)\", \n                                       style = list(color = \"red\")),\n         labels = list(style = list(color = \"red\"))))"
  },
  {
    "objectID": "posts/01_intro/01_intro.html",
    "href": "posts/01_intro/01_intro.html",
    "title": "1: Introduction",
    "section": "",
    "text": "Goal: Introduce data science\nObjective: Explore a data set with bar graphs and facets\n\n\n\n\n\nImage credit: Steven Geringer\n\n\n\n\n\n\n\n\nlibrary(\"ggtext\")\nlibrary(\"gt\")\nlibrary(\"gtExtras\")\nlibrary(\"sf\")\nlibrary(\"tidytext\")\nlibrary(\"tidyverse\")\n\nprecept_df &lt;- readr::read_csv(\"precept_workload.csv\")\ntopics_df &lt;- readr::read_csv(\"SML201_lecture_schedule.csv\")"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#learning-objectives",
    "href": "posts/01_intro/01_intro.html#learning-objectives",
    "title": "1: Introduction",
    "section": "",
    "text": "Goal: Introduce data science\nObjective: Explore a data set with bar graphs and facets\n\n\n\n\n\nImage credit: Steven Geringer"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#packages",
    "href": "posts/01_intro/01_intro.html#packages",
    "title": "1: Introduction",
    "section": "",
    "text": "library(\"ggtext\")\nlibrary(\"gt\")\nlibrary(\"gtExtras\")\nlibrary(\"sf\")\nlibrary(\"tidytext\")\nlibrary(\"tidyverse\")\n\nprecept_df &lt;- readr::read_csv(\"precept_workload.csv\")\ntopics_df &lt;- readr::read_csv(\"SML201_lecture_schedule.csv\")"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#prior",
    "href": "posts/01_intro/01_intro.html#prior",
    "title": "1: Introduction",
    "section": "Prior",
    "text": "Prior\nDuring the Fall 2024 semester, the Demographics Survey yielded the following information about majors and minors.\n\ndf_major &lt;- readr::read_csv(\"majors_minors_Fall_2024.csv\")\n\nRows: 132 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): major, minor1, minor2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndf_minor &lt;- data.frame(c(df_major$minor1, df_major$minor2))\ncolnames(df_minor) &lt;- \"minor\"\ndf_minor &lt;- df_minor |&gt; filter(!is.na(minor))\n\n\nmajorsminors"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#data-wrangling",
    "href": "posts/01_intro/01_intro.html#data-wrangling",
    "title": "1: Introduction",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nEPANIHNSFBanned\n\n\n\ncolnames(EPA_raw)\n\n [1] \"grant_id\"                   \"status\"                    \n [3] \"event_history\"              \"termination_date\"          \n [5] \"termination_indicator\"      \"reinstatement_date\"        \n [7] \"reinstatement_indicator\"    \"project_title\"             \n [9] \"organization\"               \"start_date\"                \n[11] \"original_end_date\"          \"award_value\"               \n[13] \"award_outlaid\"              \"award_frozen\"              \n[15] \"award_remaining\"            \"notes\"                     \n[17] \"org_state\"                  \"org_city\"                  \n[19] \"org_county\"                 \"org_zip\"                   \n[21] \"project_description\"        \"org_type\"                  \n[23] \"award_region\"               \"gs_org\"                    \n[25] \"cfda_number\"                \"cfda_title\"                \n[27] \"funding_opportunity_number\" \"funding_opportunity_goals\" \n[29] \"usaspending_url\"            \"nggs_url\"                  \n\n\n\n\n\ncolnames(NIH_raw)\n\n [1] \"status\"                    \"core_award_number\"        \n [3] \"full_award_number\"         \"hhs_web_reported\"         \n [5] \"hhs_pdf_reported\"          \"self_reported\"            \n [7] \"court_reported\"            \"source_reported\"          \n [9] \"targeted_start_date\"       \"targeted_end_date\"        \n[11] \"ever_frozen\"               \"frozen_date\"              \n[13] \"unfrozen_date\"             \"file_c_outlays\"           \n[15] \"termination_date\"          \"cancellation_source\"      \n[17] \"reinstatement_indicator\"   \"reinstated_est_date\"      \n[19] \"reinstatement_case\"        \"last_payment_month\"       \n[21] \"last_payment_date\"         \"project_title\"            \n[23] \"activity_code\"             \"org_name\"                 \n[25] \"org_type\"                  \"dept_type\"                \n[27] \"program_office\"            \"org_state\"                \n[29] \"org_city\"                  \"org_congdist\"             \n[31] \"us_rep\"                    \"us_rep_phone\"             \n[33] \"flagged_words\"             \"study_section\"            \n[35] \"foa\"                       \"foa_title\"                \n[37] \"abstract_text\"             \"phr_text\"                 \n[39] \"terms\"                     \"total_award\"              \n[41] \"total_estimated_outlays\"   \"total_estimated_remaining\"\n[43] \"spending_categories\"       \"notes\"                    \n[45] \"org_traits\"                \"pct_ugrad_pellgrant\"      \n[47] \"pct_ugrad_fedloan\"         \"appl_id\"                  \n[49] \"prog_office_code\"          \"funding_category\"         \n[51] \"nih_activity\"              \"court_restoration_url\"    \n[53] \"usaspending_url\"           \"taggs_url\"                \n[55] \"reporter_url\"              \"record_sha1\"              \n\n\n\n\n\ncolnames(NSF_raw)\n\n [1] \"grant_id\"                        \"status\"                         \n [3] \"terminated\"                      \"suspended\"                      \n [5] \"termination_date\"                \"termination_indicator\"          \n [7] \"reinstated\"                      \"reinstatement_date\"             \n [9] \"reinstatement_indicator\"         \"cruz_list\"                      \n[11] \"nsf_url\"                         \"usaspending_url\"                \n[13] \"project_title\"                   \"abstract\"                       \n[15] \"org_name\"                        \"org_state\"                      \n[17] \"org_city\"                        \"award_type\"                     \n[19] \"usasp_start_date\"                \"usasp_end_date\"                 \n[21] \"nsf_start_date\"                  \"nsf_end_date\"                   \n[23] \"nsf_program_name\"                \"nsf_primary_program\"            \n[25] \"usasp_nsf_office\"                \"nsf_total_budget\"               \n[27] \"nsf_obligated\"                   \"usasp_total_obligated\"          \n[29] \"usasp_obligation_hist\"           \"usasp_total_obligated_corrected\"\n[31] \"usasp_outlaid\"                   \"estimated_budget\"               \n[33] \"estimated_outlays\"               \"estimated_remaining\"            \n[35] \"post_termination_deobligation\"   \"division\"                       \n[37] \"directorate\"                     \"div\"                            \n[39] \"dir\"                             \"record_sha1\"                    \n\n\n\n\nhttps://grantwritingandfunding.com/banned-and-trigger-words-in-federal-grant-writing-in-the-trump-administration-2-0/\n\nbanned_words_df &lt;- as.data.frame(banned_words)\nbanned_words_df &lt;- banned_words_df |&gt;\n  mutate(banned_words = stringr::str_trim(banned_words),\n         n_char = nchar(banned_words)) |&gt;\n  filter(n_char &gt; 0) |&gt;\n  mutate(banned_words = stringr::str_to_lower(banned_words))\n\nbanned_words &lt;- banned_words_df |&gt;\n  pull(banned_words)\n\n\nprint(banned_words_df$banned_words)\n\n [1] \"activism\"              \"activists\"             \"advocacy\"             \n [4] \"advocate\"              \"barrier\"               \"barriers\"             \n [7] \"biased\"                \"bias\"                  \"bipoc\"                \n[10] \"black and latinx\"      \"community diversity\"   \"community equity\"     \n[13] \"cultural differences\"  \"cultural heritage\"     \"culturally responsive\"\n[16] \"disabilities\"          \"discrimination\"        \"discriminatory\"       \n[19] \"backgrounds\"           \"groups\"                \"diversified\"          \n[22] \"diversify\"             \"enhancing\"             \"equal opportunity\"    \n[25] \"equality\"              \"equitable\"             \"ethnicity\"            \n[28] \"excluded\"              \"female\"                \"fostering\"            \n[31] \"gender\"                \"hate speech\"           \"hispanic minority\"    \n[34] \"historically\"          \"implicit bias\"         \"inclusion\"            \n[37] \"inclusive\"             \"increase\"              \"indigenous community\" \n[40] \"inequalities\"          \"inequities\"            \"institutional\"        \n[43] \"lgbtq\"                 \"marginalize\"           \"minorities\"           \n[46] \"multicultural\"         \"polarization\"          \"political\"            \n[49] \"prejudice\"             \"privileges\"            \"promoting\"            \n[52] \"race\"                  \"racial\"                \"justice\"              \n[55] \"sense of belonging\"    \"sexual preferences\"    \"social justice\"       \n[58] \"sociocultural\"         \"socioeconomic\"         \"status\"               \n[61] \"stereotypes\"           \"systemic\"              \"trauma\"               \n[64] \"underappreciated\"      \"underrepresented\"      \"underserved\"          \n[67] \"victim\"                \"women\"                \n\n\n\n\n\n\nintersect(colnames(NIH_raw), colnames(NSF_raw))\n\n[1] \"status\"                  \"termination_date\"       \n[3] \"reinstatement_indicator\" \"project_title\"          \n[5] \"org_name\"                \"org_state\"              \n[7] \"org_city\"                \"usaspending_url\"        \n[9] \"record_sha1\"            \n\n\nFor now, I am going to focus on the NIH and NSF data since their tables are more similar. It appears that the column names are not uniform, so let’s search for columns of interest.\n\n\n\n\n\n\nNoteSubset\n\n\n\n\n\nI then looked at the spreadsheets directly.\n\nintersect_columns &lt;- intersect(colnames(NIH_raw), colnames(NSF_raw))\n\ndf_NIH &lt;- NIH_raw |&gt;\n  select(all_of(intersect_columns), abstract_text, total_award, dept_type)\n\ndf_NSF &lt;- NSF_raw |&gt;\n  select(all_of(intersect_columns), abstract, nsf_total_budget, nsf_program_name)\n\ncommon_columns &lt;- c(\"status\", \"termination_date\", \"reinstatement_ind\", \"project_title\", \"org_name\", \"org_state\", \"org_city\", \"usaspending_url\", \"record_sha1\", \"abstract\", \"budget\", \"subdivsion\")\ncolnames(df_NIH) &lt;- common_columns\ncolnames(df_NSF) &lt;- common_columns\n\ndf_NIH &lt;- df_NIH |&gt; mutate(agency = \"NIH\")\ndf_NSF &lt;- df_NSF |&gt; mutate(agency = \"NSF\")\n\ndf &lt;- rbind(df_NIH, df_NSF) |&gt;\n  mutate(org_name = stringr::str_to_title(org_name))"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#table",
    "href": "posts/01_intro/01_intro.html#table",
    "title": "1: Introduction",
    "section": "Table",
    "text": "Table\n\ntablegt code\n\n\n\n\n\n\n\n\n\n\nTerminated Grants at Princeton University\n\n\nNIH and NSF Grants\n\n\nagency\norg_city\ntermination_date\nbudget\nproject_title\n\n\n\n\nNIH\nPrinceton\n2025-03-12\n$148,460.00\nThe psychological underpinnings of gender disparities in adolescent mental health\n\n\nNIH\nPrinceton\n2025-03-12\n$247,610.00\nCircuit Reconstruction of Functionally-Identified Neurons in Deep Brain Regions: Application to Grid Cells\n\n\nNIH\nPrinceton\n2025-03-12\n$250,000.00\nBuilding Dendrite Architecture via Microtubule Nucleation\n\n\nNIH\nPrinceton\n2025-03-12\n$271,978.00\nMapping Neural Dynamics in Hormone-sensitive Networks\n\n\nNIH\nPrinceton\n2025-03-12\n$1,331,240.19\nViews of Gender in Adolescence\n\n\nNIH\nPrinceton\n2025-04-02\n$35,974.00\nThe Role of the Adaptor Protein Enkurin in Left-Right Patterning- a Promising Link Between Polycystin-2 and Calcium Signaling\n\n\nNIH\nPrinceton\n2025-05-16\n$32,974.00\nDelineating the impact of human karyopherins on hepatitis B virus species tropism\n\n\nNSF\nPRINCETON\n2025-05-22\n$61,560.00\nCollaborative Research: HCC: Designing Technologies for Marginalized Communities\n\n\nNSF\nPRINCETON\n2025-05-22\n$414,688.00\nDaily Watcher Tracking Survey and Monitoring the Effects of War on Public Opinion\n\n\nNSF\nPRINCETON\n2025-05-22\n$74,948.00\nRCN-UBE Incubator: Project Leadership - Embedding Inclusive Leadership Experiences in the STEM Classroom\n\n\nNSF\nPRINCETON\n2025-05-22\n$400,000.00\nConference: Support for Conferences and Mentoring of Women\n\n\nNIH\nPrinceton\n2025-06-05\n$1,015,148.00\nHyster (Rivas-Souchet) Diversity Supplement\n\n\n\nSource: Grant Witness\n\n\n\n\n\n\n\n\n\n\n\ndf_Princeton &lt;- df |&gt;\n  filter(stringr::str_detect(org_name, \"Princeton\")) |&gt;\n  tidyr::fill(termination_date, .direction = \"down\")\n\ndf_Princeton |&gt;\n  select(agency, org_city, termination_date, budget, project_title) |&gt;\n  arrange(termination_date) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  fmt_currency(columns = \"budget\", currency = \"USD\") |&gt;\n  tab_footnote(footnote = \"Source: Grant Witness\") |&gt;\n  tab_header(\n    title = \"Terminated Grants at Princeton University\",\n    subtitle = \"NIH and NSF Grants\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"#E77500\"),\n                 cell_text(color = \"#121212\",\n                           weight = \"bold\")),\n    locations = cells_body(columns = \"org_city\")\n  )"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#lecturer",
    "href": "posts/01_intro/01_intro.html#lecturer",
    "title": "1: Introduction",
    "section": "Lecturer",
    "text": "Lecturer"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#current-research-in-pedagogy",
    "href": "posts/01_intro/01_intro.html#current-research-in-pedagogy",
    "title": "1: Introduction",
    "section": "Current Research in Pedagogy",
    "text": "Current Research in Pedagogy\n\n\n\n\n\nactive learning\ncomputer programming\nflipped classrooms"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#identity-statement",
    "href": "posts/01_intro/01_intro.html#identity-statement",
    "title": "1: Introduction",
    "section": "Identity Statement",
    "text": "Identity Statement\n\n\n\nOriginally from Los Angeles\nMath: easier to understand through graphs\nComputer Programming: years of experience with R, Python, MATLAB, PHP, HTML, etc.\nLearning: drawn to puzzles and manageable tasks\nPersonality: shy, introvert"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#icebreaker",
    "href": "posts/01_intro/01_intro.html#icebreaker",
    "title": "1: Introduction",
    "section": "Icebreaker",
    "text": "Icebreaker\n\nname\nmajor (and minors or certificates if applicable)\nunusual goal\n\n(please pick a goal other than “get good grades”)\nwhatever time frame makes sense\n\n\n\n\n\n\n\n\nTipDerek’s Example\n\n\n\n\n\n“Hi. My name is Derek, and I majored in applied mathematics. My unusual goal is that I am trying to learn a bit of the Polish language.\n\n\n\n\n\n\n\n\n\nWarningDCP1"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#textbooks",
    "href": "posts/01_intro/01_intro.html#textbooks",
    "title": "1: Introduction",
    "section": "Textbooks",
    "text": "Textbooks\n\n\n\n\n\nR for Data Science\n\n\n\nThis course will loosely follow\n\nR for Data Science by Hadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund (online textbook)\nStatistical Inference via Data Science by Chester Ismay and Albert Y Kim (online textbook)\n\n\n\n\n\nModern Dive"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#additional-reading",
    "href": "posts/01_intro/01_intro.html#additional-reading",
    "title": "1: Introduction",
    "section": "Additional Reading",
    "text": "Additional Reading\nThe following list of books is optional for student studies, but the instructor may use some materials to add depth and interest to the course.\n\n\n\n\n\n\nTipAdditional Reading\n\n\n\n\n\n\nThe Seven Pillars of Statistical Wisdom by Stephen M Stigler provides a wonderful overview of the history of statistics and the field’s major developments.\nStatistical Rethinking by Richard McElreath is the premier body of work in the field of Bayesian analysis. This resource is great for people who want to build a strong foundation in philosophy and theory in this branch of mathematics.\nTeaching Statistics by Andrew Gelman and Deborah Nolan features a variety of classroom activities that engage audiences at prestigious universities into learning statistical concepts.\nBernoulli’s Fallacy by Aubrey Clayton is a scathing review of the history of statistics and posits that the foundations of the field are flawed."
  },
  {
    "objectID": "posts/01_intro/01_intro.html#nih-1",
    "href": "posts/01_intro/01_intro.html#nih-1",
    "title": "1: Introduction",
    "section": "NIH",
    "text": "NIH\n\ntablegt table\n\n\n\n\n\n\n\n\n\n\nTerminated Grants at Universities\n\n\nNIH Grants, top 10 organizations affected\n\n\nagency\norg_name\nnum_grants\n\n\n\n\nNIH\nColumbia University Health Sciences\n956\n\n\nNIH\nNorthwestern University At Chicago\n623\n\n\nNIH\nUniversity Of California Los Angeles\n519\n\n\nNIH\nHarvard Medical School\n352\n\n\nNIH\nBrown University\n244\n\n\nNIH\nCornell University\n189\n\n\nNIH\nDuke University\n185\n\n\nNIH\nHarvard University D/B/A Harvard School Of Public Health\n162\n\n\nNIH\nHarvard University\n144\n\n\nNIH\nUniversity Of California, San Francisco\n131\n\n\n\nSource: Grant Witness\n\n\n\n\n\n\n\n\n\n\n\ndf |&gt;\n  filter(agency == \"NIH\") |&gt;\n  group_by(org_name) |&gt;\n  mutate(num_grants = n()) |&gt;\n  ungroup() |&gt;\n  select(agency, org_name, num_grants) |&gt;\n  distinct() |&gt;\n  slice_max(n = 10, order_by = num_grants) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Source: Grant Witness\") |&gt;\n  tab_header(\n    title = \"Terminated Grants at Universities\",\n    subtitle = \"NIH Grants, top 10 organizations affected\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"springgreen\"),\n                 cell_text(color = \"#121212\",\n                           weight = \"bold\")),\n    locations = cells_body(columns = \"org_name\")\n  )"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#nsf-1",
    "href": "posts/01_intro/01_intro.html#nsf-1",
    "title": "1: Introduction",
    "section": "NSF",
    "text": "NSF\n\ntablegt code\n\n\n\n\n\n\n\n\n\n\nTerminated Grants at Universities\n\n\nNSF Grants, top 10 organizations affected\n\n\nagency\norg_name\nnum_grants\n\n\n\n\nNSF\nUniversity Of California-Los Angeles\n306\n\n\nNSF\nHarvard University\n200\n\n\nNSF\nArizona State University\n28\n\n\nNSF\nRegents Of The University Of Michigan - Ann Arbor\n28\n\n\nNSF\nUniversity Of Colorado At Boulder\n24\n\n\nNSF\nMichigan State University\n18\n\n\nNSF\nUniversity Of Washington\n17\n\n\nNSF\nFlorida International University\n17\n\n\nNSF\nUniversity Of Texas At Austin\n16\n\n\nNSF\nUniversity Of Wisconsin-Madison\n16\n\n\n\nSource: Grant Witness\n\n\n\n\n\n\n\n\n\n\n\ndf |&gt;\n  filter(agency == \"NSF\") |&gt;\n  group_by(org_name) |&gt;\n  mutate(num_grants = n()) |&gt;\n  ungroup() |&gt;\n  select(agency, org_name, num_grants) |&gt;\n  distinct() |&gt;\n  slice_max(n = 10, order_by = num_grants) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Source: Grant Witness\") |&gt;\n  tab_header(\n    title = \"Terminated Grants at Universities\",\n    subtitle = \"NSF Grants, top 10 organizations affected\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"cadetblue1\"),\n                 cell_text(color = \"#121212\",\n                           weight = \"bold\")),\n    locations = cells_body(columns = \"org_name\")\n  )"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#titles",
    "href": "posts/01_intro/01_intro.html#titles",
    "title": "1: Introduction",
    "section": "Titles",
    "text": "Titles\n\nNIHcodeNSFcode\n\n\n\n\n\n\n\n\n\n\n\n\nTerminated NIH Grants\n\n\nMost Frequent Words in Titles\n\n\nword\nn\n\n\n\n\nhealth\n518\n\n\nresearch\n404\n\n\nmechanisms\n364\n\n\ndisease\n358\n\n\nrole\n346\n\n\ncell\n335\n\n\nprogram\n283\n\n\nhiv\n280\n\n\ntraining\n272\n\n\ncancer\n265\n\n\n\nSource: Grant Witness\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerminated NIH Grants\n\n\nMost Frequent Words in Titles\n\n\nword\nn\n\n\n\n\namong\n260\n\n\nregulation\n229\n\n\ndevelopment\n224\n\n\nrisk\n207\n\n\ncare\n187\n\n\nfunction\n182\n\n\nuniversity\n181\n\n\nuse\n172\n\n\nhuman\n168\n\n\nbrain\n165\n\n\n\nSource: Grant Witness\n\n\n\n\n\n\n\n\n\n\n\n\n\nNIH_title_words &lt;- df |&gt;\n  filter(agency == \"NIH\") |&gt;\n  select(project_title) |&gt;\n  tidytext::unnest_tokens(word, project_title) |&gt;\n  count(word) |&gt;\n  anti_join(stop_words, by = \"word\")\n\ntab1 &lt;- NIH_title_words |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1:10) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Source: Grant Witness\") |&gt;\n  tab_header(\n    title = \"Terminated NIH Grants\",\n    subtitle = \"Most Frequent Words in Titles\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"springgreen\"),\n                 cell_text(color = \"#121212\",\n                           weight = \"bold\")),\n    locations = cells_body(columns = \"word\")\n  )\n\ntab2 &lt;- NIH_title_words |&gt;\n  arrange(desc(n)) |&gt;\n  slice(11:20) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Source: Grant Witness\") |&gt;\n  tab_header(\n    title = \"Terminated NIH Grants\",\n    subtitle = \"Most Frequent Words in Titles\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"springgreen\"),\n                 cell_text(color = \"#121212\",\n                           weight = \"bold\")),\n    locations = cells_body(columns = \"word\")\n  )\n\nlist(tab1, tab2) |&gt; \n  gtExtras::gt_two_column_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerminated NSF Grants\n\n\nMost Frequent Words in Titles\n\n\nword\nn\n\n\n\n\nresearch\n845\n\n\ncollaborative\n632\n\n\nstem\n405\n\n\ncareer\n211\n\n\nequity\n200\n\n\nlearning\n177\n\n\nalliance\n149\n\n\nengineering\n145\n\n\nscience\n142\n\n\neducation\n141\n\n\n\nSource: Grant Witness\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerminated NSF Grants\n\n\nMost Frequent Words in Titles\n\n\nword\nn\n\n\n\n\nfaculty\n129\n\n\nstudents\n119\n\n\ncommunity\n113\n\n\nadvance\n107\n\n\ninclusive\n97\n\n\nbased\n89\n\n\nparticipation\n83\n\n\nproject\n82\n\n\nblack\n78\n\n\nchange\n78\n\n\n\nSource: Grant Witness\n\n\n\n\n\n\n\n\n\n\n\n\n\nNSF_title_words &lt;- df |&gt;\n  filter(agency == \"NSF\") |&gt;\n  select(project_title) |&gt;\n  tidytext::unnest_tokens(word, project_title) |&gt;\n  count(word) |&gt;\n  anti_join(stop_words, by = \"word\")\n\ntab1 &lt;- NSF_title_words |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1:10) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Source: Grant Witness\") |&gt;\n  tab_header(\n    title = \"Terminated NSF Grants\",\n    subtitle = \"Most Frequent Words in Titles\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"springgreen\"),\n                 cell_text(color = \"#121212\",\n                           weight = \"bold\")),\n    locations = cells_body(columns = \"word\")\n  )\n\ntab2 &lt;- NSF_title_words |&gt;\n  arrange(desc(n)) |&gt;\n  slice(11:20) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Source: Grant Witness\") |&gt;\n  tab_header(\n    title = \"Terminated NSF Grants\",\n    subtitle = \"Most Frequent Words in Titles\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"springgreen\"),\n                 cell_text(color = \"#121212\",\n                           weight = \"bold\")),\n    locations = cells_body(columns = \"word\")\n  )\n\nlist(tab1, tab2) |&gt; \n  gtExtras::gt_two_column_layout()"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#abstracts",
    "href": "posts/01_intro/01_intro.html#abstracts",
    "title": "1: Introduction",
    "section": "Abstracts",
    "text": "Abstracts\n\nNIHcodeNSFcode\n\n\n\n\n\n\n\n\n\n\n\n\nTerminated NIH Grants\n\n\nMost Frequent Words in Abstracts\n\n\nword\nn\n\n\n\n\nresearch\n11899\n\n\nhealth\n8300\n\n\naim\n7305\n\n\n1\n6698\n\n\n2\n6580\n\n\ncell\n6488\n\n\nproject\n6216\n\n\ndata\n5630\n\n\ncells\n5325\n\n\nprogram\n4970\n\n\n\nSource: Grant Witness\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerminated NIH Grants\n\n\nMost Frequent Words in Abstracts\n\n\nword\nn\n\n\n\n\nuse\n4892\n\n\ntraining\n4850\n\n\ndevelopment\n4716\n\n\nstudy\n4487\n\n\n3\n4419\n\n\nspecific\n4396\n\n\ndisease\n4239\n\n\nnew\n4052\n\n\ncan\n4040\n\n\nstudies\n3962\n\n\n\nSource: Grant Witness\n\n\n\n\n\n\n\n\n\n\n\n\n\nNIH_abstract_words &lt;- df |&gt;\n  filter(agency == \"NIH\") |&gt;\n  select(abstract) |&gt;\n  tidytext::unnest_tokens(word, abstract) |&gt;\n  count(word) |&gt;\n  anti_join(stop_words, by = \"word\")\n\ntab1 &lt;- NIH_abstract_words |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1:10) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Source: Grant Witness\") |&gt;\n  tab_header(\n    title = \"Terminated NIH Grants\",\n    subtitle = \"Most Frequent Words in Abstracts\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"springgreen\"),\n                 cell_text(color = \"#121212\",\n                           weight = \"bold\")),\n    locations = cells_body(columns = \"word\")\n  )\n\ntab2 &lt;- NIH_abstract_words |&gt;\n  arrange(desc(n)) |&gt;\n  slice(11:20) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Source: Grant Witness\") |&gt;\n  tab_header(\n    title = \"Terminated NIH Grants\",\n    subtitle = \"Most Frequent Words in Abstracts\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"springgreen\"),\n                 cell_text(color = \"#121212\",\n                           weight = \"bold\")),\n    locations = cells_body(columns = \"word\")\n  )\n\nlist(tab1, tab2) |&gt; \n  gtExtras::gt_two_column_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerminated NSF Grants\n\n\nMost Frequent Words in Abstracts\n\n\nword\nn\n\n\n\n\nproject\n7422\n\n\nstem\n7149\n\n\nresearch\n7055\n\n\nstudents\n5050\n\n\nsupport\n3859\n\n\neducation\n2928\n\n\nprogram\n2880\n\n\nscience\n2846\n\n\nusing\n2778\n\n\nlearning\n2625\n\n\n\nSource: Grant Witness\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerminated NSF Grants\n\n\nMost Frequent Words in Abstracts\n\n\nword\nn\n\n\n\n\nfaculty\n2549\n\n\nimpacts\n2512\n\n\ndata\n2507\n\n\nevaluation\n2419\n\n\nbroader\n2374\n\n\naward\n2359\n\n\nengineering\n2357\n\n\ndevelopment\n2208\n\n\ncommunity\n2174\n\n\nnsf's\n2102\n\n\n\nSource: Grant Witness\n\n\n\n\n\n\n\n\n\n\n\n\n\nNSF_abstract_words &lt;- df |&gt;\n  filter(agency == \"NSF\") |&gt;\n  select(abstract) |&gt;\n  tidytext::unnest_tokens(word, abstract) |&gt;\n  count(word) |&gt;\n  anti_join(stop_words, by = \"word\")\n\ntab1 &lt;- NSF_abstract_words |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1:10) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Source: Grant Witness\") |&gt;\n  tab_header(\n    title = \"Terminated NSF Grants\",\n    subtitle = \"Most Frequent Words in Abstracts\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"springgreen\"),\n                 cell_text(color = \"#121212\",\n                           weight = \"bold\")),\n    locations = cells_body(columns = \"word\")\n  )\n\ntab2 &lt;- NSF_abstract_words |&gt;\n  arrange(desc(n)) |&gt;\n  slice(11:20) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Source: Grant Witness\") |&gt;\n  tab_header(\n    title = \"Terminated NSF Grants\",\n    subtitle = \"Most Frequent Words in Abstracts\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"springgreen\"),\n                 cell_text(color = \"#121212\",\n                           weight = \"bold\")),\n    locations = cells_body(columns = \"word\")\n  )\n\nlist(tab1, tab2) |&gt; \n  gtExtras::gt_two_column_layout()"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#word-clouds",
    "href": "posts/01_intro/01_intro.html#word-clouds",
    "title": "1: Introduction",
    "section": "Word Clouds",
    "text": "Word Clouds\n\ntoolsNIHcodeNSFcode\n\n\nWhile there are programmatic ways to make word clouds, I like the functionality of the WordClouds.com website. To use that third-party software, make a CSV file whose columns are: weight, word, color, url.\n\n\n\n\n\nNIH Abstracts\n\n\n\n\n\nNIH_wordcloud_df &lt;- NIH_abstract_words |&gt;\n  mutate(weight = n, .before = \"word\") |&gt;\n  select(weight, word) |&gt;\n  arrange(desc(weight)) |&gt;\n  slice(1:100) |&gt;\n  mutate(color = ifelse(word %in% banned_words, \"#ff0000\", \"\"),\n         url = \"\")\nreadr::write_csv(NIH_wordcloud_df, \"NIH_for_wordcloud.csv\")\n\n\n\n\n\n\nNSF Abstracts\n\n\n\n\n\nNSF_wordcloud_df &lt;- NSF_abstract_words |&gt;\n  mutate(weight = n, .before = \"word\") |&gt;\n  select(weight, word) |&gt;\n  arrange(desc(weight)) |&gt;\n  slice(1:100) |&gt;\n  mutate(color = ifelse(word %in% banned_words, \"#ff0000\", \"\"),\n         url = \"\")\nreadr::write_csv(NSF_wordcloud_df, \"NSF_for_wordcloud.csv\")"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#audience",
    "href": "posts/01_intro/01_intro.html#audience",
    "title": "1: Introduction",
    "section": "Audience",
    "text": "Audience\nDuring the Fall 2024 semester, the Demographics Survey yielded the following information about majors and minors.\n\nmajorsminorsR code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_major &lt;- readr::read_csv(\"majors_minors_Fall_2024.csv\")\n\ndf_minor &lt;- data.frame(c(df_major$minor1, df_major$minor2))\ncolnames(df_minor) &lt;- \"minor\"\ndf_minor &lt;- df_minor |&gt; filter(!is.na(minor))\n\ndf_major |&gt;\n  ggplot(aes(y = fct_rev(fct_infreq(major)))) +\n  geom_bar(color = \"#000000\", fill = \"#EE7F2D\", stat = \"count\") +\n  labs(title = \"Student Majors for SML 201\",\n       subtitle = \"Fall 2024 semester\",\n       caption = \"SML 201\",\n       x = \"count\", y = \"\") +\n  theme_minimal()\n\ndf_minor |&gt;\n  group_by(minor) |&gt;\n  mutate(n = n()) |&gt;\n  ungroup() |&gt;\n  filter(n &gt; 1) |&gt;\n  ggplot(aes(y = fct_rev(fct_infreq(minor)))) +\n  geom_bar(color = \"#000000\", fill = \"#EE7F2D\", stat = \"count\") +\n  labs(title = \"Student Minors for SML 201\",\n       subtitle = \"Fall 2024 semester\",\n       caption = \"SML 201\\n\n       subgroups with n &gt; 1\",\n       x = \"count\", y = \"\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#workload",
    "href": "posts/01_intro/01_intro.html#workload",
    "title": "1: Introduction",
    "section": "Workload",
    "text": "Workload\nAs part of an exit-survey, I asked students how long (in minutes) they spent on the “shortest” and “longest” precept assignments.\n\nsummarydensityR code\n\n\n\nsummary(precept_df$short)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   30.00   40.00   49.76   60.00  300.00 \n\n\n\nsummary(precept_df$long)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    1.5    90.0   120.0   150.9   180.0   900.0 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprecept_long &lt;- precept_df |&gt;\n  pivot_longer(cols = c(\"short\", \"long\"), names_to = \"contrast\", values_to = \"time\")\n\ntitle_string &lt;- \"Time Spent on &lt;span style = 'color:#0000FF'&gt;short&lt;/span&gt; and&lt;br&gt;&lt;span style = 'color:#FF0000'&gt;long&lt;/span&gt; precept assignments\"\n\nprecept_long |&gt;\n  ggplot() +\n  geom_density(aes(x = time, fill = contrast),\n               alpha = 0.75) +\n  labs(title = title_string,\n       subtitle = \"Fall 2024\",\n       caption = \"SML 201\",\n       x = \"time (minutes)\") +\n  scale_fill_manual(values = c(\"red\", \"blue\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n\n\n\n\n\n\n\n\n\n\nWarningDCP2\n\n\n\n\n\n\n\n\nincomplete example\n\n\n\ndata source\nbetter example by Nicola Rennie\n\nLinkedIn"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#cooperative-classroom",
    "href": "posts/01_intro/01_intro.html#cooperative-classroom",
    "title": "1: Introduction",
    "section": "Cooperative Classroom",
    "text": "Cooperative Classroom\nLearning in a cooperative environment should be stimulating, demanding, and fair. Because this approach to learning is different from the competitive classroom structure that many other courses used to be based on, it is important for us to be clear about mutual expectations. Below are my expectations for students in this class. This set of expectations is intended to maximize debate and exchange of ideas in an atmosphere of mutual respect while preserving individual ownership of ideas and written words. If you feel you do not understand or cannot agree to these expectations, you should discuss this with your instructor and classmates.\n\nStudents are expected to work cooperatively with other members of the class and show respect for the ideas and contributions of other people.\nWhen working as part of a group, students should strive to be good contributors to the group, listen to others, not dominate, and recognize the contributions of others. Students should try to ensure that everyone in the group is welcome to contribute and recognize that everyone contributes in different ways to a group process.\nStudents should explore data, make observations, and develop inferences as part of a group. If you use material from published sources, you must provide appropriate attribution.\n\n\n\n\nThis document has been adapted from Scientific Teaching by Jo Handelsman, Sarah Miller, and Christine Pfund\n\n\n\n\nScientific Teaching"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#pep-talk",
    "href": "posts/01_intro/01_intro.html#pep-talk",
    "title": "1: Introduction",
    "section": "Pep Talk",
    "text": "Pep Talk\nLearning R can be difficult at first—it is like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you will be using like ggplot2—made this wise observation:\n\n\n\n\n\n\nTipWisdom from Hadley Wickham\n\n\n\n\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\n\n\nIf you are finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, ask questions … e-mail [Derek], etc. I promise you can do this.\n—Andrew Heiss, Georgia State University"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#inclusion-statement",
    "href": "posts/01_intro/01_intro.html#inclusion-statement",
    "title": "1: Introduction",
    "section": "Inclusion Statement",
    "text": "Inclusion Statement\nI value all students regardless of their background, country of origin, race, religion, ethnicity, gender, sexual orientation, disability status, etc. and am committed to providing a climate of excellence and inclusiveness within all aspects of the course. If there are aspects of your culture or identity that you would like to share with me as they relate to your success in this class, I am happy to meet to discuss. Likewise, if you have any concerns in this area or facing any special issues or challenges, you are encouraged to discuss the matter with me (set up a meeting by e-mail) with an assurance of full confidentiality (only exception being mandatory reporting of academic integrity code violations or sexual harassment)."
  },
  {
    "objectID": "posts/01_intro/01_intro.html#learner-profiles",
    "href": "posts/01_intro/01_intro.html#learner-profiles",
    "title": "1: Introduction",
    "section": "Learner Profiles",
    "text": "Learner Profiles\n\nPedagogy1234\n\n\nSharing an overview of the types of students that might be taking this course.\n\n\n\n\n\nSpike\n\n\n\nSpike\n\n\n\n\n\n\n\nSenior\nHistory\nDid not like computer programming in the past, but is willing to learn now\n\n\n\n\n\n\n\n\nJet\n\n\n\nJet\n\n\n\n\n\n\n\nJunior\nPsychology\nTook AP Statistics years ago, and wants more complex case studies\n\n\n\n\n\n\n\n\nFaye\n\n\n\nFaye\n\n\n\n\n\n\n\nSophomore\nAnthropology\nWants to add “data science” to CV before applying to internships\n\n\n\n\n\n\n\n\nEd\n\n\n\nEd\n\n\n\n\n\n\n\nComputer Science\nORFE\nHas a lot of experience programming in Python, but is wondering why this class is taught in R"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#why-not-spreadsheets",
    "href": "posts/01_intro/01_intro.html#why-not-spreadsheets",
    "title": "1: Introduction",
    "section": "Why not Spreadsheets?",
    "text": "Why not Spreadsheets?\n\nengineeringreproducibility\n\n\n\n\n\nneed expansive storage\nneed control over data types\n\n\n\n\n\n\n\nCovid memories\n\n\n\nimage source: The Guardian\n\n\n\n\n\n\n\n“Reproducibility refers to the ability of a researcher to duplicate the results of a prior study using the same materials and procedures as were used by the original investigator. So in an attempt to reproduce a published statistical analysis, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis to determine whether they yield the same results.” — Harvard Data Management\n\n\n\n\n\n\nreproducible code\n\n\n\nimage source: Netherlands eScience Center"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#why-r",
    "href": "posts/01_intro/01_intro.html#why-r",
    "title": "1: Introduction",
    "section": "Why R?",
    "text": "Why R?\n\n\n\n\n\nR\n\n\n\n\n\n\nmatches data science concepts well\nlanguage made for statistics and probability calculations\nsoftware compatibility\neasier to learn\neasier to teach\ngaining popularity in areas such as consulting, finance, epidemiology, genomics, pharmaceuticals, etc."
  },
  {
    "objectID": "posts/01_intro/01_intro.html#r-vs-python",
    "href": "posts/01_intro/01_intro.html#r-vs-python",
    "title": "1: Introduction",
    "section": "R vs Python",
    "text": "R vs Python\n\ntablegt code\n\n\n\n\n\n\n\n\n\n\nData Science Programming Languages\n\n\nWhich one is better?\n\n\nR\nPython\n\n\n\n\nData Science\nMachine Learning\n\n\nDashboards\nSoftware Development\n\n\nInteractvity\nObject-Oriented Programming\n\n\nVisualization\nBig Data\n\n\nDebugging\nFaster\n\n\n\nsource: Derek's opinion\n\n\n\n\n\n\n\n\n\n\n\nlanguages_df &lt;- data.frame(\n  R = c(\"Data Science\", \"Dashboards\", \"Interactvity\", \"Visualization\", \"Debugging\"),\n  Python = c(\"Machine Learning\", \"Software Development\", \"Object-Oriented Programming\", \"Big Data\", \"Faster\")\n)\n\nlanguages_df |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"source: Derek's opinion\") |&gt;\n  tab_header(\n    title = \"Data Science Programming Languages\",\n    subtitle = \"Which one is better?\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#ffc100\")\n    ),\n    locations = cells_body(columns = R)\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#d0cef3\")\n    ),\n    locations = cells_body(columns = Python)\n  )"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#conditional-statements",
    "href": "posts/01_intro/01_intro.html#conditional-statements",
    "title": "1: Introduction",
    "section": "Conditional Statements",
    "text": "Conditional Statements\n\nIf a grant was restored, as seen in the reinstatement_ind column, set restated to TRUE\nIf a grant was not restored, as seen in the reinstatement_ind column, set restated to FALSE\nR: mutate(reinstated = ifelse(is.na(reinstatement_ind), FALSE, TRUE))\n\n\ntableR code\n\n\n\n\n\n\n\n\n\n\nReinstated NIH Grants\n\n\nTracking Missing Values\n\n\norg_state\nreinstatement_ind\nreinstated\n\n\n\n\nNY\nRemoved from TAGGS list\nTRUE\n\n\nDC\nRemoved from TAGGS list\nTRUE\n\n\nTN\nNA\nFALSE\n\n\nIL\nNA\nFALSE\n\n\nIL\nNA\nFALSE\n\n\nMA\nTermination removed from RePORTER\nTRUE\n\n\nMA\nAAUP-Harvard v DOJ, Harvard v HHS\nTRUE\n\n\nRI\nNA\nFALSE\n\n\nCA\nMA v. RFK 2025-06-18\nTRUE\n\n\nMA\nAAUP-Harvard v DOJ, Harvard v HHS\nTRUE\n\n\n\nSource: Grant Witness, sample of grants\n\n\n\n\n\n\n\n\n\n\n\ndf_NIH |&gt;\n  select(org_state, reinstatement_ind) |&gt;\n  filter(!is.na(org_state)) |&gt;\n  slice(1:10) |&gt;\n  mutate(reinstated = ifelse(is.na(reinstatement_ind), FALSE, TRUE)) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Source: Grant Witness, sample of grants\") |&gt;\n  tab_header(\n    title = \"Reinstated NIH Grants\",\n    subtitle = \"Tracking Missing Values\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"cyan\")\n    ),\n    locations = cells_body(columns = c(reinstatement_ind, reinstated),\n                           rows = reinstated == FALSE)\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"darkorange\")\n    ),\n    locations = cells_body(columns = c(reinstatement_ind, reinstated),\n                           rows = reinstated == TRUE)\n  )"
  },
  {
    "objectID": "posts/01_intro/01_intro.html#proportion",
    "href": "posts/01_intro/01_intro.html#proportion",
    "title": "1: Introduction",
    "section": "Proportion",
    "text": "Proportion\n\nConceptR codeNIHR codeNSFR code\n\n\nFinally, I am curious if some states performed better than others in getting those research grants reinstated. We will seek out the proportion of grants that were reinstated for each state.\n\n\n\ndf_state_group &lt;- df |&gt;\n  group_by(agency, org_state) |&gt;\n  mutate(num_grants = n()) |&gt;\n  ungroup() |&gt;\n  mutate(reinstated = ifelse(is.na(reinstatement_ind), FALSE, TRUE)) |&gt;\n  group_by(agency, org_state) |&gt;\n  mutate(num_reinstated = sum(reinstated)) |&gt;\n  ungroup() |&gt;\n  mutate(prop_reinstated = num_reinstated / num_grants) |&gt;\n  select(agency, org_state, prop_reinstated) |&gt;\n  distinct()\n\nshp_and_df &lt;- states_shp |&gt;\n  left_join(df_state_group, by = join_by(STUSPS == org_state))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshp_and_df |&gt;\n  filter(agency == \"NIH\") |&gt;\n  ggplot() +\n  geom_sf(aes(fill = prop_reinstated)) +\n  labs(title = \"Reinstated Grants at Universities\",\n       subtitle = \"NIH Grants\",\n       caption = \"Source: Grant Witness\") +\n  scale_fill_gradient(low = \"red\", high = \"blue\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshp_and_df |&gt;\n  filter(agency == \"NSF\") |&gt;\n  ggplot() +\n  geom_sf(aes(fill = prop_reinstated)) +\n  labs(title = \"Reinstated Grants at Universities\",\n       subtitle = \"NSF Grants\",\n       caption = \"Source: Grant Witness\") +\n  scale_fill_gradient(low = \"red\", high = \"blue\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#data-types",
    "href": "posts/02_centrality/02_Centrality.html#data-types",
    "title": "2: Centrality",
    "section": "Data types",
    "text": "Data types\nIn R, we use the str command to look at the structure of a data frame.\n\nstr(segmented_data, give.attr = FALSE)\n\ntibble [9 × 5] (S3: tbl_df/tbl/data.frame)\n $ country_code: chr [1:9] \"CAN\" \"CAN\" \"CAN\" \"MEX\" ...\n $ gender      : chr [1:9] \"Female\" \"Female\" \"Male\" \"Male\" ...\n $ height      : num [1:9] 0 0 0 0 0 0 0 180 0\n $ age         : num [1:9] 19 22 28 27 31 56 22 32 28\n $ xbar        : num [1:9] 27.2 27.2 27.2 27 27 ...\n\n\nIn this example,\n\ncountry_code and gender may be treated as categorical data\nheight and age may be treated as numerical data"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#process",
    "href": "posts/02_centrality/02_Centrality.html#process",
    "title": "2: Centrality",
    "section": "Process",
    "text": "Process\nSegmentation is a group by operation followed by an aggregation:\n\nmean\nmedian\nmode\nminimum\nmaximum\n\n\ngroup bygtmutategtaggregationgt\n\n\n\n\n\n\n\n\n\n\nSegmenting Data\n\n\n(3 observations per group shown)\n\n\ncountry_code\nage\n\n\n\n\nCAN\n19\n\n\nCAN\n22\n\n\nCAN\n28\n\n\nMEX\n27\n\n\nMEX\n31\n\n\nMEX\n56\n\n\nUSA\n22\n\n\nUSA\n32\n\n\nUSA\n28\n\n\n\nSource: Kaggle, Petro Ivaniuk\n\n\n\n\n\n\n\n\n\n\n\nsegmented_data |&gt;\n  select(country_code, age) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Source: Kaggle, Petro Ivaniuk\") |&gt;\n  tab_header(\n    title = \"Segmenting Data\",\n    subtitle = \"(3 observations per group shown)\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_text(color = \"white\", weight = \"bold\"),\n      cell_fill(color = \"#D80621\")),\n    locations = cells_body(columns = country_code,\n                           rows = country_code == \"CAN\")\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_text(color = \"white\", weight = \"bold\"),\n      cell_fill(color = \"#006341\")),\n    locations = cells_body(columns = country_code,\n                           rows = country_code == \"MEX\")\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_text(color = \"white\", weight = \"bold\"),\n      cell_fill(color = \"#0A3161\")),\n    locations = cells_body(columns = country_code,\n                           rows = country_code == \"USA\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nSegmenting Data\n\n\n(3 observations per group shown)\n\n\ncountry_code\nage\nxbar\n\n\n\n\nCAN\n19\nNA\n\n\nCAN\n22\nNA\n\n\nCAN\n28\nNA\n\n\nMEX\n27\nNA\n\n\nMEX\n31\nNA\n\n\nMEX\n56\nNA\n\n\nUSA\n22\nNA\n\n\nUSA\n32\nNA\n\n\nUSA\n28\nNA\n\n\n\nSource: Kaggle, Petro Ivaniuk\n\n\n\n\n\n\n\n\n\n\n\nsegmented_data |&gt;\n  select(country_code, age) |&gt;\n  mutate(xbar = NA) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Source: Kaggle, Petro Ivaniuk\") |&gt;\n  tab_header(\n    title = \"Segmenting Data\",\n    subtitle = \"(3 observations per group shown)\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_text(color = \"#121212\"),\n      cell_fill(color = \"#E77500\")),\n    locations = cells_body(columns = xbar)\n  )\n\n\n\n\n\n\n\n\n\n\n\nSegmenting Data\n\n\n(3 observations per group shown)\n\n\ncountry_code\nage\nxbar\n\n\n\n\nCAN\n19\n27.18072\n\n\nCAN\n22\n27.18072\n\n\nCAN\n28\n27.18072\n\n\nMEX\n27\n26.95370\n\n\nMEX\n31\n26.95370\n\n\nMEX\n56\n26.95370\n\n\nUSA\n22\n26.98869\n\n\nUSA\n32\n26.98869\n\n\nUSA\n28\n26.98869\n\n\n\nSource: Kaggle, Petro Ivaniuk\n\n\n\n\n\n\n\n\n\n\n\nsegmented_data |&gt;\n  select(country_code, age, xbar) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Source: Kaggle, Petro Ivaniuk\") |&gt;\n  tab_header(\n    title = \"Segmenting Data\",\n    subtitle = \"(3 observations per group shown)\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_text(color = \"white\", weight = \"bold\"),\n      cell_fill(color = \"#D80621\")),\n    locations = cells_body(columns = xbar,\n                           rows = country_code == \"CAN\")\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_text(color = \"white\", weight = \"bold\"),\n      cell_fill(color = \"#006341\")),\n    locations = cells_body(columns = xbar,\n                           rows = country_code == \"MEX\")\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_text(color = \"white\", weight = \"bold\"),\n      cell_fill(color = \"#0A3161\")),\n    locations = cells_body(columns = xbar,\n                           rows = country_code == \"USA\")\n  )\n\n\n\n\n\n\n\n\n\n\nWarningDCP3"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#summary-statistics-1",
    "href": "posts/02_centrality/02_Centrality.html#summary-statistics-1",
    "title": "2: Centrality",
    "section": "Summary statistics",
    "text": "Summary statistics\n\nmu1_mdeq &lt;- mean(flint_mdeq$lead, na.rm = TRUE)\nmu2_mdeq &lt;- mean(flint_mdeq$lead2, na.rm = TRUE)\nmu_vt &lt;- mean(flint_vt$lead, na.rm = TRUE)\n\ntop10_1_mdeq &lt;- quantile(flint_mdeq$lead, 0.90, na.rm = TRUE)\ntop10_2_mdeq &lt;- quantile(flint_mdeq$lead2, 0.90, na.rm = TRUE)\ntop10_vt &lt;- quantile(flint_vt$lead, 0.90, na.rm = TRUE)\n\n\nAll MDEQ Samples\n\nsummary(flint_mdeq$lead)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    2.00    3.00    7.31    6.50  104.00 \n\n\n\n\nOutliers Removed\n\nsummary(flint_mdeq$lead2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  0.000   2.000   3.000   5.725   6.000  42.000       2 \n\n\n\n\nIndependent Test\n\nsummary(flint_vt$lead)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.344   1.578   3.521  10.646   9.050 158.000"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#histograms",
    "href": "posts/02_centrality/02_Centrality.html#histograms",
    "title": "2: Centrality",
    "section": "Histograms",
    "text": "Histograms\n\nAll MDEQ SamplesOutliers RemovedIndependent Test\n\n\n\nflint_mdeq |&gt;\n  ggplot() +\n  geom_histogram(aes(x = lead),\n                 binwidth = 5, color = \"black\", fill = \"gray75\") +\n  geom_vline(xintercept = mu1_mdeq, color = \"blue\", \n             linetype = 2, linewidth = 2) +\n  geom_vline(xintercept = top10_1_mdeq, color = \"red\", \n             linetype = 2, linewidth = 2) +\n  labs(title = \"Flint Water Samples in 2015\",\n       subtitle = \"MDEQ measurements (average in blue, top 10 percentile in red)\",\n       caption = \"Source: Jen Richmond\",\n       x = \"lead (ppb)\", y = \"number of samples\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nflint_mdeq |&gt;\n  ggplot() +\n  geom_histogram(aes(x = lead2),\n                 binwidth = 5, color = \"black\", fill = \"gray75\") +\n  geom_vline(xintercept = mu1_mdeq, color = \"blue\", \n             linetype = 2, linewidth = 2) +\n  geom_vline(xintercept = top10_2_mdeq, color = \"red\", \n             linetype = 2, linewidth = 2) +\n  labs(title = \"Flint Water Samples in 2015\",\n       subtitle = \"MDEQ measurements, two samples missing (average in blue, top 10 percentile in red)\",\n       caption = \"Source: Jen Richmond\",\n       x = \"lead (ppb)\", y = \"number of samples\") +\n  theme_minimal()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\nflint_vt |&gt;\n  ggplot() +\n  geom_histogram(aes(x = lead),\n                 binwidth = 5, color = \"black\", fill = \"gray75\") +\n  geom_vline(xintercept = mu1_mdeq, color = \"blue\", \n             linetype = 2, linewidth = 2) +\n  geom_vline(xintercept = top10_vt, color = \"red\", \n             linetype = 2, linewidth = 2) +\n  labs(title = \"Flint Water Samples in 2015\",\n       subtitle = \"VT measurements (average in blue, top 10 percentile in red)\",\n       caption = \"Source: Jen Richmond\",\n       x = \"lead (ppb)\", y = \"number of samples\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#diagnoses",
    "href": "posts/02_centrality/02_Centrality.html#diagnoses",
    "title": "2: Centrality",
    "section": "Diagnoses",
    "text": "Diagnoses\n“the Lead and Copper Rule (LCR) of 1991 is 15 parts per billion (ppb). If this is exceeded in more than 10% of homes tested (or if the 90th percentile value of the total sample is above 15 ppb), action is required.” — Significance, Vol 14, Issue 2\n\nAll MDEQ Samples\n\nifelse(quantile(flint_mdeq$lead, 0.90, na.rm = TRUE) &gt; 15,\n       \"action required\", \"safe water\")\n\n              90% \n\"action required\" \n\n\n\n\nOutliers Removed\n\nifelse(quantile(flint_mdeq$lead2, 0.90, na.rm = TRUE) &gt; 15,\n       \"action required\", \"safe water\")\n\n         90% \n\"safe water\" \n\n\n\n\nIndependent Test\n\nifelse(quantile(flint_vt$lead, 0.90, na.rm = TRUE) &gt; 15,\n       \"action required\", \"safe water\")\n\n              90% \n\"action required\""
  },
  {
    "objectID": "posts/02_centrality/02_Centrality.html#application-optional-rolling-mean",
    "href": "posts/02_centrality/02_Centrality.html#application-optional-rolling-mean",
    "title": "2: Centrality",
    "section": "Application (optional): Rolling Mean",
    "text": "Application (optional): Rolling Mean\n\nDataWrangling\n\n\n\n\n\nsource: TidyTuesday (2019-12-03)\nOpen Data Philly\n\nfiltered to year 2017 data that had latitude/longitude\n\nobjective: summarize trends in ticketing\n\n\n\n\n\n\n\nPhiladelphia Parking Authority\n\n\n\nimage source: Matt Rourke/AP Photo\n\n\n\n\n\n\n# i.e. started with a very large data set\n# and needed to pare it down\ntickets_raw &lt;- readr::read_csv(\"tickets.csv\")\ntickets_days &lt;- tickets_raw |&gt;\n  separate(issue_datetime, sep = \" \",\n           into = c(\"date\", \"time\")) |&gt;\n  group_by(date) |&gt;\n  count(date) |&gt;\n  ungroup() |&gt;\n  select(date, n)\nreadr::write_csv(tickets_days, \"tickets_days.csv\")\n\ntickets_df &lt;- readr::read_csv(\"tickets_days.csv\")\n\n\n\n\n\nTime Series\n\nVizCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntickets_df |&gt;\n  ggplot(aes(x = date, y = n)) +\n  geom_line() +\n  labs(title = \"Parking Tickets in Philadelphia\",\n       subtitle = \"Street Sweeping Violations (2017)\",\n       caption = \"Source: Open Data Philly\",\n       y = \"number of tickets\") +\n  theme_minimal()\n\n\n\n\n\n\nMoving Average\n\nConcept357911Code\n\n\nA rolling mean or moving average compues the mean across a group of \\(L\\) (lag) consecutive data points in a time series and slides the “window”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntickets_df |&gt;\n  mutate(roll_mean = zoo::rollapply(\n    n, 3, mean, align = 'left', fill = NA\n  )) |&gt;\n    ggplot() +\n    geom_point(aes(x = date, y = n),\n               color = \"black\") +\n  geom_line(aes(x = date, y = roll_mean),\n            color = \"blue\") +\n    labs(title = \"Parking Tickets in Philadelphia\",\n         subtitle = \"Rolling mean in blue (n = 3 day window)\",\n         caption = \"Source: Open Data Philly\",\n         y = \"number of tickets\") +\n    theme_minimal()\n\n\n\n\n\n\nRolling Median\n\n357911Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntickets_df |&gt;\n  mutate(roll_median = zoo::rollapply(\n    n, 3, median, align = 'left', fill = NA\n  )) |&gt;\n    ggplot() +\n    geom_point(aes(x = date, y = n),\n               color = \"black\") +\n  geom_line(aes(x = date, y = roll_median),\n            color = \"red\") +\n    labs(title = \"Parking Tickets in Philadelphia\",\n         subtitle = \"Rolling median in red (n = 3 day window)\",\n         caption = \"Source: Open Data Philly\",\n         y = \"number of tickets\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\nNoteSession Info\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.5.2 (2025-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] zoo_1.8-14      lubridate_1.9.4 forcats_1.0.0   stringr_1.5.1  \n [5] dplyr_1.1.4     purrr_1.1.0     readr_2.1.5     tidyr_1.3.1    \n [9] tibble_3.3.0    ggplot2_4.0.0   tidyverse_2.0.0 gt_1.0.0       \n\nloaded via a namespace (and not attached):\n [1] sass_0.4.10        generics_0.1.4     xml2_1.3.8         stringi_1.8.7     \n [5] lattice_0.22-7     hms_1.1.3          digest_0.6.37      magrittr_2.0.3    \n [9] evaluate_1.0.4     grid_4.5.2         timechange_0.3.0   RColorBrewer_1.1-3\n[13] fastmap_1.2.0      jsonlite_2.0.0     scales_1.4.0       cli_3.6.5         \n[17] rlang_1.1.6        crayon_1.5.3       bit64_4.6.0-1      withr_3.0.2       \n[21] yaml_2.3.10        tools_4.5.2        parallel_4.5.2     tzdb_0.5.0        \n[25] curl_6.4.0         vctrs_0.6.5        R6_2.6.1           lifecycle_1.0.4   \n[29] htmlwidgets_1.6.4  bit_4.6.0          vroom_1.6.5        pkgconfig_2.0.3   \n[33] pillar_1.11.0      gtable_0.3.6       glue_1.8.0         xfun_0.52         \n[37] tidyselect_1.2.1   rstudioapi_0.17.1  knitr_1.50         farver_2.1.2      \n[41] htmltools_0.5.8.1  rmarkdown_2.29     labeling_0.4.3     compiler_4.5.2    \n[45] S7_0.2.0"
  },
  {
    "objectID": "posts/03_variance/03_variance.html",
    "href": "posts/03_variance/03_variance.html",
    "title": "3: Variance",
    "section": "",
    "text": "Goal: Introduce the concept of variance\nObjective: Compute range, variance, and standard deviation\n\n\n\n\n\n\n\nSpread!\n\n\n\n\n\nlibrary(\"ggtext\")\nlibrary(\"gt\")\nlibrary(\"tidyverse\")\n\ndow_df &lt;- readr::read_csv(\"DOW_2025.csv\")"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#start",
    "href": "posts/03_variance/03_variance.html#start",
    "title": "3: Variance",
    "section": "",
    "text": "Goal: Introduce the concept of variance\nObjective: Compute range, variance, and standard deviation\n\n\n\n\n\n\n\nSpread!\n\n\n\n\n\nlibrary(\"ggtext\")\nlibrary(\"gt\")\nlibrary(\"tidyverse\")\n\ndow_df &lt;- readr::read_csv(\"DOW_2025.csv\")"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#centrality",
    "href": "posts/03_variance/03_variance.html#centrality",
    "title": "3: Variance",
    "section": "Centrality",
    "text": "Centrality\nRecall that we can compute means and medians.\n\n\n\nmean(A)\n\n[1] 0\n\nmean(B)\n\n[1] 0\n\n\n\n\n\n\nmedian(A)\n\n[1] 0\n\nmedian(B)\n\n[1] 0"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#visualization",
    "href": "posts/03_variance/03_variance.html#visualization",
    "title": "3: Variance",
    "section": "Visualization",
    "text": "Visualization\n\nVizCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsimple_df &lt;- data.frame(A,B)\ntitle_string &lt;- \"Compare and Contrast: &lt;span style='color:blue'&gt;Set A&lt;/span&gt; versus &lt;span style='color:red'&gt;Set B&lt;/span&gt;\"\n\nsimple_df |&gt;\n  ggplot() +\n  geom_point(aes(x = A, y = 1), color = \"blue\", size = 10) +\n  geom_point(aes(x = B, y = 2), color = \"red\", size = 10) +\n  labs(title = title_string,\n       subtitle = \"What is alike?  What is different?\",\n       caption = \"SML 201\",\n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        plot.title = element_markdown(face = \"bold\", hjust = 0.5,size = 20),\n        plot.subtitle = element_markdown(hjust = 0.5,size = 15)) +\n  ylim(0,3)"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#sample-mean",
    "href": "posts/03_variance/03_variance.html#sample-mean",
    "title": "3: Variance",
    "section": "Sample Mean",
    "text": "Sample Mean\nRecall that we compute the sample mean of the data as\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}\\]\n\nH &lt;- c(75,76,63,62,58)\nxbar &lt;- mean(H)"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#deviations",
    "href": "posts/03_variance/03_variance.html#deviations",
    "title": "3: Variance",
    "section": "Deviations",
    "text": "Deviations\nNext, we can compute deviations from the mean\n\ndeviations &lt;- H - xbar\ndeviations\n\n[1]  8.2  9.2 -3.8 -4.8 -8.8"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#squared-deviations",
    "href": "posts/03_variance/03_variance.html#squared-deviations",
    "title": "3: Variance",
    "section": "Squared Deviations",
    "text": "Squared Deviations\nWe don’t need negative signs in this calculations. One way around this is to square the deviations.\n\nsq_deviations &lt;- deviations^2\nsq_deviations\n\n[1] 67.24 84.64 14.44 23.04 77.44"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#tabulation",
    "href": "posts/03_variance/03_variance.html#tabulation",
    "title": "3: Variance",
    "section": "Tabulation",
    "text": "Tabulation\nSo far we have\n\nTableCode\n\n\n\n\n\n\n\n\n\n\nNathan's Hot Dog Eating Contest\n\n\nRecent winning amounts\n\n\nhot_dogs\nxbar\ndeviations\nsq_deviations\n\n\n\n\n75\n66.8\n8.2\n67.24\n\n\n76\n66.8\n9.2\n84.64\n\n\n63\n66.8\n-3.8\n14.44\n\n\n62\n66.8\n-4.8\n23.04\n\n\n58\n66.8\n-8.8\n77.44\n\n\n\nMen's competition\n\n\n\n\n\n\n\n\n\n\n\nhot_dog_data &lt;- data.frame(hot_dogs = c(75,76,63,62,58))\nhot_dog_data |&gt;\n  mutate(xbar = mean(hot_dogs, na.rm = TRUE),\n         deviations = hot_dogs - xbar,\n         sq_deviations = deviations^2) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Men's competition\") |&gt;\n  tab_header(\n    title = \"Nathan's Hot Dog Eating Contest\",\n    subtitle = \"Recent winning amounts\") |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  )"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#summarizing",
    "href": "posts/03_variance/03_variance.html#summarizing",
    "title": "3: Variance",
    "section": "Summarizing",
    "text": "Summarizing\nLike before, we want to summarize a list of numbers.\n\\[s^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2} = \\frac{266.8}{4} = 66.7\\]\nAt this point, the calculation has produced a sample variance\n\nWhy “n-1”? See later session about “Estimators”\nBut what are the units?"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#dimensional-analysis",
    "href": "posts/03_variance/03_variance.html#dimensional-analysis",
    "title": "3: Variance",
    "section": "Dimensional Analysis",
    "text": "Dimensional Analysis\n\nTableCodeWhat?\n\n\n\n\n\n\n\n\n\n\nNathan's Hot Dog Eating Contest\n\n\nRecent winning amounts\n\n\nhot_dogs\nxbar\ndeviations\nsq_deviations\n\n\n\n\n75\n66.8\n8.2\n67.24 (hot dogs)^2\n\n\n76\n66.8\n9.2\n84.64 (hot dogs)^2\n\n\n63\n66.8\n-3.8\n14.44 (hot dogs)^2\n\n\n62\n66.8\n-4.8\n23.04 (hot dogs)^2\n\n\n58\n66.8\n-8.8\n77.44 (hot dogs)^2\n\n\n\nMen's competition\n\n\n\n\n\n\n\n\n\n\n\nhot_dog_data &lt;- data.frame(hot_dogs = c(75,76,63,62,58))\nhot_dog_data |&gt;\n  mutate(xbar = mean(hot_dogs, na.rm = TRUE),\n         deviations = hot_dogs - xbar,\n         sq_deviations = paste(round(deviations^2,2), \n                               \"(hot dogs)^2\")) |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"Men's competition\") |&gt;\n  tab_header(\n    title = \"Nathan's Hot Dog Eating Contest\",\n    subtitle = \"Recent winning amounts\") |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = cell_text(color = \"red\"),\n    locations = cells_body(columns = sq_deviations)\n  )\n\n\n\nThe sample variance is\n\\[s^{2} = 66.7 \\,(\\text{hot dogs})^{2}\\]\n\n\n\nsquare hot dogs?\n\n\n\nimage created with Canva AI"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#rectify",
    "href": "posts/03_variance/03_variance.html#rectify",
    "title": "3: Variance",
    "section": "Rectify",
    "text": "Rectify\nIf we need to use these results in subsequent calculations, we can fix the units by taking the square root of the sample variance. This yields the sample standard deviation\n\\[s = \\sqrt{66.7} \\approx 8.1670 \\text{ hot dogs}\\]"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#sse",
    "href": "posts/03_variance/03_variance.html#sse",
    "title": "3: Variance",
    "section": "SSE",
    "text": "SSE\nFor the sum of squared errors, what value of \\(c\\) will minimize the error?\n\\[\\text{SSE} = \\sum_{i=1}^{n} (x_{i} - c)^{2}, \\quad H = \\{75,76,63,62,58\\}\\]\n\ncvals &lt;- seq(58, 76)\nSSE &lt;- rep(NA, length(cvals))\nfor(i in 1:length(cvals)){\n  SSE[i] &lt;- sum((H - cvals[i])^{2})\n}\nmin(SSE)\n\n[1] 267\n\ncvals[which.min(SSE)]\n\n[1] 67\n\n\n\ncvals &lt;- seq(58, 76, by = 0.1)\nSSE &lt;- rep(NA, length(cvals))\nfor(i in 1:length(cvals)){\n  SSE[i] &lt;- sum((H - cvals[i])^{2})\n}\nmin(SSE)\n\n[1] 266.8\n\ncvals[which.min(SSE)]\n\n[1] 66.8\n\nmean(H)\n\n[1] 66.8\n\n\nClaim: The sample mean minimizes the sum of squared errors.\n\n\n\n\n\n\nNote(optional, not on exam) Calculus proof\n\n\n\n\n\nFor a non-constant data set \\(\\{x_{i}\\}_{i=1}^{n}\\), and for the sum of squared errors\n\\[S(c) = \\sum_{i=1}^{n} (x_{i} - c)^{2}\\]\nwe can set the derivative equal to zero\n\\[\\begin{array}{rcl}\n  0 & = & \\frac{dS}{dc} \\\\\n  0 & = & \\frac{d}{dc} \\sum_{i=1}^{n} (x_{i} - c)^{2} \\\\\n  0 & = & \\sum_{i=1}^{n} \\frac{d}{dc} (x_{i} - c)^{2} \\\\\n  0 & = & \\sum_{i=1}^{n} 2(x_{i} - c) \\\\\n  0 & = &  2\\sum_{i=1}^{n} x_{i} - 2nc \\\\\n  0 & = &  \\sum_{i=1}^{n} x_{i} - nc \\\\\n  nc & = &  \\sum_{i=1}^{n} x_{i} \\\\\n  c & = & \\frac{1}{n}\\sum_{i=1}^{n} x_{i} \\\\\n\\end{array}\\]\nWe recognize that the right-hand side is the sample mean. Since the function was a concave up parabola, we know that this critical point is a global minimum."
  },
  {
    "objectID": "posts/03_variance/03_variance.html#absolute-value",
    "href": "posts/03_variance/03_variance.html#absolute-value",
    "title": "3: Variance",
    "section": "Absolute Value",
    "text": "Absolute Value\nWhat if we had used the absolute value instead? We can use a similar argument on the sum of absolute errors.\n\ncvals &lt;- seq(58, 76, by = 0.1)\nSE &lt;- rep(NA, length(cvals))\nfor(i in 1:length(cvals)){\n  SE[i] &lt;- sum(abs(H - cvals[i]))\n}\nmin(SE)\n\n[1] 31\n\ncvals[which.min(SE)]\n\n[1] 63\n\nmedian(H)\n\n[1] 63\n\n\nClaim: The sample median minimizes the sum of absolute errors.\n\\[SE(c) = \\sum_{i=1}^{n} |x_{i} - c|\\]\n\n\n\n\n\n\nNote(optional, not on exam) Outline of proof\n\n\n\n\n\n\nargue that the summation is smallest when one of terms is zero\ninterpolate for the case when the number of observations is even"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#data-dow-jones-industrial-average",
    "href": "posts/03_variance/03_variance.html#data-dow-jones-industrial-average",
    "title": "3: Variance",
    "section": "Data: Dow Jones Industrial Average",
    "text": "Data: Dow Jones Industrial Average\n\n\n\n30 popular stocks\nYear 2025\nsource: Yahoo Finance\n\n\n\n\n\n\n\nDow Jones Industrial Average\n\n\n\nimage source: OPO Finance"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#exploration",
    "href": "posts/03_variance/03_variance.html#exploration",
    "title": "3: Variance",
    "section": "Exploration",
    "text": "Exploration\n\nhead(dow_df)\n\n# A tibble: 6 × 11\n  ticker ref_date   price_open price_high price_low price_close   volume\n  &lt;chr&gt;  &lt;date&gt;          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 AAPL   2025-01-02       249.       249.      242.        244. 55740700\n2 AAPL   2025-01-03       243.       244.      242.        243. 40244100\n3 AAPL   2025-01-06       244.       247.      243.        245  45045600\n4 AAPL   2025-01-07       243.       246.      241.        242. 40856000\n5 AAPL   2025-01-08       242.       244.      240.        243. 37628900\n6 AAPL   2025-01-10       240.       240.      233         237. 61710900\n# ℹ 4 more variables: price_adjusted &lt;dbl&gt;, ret_adjusted_prices &lt;dbl&gt;,\n#   ret_closing_prices &lt;dbl&gt;, cumret_adjusted_prices &lt;dbl&gt;\n\n\n\nstr(dow_df, give.attr = FALSE)\n\nspc_tbl_ [8,100 × 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ ticker                : chr [1:8100] \"AAPL\" \"AAPL\" \"AAPL\" \"AAPL\" ...\n $ ref_date              : Date[1:8100], format: \"2025-01-02\" \"2025-01-03\" ...\n $ price_open            : num [1:8100] 249 243 244 243 242 ...\n $ price_high            : num [1:8100] 249 244 247 246 244 ...\n $ price_low             : num [1:8100] 242 242 243 241 240 ...\n $ price_close           : num [1:8100] 244 243 245 242 243 ...\n $ volume                : num [1:8100] 55740700 40244100 45045600 40856000 37628900 ...\n $ price_adjusted        : num [1:8100] 243 242 244 241 242 ...\n $ ret_adjusted_prices   : num [1:8100] NA -0.00201 0.00674 -0.01139 0.00202 ...\n $ ret_closing_prices    : num [1:8100] NA -0.00201 0.00674 -0.01139 0.00202 ...\n $ cumret_adjusted_prices: num [1:8100] 1 0.998 1.005 0.993 0.995 ...\n\n\n\ncolnames(dow_df)\n\n [1] \"ticker\"                 \"ref_date\"               \"price_open\"            \n [4] \"price_high\"             \"price_low\"              \"price_close\"           \n [7] \"volume\"                 \"price_adjusted\"         \"ret_adjusted_prices\"   \n[10] \"ret_closing_prices\"     \"cumret_adjusted_prices\""
  },
  {
    "objectID": "posts/03_variance/03_variance.html#which-stocks",
    "href": "posts/03_variance/03_variance.html#which-stocks",
    "title": "3: Variance",
    "section": "Which Stocks?",
    "text": "Which Stocks?\nThe table command tallys the observations in a categorical variable.\n\ntable(dow_df$ticker)\n\n\nAAPL AMGN AMZN  AXP   BA  CAT  CRM CSCO  CVX  DIS   GS   HD  HON  IBM  JNJ  JPM \n 270  270  270  270  270  270  270  270  270  270  270  270  270  270  270  270 \n  KO  MCD  MMM  MRK MSFT  NKE NVDA   PG  SHW  TRV  UNH    V   VZ  WMT \n 270  270  270  270  270  270  270  270  270  270  270  270  270  270"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#histograms",
    "href": "posts/03_variance/03_variance.html#histograms",
    "title": "3: Variance",
    "section": "Histograms",
    "text": "Histograms\n\n\n\n\n\n\nNoteData Visualization Guideline\n\n\n\nA histogram shows the distribution of one numerical variable.\n\n\n\ndow_df |&gt;\n  filter(ticker == \"VZ\") |&gt;\n  ggplot(aes(x = price_close)) +\n  geom_histogram(bins = 25) +\n  labs(title = \"Verizon stock\",\n       subtitle = \"2025\",\n       caption = \"SML 201\")\n\n\n\n\n\n\n\n\n\ndow_df |&gt;\n  filter(ticker == \"GS\") |&gt;\n  ggplot(aes(x = price_close)) +\n  geom_histogram(bins = 25) +\n  labs(title = \"Goldman Sachs stock\",\n       subtitle = \"2025\",\n       caption = \"SML 201\")"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#z-scores-1",
    "href": "posts/03_variance/03_variance.html#z-scores-1",
    "title": "3: Variance",
    "section": "Z-scores",
    "text": "Z-scores\nSometimes, we want to rescale numerical columns to be able to compare them together.\n\nsummary(dow_df$price_close)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  37.81  137.82  227.69  241.77  317.19  975.86 \n\n\n\ndow_df &lt;- dow_df |&gt;\n  mutate(price_scaled = scale(price_close)) #z-scores\n\n\nsummary(dow_df$price_scaled)\n\n       V1          \n Min.   :-1.38266  \n 1st Qu.:-0.70469  \n Median :-0.09546  \n Mean   : 0.00000  \n 3rd Qu.: 0.51134  \n Max.   : 4.97654"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#expensive-stocks",
    "href": "posts/03_variance/03_variance.html#expensive-stocks",
    "title": "3: Variance",
    "section": "Expensive Stocks",
    "text": "Expensive Stocks\nWhich stocks have had the highest average price_close this year?\n\ndow_df |&gt;\n  group_by(ticker) |&gt;\n  mutate(avg_price = mean(price_close, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(ticker, avg_price) |&gt;\n  distinct() |&gt;\n  arrange(desc(avg_price))\n\n\n\n\n\n# A tibble: 30 × 2\n   ticker avg_price\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 GS          706.\n 2 MSFT        464.\n 3 CAT         435.\n 4 UNH         378.\n 5 HD          377.\n 6 SHW         346.\n 7 V           343.\n 8 AXP         318.\n 9 MCD         305.\n10 AMGN        299.\n# ℹ 20 more rows\n\n\n\n\n\n\nGoldman Sachs\nMicrosoft\nCaterpillar\nUnited Health Group\nHome Depot\nSherwin-Williams\nVisa\nAmerican Express\nMcDonald’s\nAmgen"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#volatility",
    "href": "posts/03_variance/03_variance.html#volatility",
    "title": "3: Variance",
    "section": "Volatility",
    "text": "Volatility\n\nMost VolatileLeast Volatile\n\n\nWhich stocks were the most volatile last year?\n\ndow_df |&gt;\n  group_by(ticker) |&gt;\n  mutate(volatility = sd(price_close, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(ticker, volatility) |&gt;\n  distinct() |&gt;\n  arrange(desc(volatility)) #from largest to smallest\n\n\n\n\n\n# A tibble: 30 × 2\n   ticker volatility\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 GS          124. \n 2 CAT         104. \n 3 UNH          92.8\n 4 MSFT         48.8\n 5 AXP          36.0\n 6 NVDA         29.9\n 7 CRM          29.8\n 8 JPM          28.5\n 9 IBM          26.7\n10 AAPL         26.4\n# ℹ 20 more rows\n\n\n\n\n\n\nGoldman Sachs\nCaterpillar\nUnited Health Group\nMicrosoft\nAmerican Express\nNVidia\nSalesforce\nJP Morgan Chase\nIBM\nApple\n\n\n\n\n\nWhich stocks were the least volatile last year?\n\ndow_df |&gt;\n  group_by(ticker) |&gt;\n  mutate(volatility = sd(price_close, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(ticker, volatility) |&gt;\n  distinct() |&gt;\n  arrange(volatility) #from smallest to largest\n\n\n\n\n\n# A tibble: 30 × 2\n   ticker volatility\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 VZ           1.86\n 2 KO           2.83\n 3 CSCO         6.37\n 4 NKE          6.78\n 5 WMT          8.27\n 6 CVX          8.32\n 7 MCD          8.86\n 8 DIS          8.86\n 9 PG           8.87\n10 MRK          9.66\n# ℹ 20 more rows\n\n\n\n\n\n\nVerizon\nCoca-Cola\nCisco\nNike\nWalmart\nChevron\nMcDonald’s\nDisney\nProctor and Gamble\nMerck"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#line-plots",
    "href": "posts/03_variance/03_variance.html#line-plots",
    "title": "3: Variance",
    "section": "Line Plots",
    "text": "Line Plots\nLet us compare the stocks with the lowest and the most volatility.\n\nVizCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitle_string &lt;- \"&lt;span style='color:blue'&gt;Verizon&lt;/span&gt; and &lt;span style='color:red'&gt;Goldman Sachs&lt;/span&gt;\"\nsubtitle_string &lt;- \"2025 stock prices\"\n\ndow_df |&gt;\n  ggplot() +\n  geom_line(aes(x = ref_date, y = price_close),\n            color = \"blue\", linewidth = 2,\n            data = dow_df |&gt; filter(ticker == \"VZ\")) +\n  geom_line(aes(x = ref_date, y = price_close),\n            color = \"red\", linewidth = 3,\n            data = dow_df |&gt; filter(ticker == \"GS\")) +\n  labs(title = title_string,\n       subtitle = subtitle_string,\n       caption = \"SML 201\",\n       x = \"date\", y = \"closing price\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", hjust = 0.5,size = 25),\n        plot.subtitle = element_markdown(hjust = 0.5,size = 20))"
  },
  {
    "objectID": "posts/03_variance/03_variance.html#coefficient-of-variation",
    "href": "posts/03_variance/03_variance.html#coefficient-of-variation",
    "title": "3: Variance",
    "section": "Coefficient of Variation",
    "text": "Coefficient of Variation\nWouldn’t the most expensive stocks naturally vary more?\n\\[\\text{CoV} = \\frac{s}{\\bar{x}}\\]\n\n\\(\\bar{x}\\): sample mean\n\\(s\\): sample standard deviation\n\nWhich stocks have the highest coefficients of variation this year?\n\ndow_df |&gt;\n  group_by(ticker) |&gt;\n  mutate(avg_price = mean(price_close, na.rm = TRUE)) |&gt;\n  mutate(volatility = sd(price_close, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(ticker, avg_price, volatility) |&gt;\n  distinct() |&gt;\n  mutate(coef_var = volatility / avg_price) |&gt;\n  arrange(desc(coef_var))\n\n\n\n\n\n# A tibble: 30 × 4\n   ticker avg_price volatility coef_var\n   &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 UNH        378.       92.8     0.245\n 2 CAT        435.      104.      0.239\n 3 NVDA       156.       29.9     0.191\n 4 GS         706.      124.      0.176\n 5 JNJ        174.       22.0     0.127\n 6 BA         202.       25.5     0.126\n 7 AXP        318.       36.0     0.113\n 8 AAPL       234.       26.4     0.113\n 9 CRM        267.       29.8     0.112\n10 MRK         88.8       9.66    0.109\n# ℹ 20 more rows\n\n\n\n\n\n\nUnited Health Care\nCaterpillar\nNVidia\nGoldman Sachs\nJohnson and Johnson\nBoeing\nAmerican Express\nApple\nSalesforce\nMerck"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#obtaining-the-data",
    "href": "posts/04_categories/04_categories.html#obtaining-the-data",
    "title": "4: Categories",
    "section": "Obtaining the Data",
    "text": "Obtaining the Data\n\nlibrary(\"tidyverse\")\n\n# downloading data directly from Derek's (online) GitHub repository\n# demo_df &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/data/demographics_data.csv.csv\")\n\n# or if you have the file also in your SML 201 folder\ndemo_df &lt;- readr::read_csv(\"demographics_data.csv\")"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#quick-exploration",
    "href": "posts/04_categories/04_categories.html#quick-exploration",
    "title": "4: Categories",
    "section": "Quick Exploration",
    "text": "Quick Exploration\n\nheadstructurecolumn names\n\n\n\nhead(demo_df)\n\n# A tibble: 6 × 89\n  course  stats_background   class_standing num_courses_past major            \n  &lt;chr&gt;   &lt;chr&gt;              &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;            \n1 SML 201 No                 Senior                       31 Chemistry        \n2 SML 201 No                 Sophomore                    14 Molecular Biology\n3 SML 201 No                 Sophomore                    12 Molecular Biology\n4 SML 201 Yes: AP Statistics Sophomore                    12 Anthropology     \n5 SML 201 No                 Sophomore                    14 Neuroscience     \n6 SML 201 Yes: AP Statistics Sophomore                    13 Molecular Biology\n# ℹ 84 more variables: residential_college &lt;chr&gt;, dining_hall &lt;chr&gt;,\n#   GPA_uni &lt;dbl&gt;, gender &lt;chr&gt;, hours_study &lt;dbl&gt;, birth_month &lt;chr&gt;,\n#   age &lt;dbl&gt;, height &lt;dbl&gt;, shoe_size &lt;dbl&gt;, weight &lt;dbl&gt;, calories &lt;dbl&gt;,\n#   exercise &lt;dbl&gt;, sleep_start &lt;chr&gt;, sleep_duration &lt;dbl&gt;,\n#   social_media &lt;chr&gt;, smart_phones &lt;chr&gt;, baseball &lt;chr&gt;, football &lt;chr&gt;,\n#   basketball &lt;chr&gt;, hockey &lt;chr&gt;, politics &lt;dbl&gt;, religious &lt;dbl&gt;,\n#   sexuality &lt;chr&gt;, happiness_campus &lt;dbl&gt;, happiness_city &lt;dbl&gt;, …\n\n\n\n\n\nstr(demo_df, give.attr = FALSE)\n\nspc_tbl_ [161 × 89] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ course               : chr [1:161] \"SML 201\" \"SML 201\" \"SML 201\" \"SML 201\" ...\n $ stats_background     : chr [1:161] \"No\" \"No\" \"No\" \"Yes: AP Statistics\" ...\n $ class_standing       : chr [1:161] \"Senior\" \"Sophomore\" \"Sophomore\" \"Sophomore\" ...\n $ num_courses_past     : num [1:161] 31 14 12 12 14 13 16 22 NA 21 ...\n $ major                : chr [1:161] \"Chemistry\" \"Molecular Biology\" \"Molecular Biology\" \"Anthropology\" ...\n $ residential_college  : chr [1:161] \"Yeh College\" \"Mathey\" \"Rockefeller\" \"Rockefeller\" ...\n $ dining_hall          : chr [1:161] \"Yeh\" \"Yeh\" \"Yeh\" \"Whitman\" ...\n $ GPA_uni              : num [1:161] NA NA NA 3.89 3.5 3.35 NA 3.9 4 NA ...\n $ gender               : chr [1:161] \"Male\" \"Female\" \"Female\" \"Male\" ...\n $ hours_study          : num [1:161] 24 NA NA 21 10 15 NA 20 60 NA ...\n $ birth_month          : chr [1:161] \"August\" \"August\" \"July\" \"January\" ...\n $ age                  : num [1:161] 21 20 19 19 19 20 19 20 18 20 ...\n $ height               : num [1:161] 70 67 NA 71 52 61 63 69 64 62 ...\n $ shoe_size            : num [1:161] 9.5 6.5 5 11 7.5 6 7.5 10.5 7 NA ...\n $ weight               : num [1:161] 160 115 130 172 128 105 NA 135 120 NA ...\n $ calories             : num [1:161] 1200 NA 1500 3000 1200 1500 NA 2200 1500 NA ...\n $ exercise             : num [1:161] 5 8 2 8 2 1 NA 7 2 NA ...\n $ sleep_start          : chr [1:161] \"2 AM\" \"12 midnight\" \"1 AM\" \"12 midnight\" ...\n $ sleep_duration       : num [1:161] 7 7 6 7 5 7 6 8 8 NA ...\n $ social_media         : chr [1:161] \"Instagram\" \"Instagram\" \"Instagram\" \"Instagram\" ...\n $ smart_phones         : chr [1:161] \"iPhone\" \"iPhone\" \"iPhone\" \"iPhone\" ...\n $ baseball             : chr [1:161] \"Los Angeles Dodgers\" \"Texas Rangers\" NA \"Pittsburgh Pirates\" ...\n $ football             : chr [1:161] NA \"Dallas Cowboys\" NA \"Pittsburgh Steelers\" ...\n $ basketball           : chr [1:161] \"Los Angeles Lakers\" \"Dallas Mavericks\" NA \"Los Angeles Lakers\" ...\n $ hockey               : chr [1:161] NA \"Dallas Stars\" NA \"Pittsburgh Penguins\" ...\n $ politics             : num [1:161] 37 NA 0 10 100 30 NA 50 30 NA ...\n $ religious            : num [1:161] 20 80 60 30 29 75 NA 90 50 0 ...\n $ sexuality            : chr [1:161] \"0\" \"0\" \"0\" \"5\" ...\n $ happiness_campus     : num [1:161] 90 92 100 90 67 70 NA 90 40 NA ...\n $ happiness_city       : num [1:161] 90 85 80 60 8 30 NA 90 30 NA ...\n $ anxious_201          : num [1:161] 30 35 100 30 78 80 NA 25 80 NA ...\n $ high_school_type     : chr [1:161] \"public school\" \"private school\" \"private school\" \"public school\" ...\n $ college_transition   : num [1:161] 70 NA 50 50 67 80 NA 75 60 NA ...\n $ drug_use             : chr [1:161] \"No\" \"No\" \"No\" \"Yes\" ...\n $ first_generation     : chr [1:161] \"No\" \"No\" \"Yes\" \"No\" ...\n $ languages_spoken     : num [1:161] 1.2 1 2 1.5 2.3 3 2 2.5 1.7 NA ...\n $ making_friends       : num [1:161] 90 90 60 70 59 80 NA 50 20 NA ...\n $ campus_sports        : chr [1:161] \"Yes: intramural team\" \"Yes: athletic scholarship\" \"No\" \"Yes: intramural team\" ...\n $ office_hours         : num [1:161] 90 75 50 100 87 60 NA 90 70 NA ...\n $ study_groups         : num [1:161] 50 80 80 40 89 60 NA 90 0 NA ...\n $ financial_planning   : chr [1:161] \"Yes\" \"No\" \"No\" \"Yes\" ...\n $ happiness            : num [1:161] 85 90 70 70 78 70 NA 90 50 NA ...\n $ intelligence         : num [1:161] 70 80 60 90 99 80 NA 80 50 NA ...\n $ attractiveness       : num [1:161] 45 NA 60 75 100 70 NA 65 30 NA ...\n $ favorite_color       : chr [1:161] \"blue\" \"Purple\" \"pink\" \"green\" ...\n $ favorite_number      : num [1:161] 4 37 11 6 11 10 7 7 27 NA ...\n $ showering            : num [1:161] 7 9 7 7 7 11 NA 7 7 NA ...\n $ brushing_teeth       : num [1:161] 13 NA 7 14 14 14 14 14 14 14 ...\n $ flossing             : num [1:161] 0 NA 2 0 7 7 7 7 14 7 ...\n $ hair_washing         : num [1:161] 7 3 3 7 1 2 NA 7 1 NA ...\n $ water_drinking       : num [1:161] 2 NA 3 18 2 2 NA 10 30 NA ...\n $ financial_aid        : num [1:161] 0 NA 70 100 100 80 100 50 0 NA ...\n $ future_career        : num [1:161] 70 70 80 80 0 90 NA 90 50 NA ...\n $ siblings             : num [1:161] 1 2 2 1 4 4 NA 1 3 1 ...\n $ favorite_movie       : chr [1:161] \"Star Wars\" NA \"Dead Poets Society\" \"hunger games \" ...\n $ loneliness           : num [1:161] 40 NA 50 20 50 70 NA 30 40 NA ...\n $ organized            : num [1:161] 75 NA 70 80 38 40 NA 80 50 NA ...\n $ GPA_HS               : num [1:161] NA NA NA 4 4 3.9 4 4 4 NA ...\n $ SAT                  : num [1:161] NA NA NA 1540 NA 1510 NA 1560 1570 NA ...\n $ social_active        : num [1:161] 55 NA 70 60 37 60 NA 80 40 NA ...\n $ num_friends          : num [1:161] 20 NA 15 25 7 5 NA 20 4 NA ...\n $ music_studying       : chr [1:161] \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ family_influence     : num [1:161] 0 NA NA 30 50 50 NA 50 30 NA ...\n $ living_on_campus     : chr [1:161] \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ college_job          : chr [1:161] \"No\" \"No\" \"No\" \"No\" ...\n $ campus_groups        : chr [1:161] \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ attendance           : num [1:161] 70 98 95 99 98 100 NA 90 100 NA ...\n $ pancakes_waffles     : chr [1:161] \"waffles\" \"pancakes\" \"pancakes\" \"waffles\" ...\n $ coffee_tea           : chr [1:161] \"tea\" \"(neither)\" \"tea\" \"coffee\" ...\n $ dogs_cats            : chr [1:161] \"cats\" \"dogs\" \"dogs\" \"dogs\" ...\n $ ocean_snow           : chr [1:161] \"ocean\" \"ocean\" \"ocean\" \"ocean\" ...\n $ pineapple_pizza      : chr [1:161] \"tolerable\" NA \"No!\" \"No!\" ...\n $ spicy_food           : num [1:161] 3 NA NA 3 6 4 NA 4 5 NA ...\n $ campus_acceptance    : num [1:161] 100 90 NA 70 67 70 NA 90 70 NA ...\n $ social_media_duration: num [1:161] 7 NA 3 3 7 5 NA 3 0 NA ...\n $ continents           : num [1:161] 3 2 NA 2 2 1 NA 3 6 2 ...\n $ superhero            : chr [1:161] \"Spider-Man\" NA NA \"green arrow \" ...\n $ supervillain         : chr [1:161] \"Doctor Doom\" NA NA \"octoman\" ...\n $ season               : chr [1:161] \"Spring\" \"Fall\" \"Fall\" \"Summer\" ...\n $ handedness           : chr [1:161] \"right-handed\" \"right-handed\" \"right-handed\" \"left-handed\" ...\n $ musical              : chr [1:161] \"No\" \"Yes, casually\" \"Yes, casually\" \"No\" ...\n $ campus_safety        : num [1:161] 90 NA 80 100 98 90 NA 95 70 NA ...\n $ campus_resources     : num [1:161] 70 NA 80 80 89 70 NA 100 60 NA ...\n $ laundry              : num [1:161] 2 NA 4 5 4 1 NA 4 2 NA ...\n $ favorite_class       : chr [1:161] \"EAS 224\" NA NA \"FRS 135\" ...\n $ favorite_teacher     : chr [1:161] \"Dr. VanderKam\" NA NA \"Susanna Moore \" ...\n $ uni_applied          : num [1:161] 20 1 21 2 NA 10 10 17 3 19 ...\n $ gap_year             : chr [1:161] \"No\" \"No\" \"No\" \"No\" ...\n $ survey_comfort       : num [1:161] 90 90 60 100 67 40 NA 100 50 NA ...\n\n\n\n\n\ncolnames(demo_df)\n\n [1] \"course\"                \"stats_background\"      \"class_standing\"       \n [4] \"num_courses_past\"      \"major\"                 \"residential_college\"  \n [7] \"dining_hall\"           \"GPA_uni\"               \"gender\"               \n[10] \"hours_study\"           \"birth_month\"           \"age\"                  \n[13] \"height\"                \"shoe_size\"             \"weight\"               \n[16] \"calories\"              \"exercise\"              \"sleep_start\"          \n[19] \"sleep_duration\"        \"social_media\"          \"smart_phones\"         \n[22] \"baseball\"              \"football\"              \"basketball\"           \n[25] \"hockey\"                \"politics\"              \"religious\"            \n[28] \"sexuality\"             \"happiness_campus\"      \"happiness_city\"       \n[31] \"anxious_201\"           \"high_school_type\"      \"college_transition\"   \n[34] \"drug_use\"              \"first_generation\"      \"languages_spoken\"     \n[37] \"making_friends\"        \"campus_sports\"         \"office_hours\"         \n[40] \"study_groups\"          \"financial_planning\"    \"happiness\"            \n[43] \"intelligence\"          \"attractiveness\"        \"favorite_color\"       \n[46] \"favorite_number\"       \"showering\"             \"brushing_teeth\"       \n[49] \"flossing\"              \"hair_washing\"          \"water_drinking\"       \n[52] \"financial_aid\"         \"future_career\"         \"siblings\"             \n[55] \"favorite_movie\"        \"loneliness\"            \"organized\"            \n[58] \"GPA_HS\"                \"SAT\"                   \"social_active\"        \n[61] \"num_friends\"           \"music_studying\"        \"family_influence\"     \n[64] \"living_on_campus\"      \"college_job\"           \"campus_groups\"        \n[67] \"attendance\"            \"pancakes_waffles\"      \"coffee_tea\"           \n[70] \"dogs_cats\"             \"ocean_snow\"            \"pineapple_pizza\"      \n[73] \"spicy_food\"            \"campus_acceptance\"     \"social_media_duration\"\n[76] \"continents\"            \"superhero\"             \"supervillain\"         \n[79] \"season\"                \"handedness\"            \"musical\"              \n[82] \"campus_safety\"         \"campus_resources\"      \"laundry\"              \n[85] \"favorite_class\"        \"favorite_teacher\"      \"uni_applied\"          \n[88] \"gap_year\"              \"survey_comfort\""
  },
  {
    "objectID": "posts/04_categories/04_categories.html#preprocessing",
    "href": "posts/04_categories/04_categories.html#preprocessing",
    "title": "4: Categories",
    "section": "Preprocessing",
    "text": "Preprocessing\n\n\n\n\n\n\nNoteHere is how the data was cleaned\n\n\n\n\n\n\nAdvice: you should understand each part of this code by the first exam.\n\n\ndemo_raw &lt;- readr::read_csv(\"Demographics Survey Survey Student Analysis Report.csv\",\n                          col_names = FALSE, skip = 1)\n\nshort_names &lt;- c(\"course\", \"stats_background\", \"class_standing\",\n                 \"num_courses_past\", \"major\", \"minor1\",\n                 \"minor2\", \"residential_college\", \"dining_hall\",\n                 \"GPA_uni\", \"gender\", \"ethnicity\",\n                 \"hours_study\", \"birth_month\", \"age\",\n                 \"height\", \"shoe_size\", \"weight\",\n                 \"calories\", \"exercise\", \"sleep_start\",\n                 \"sleep_duration\", \"social_media\", \"smart_phones\",\n                 \"baseball\", \"football\", \"basketball\", \n                 \"hockey\", \"politics\", \"religious\",\n                 \"sexuality\", \"happiness_campus\", \"happiness_city\",\n                 \"anxious_201\", \"high_school_type\", \"college_transition\",\n                 \"drug_use\", \"first_generation\", \"languages_spoken\",\n                 \"making_friends\", \"campus_sports\", \"office_hours\",\n                 \"study_groups\", \"financial_planning\", \"happiness\",\n                 \"intelligence\", \"attractiveness\", \"favorite_color\",\n                 \"favorite_number\", \"showering\", \"brushing_teeth\",\n                 \"flossing\", \"hair_washing\", \"water_drinking\",\n                 \"financial_aid\", \"future_career\", \"siblings\",\n                 \"favorite_movie\", \"loneliness\", \"organized\",\n                 \"GPA_HS\", \"SAT\", \"social_active\",\n                 \"num_friends\", \"music_studying\", \"family_influence\",\n                 \"living_on_campus\", \"college_job\", \"campus_groups\",\n                 \"attendance\", \"pancakes_waffles\", \"coffee_tea\",\n                 \"dogs_cats\", \"ocean_snow\", \"pineapple_pizza\",\n                 \"spicy_food\", \"campus_acceptance\", \"social_media_duration\",\n                 \"continents\", \"superhero\", \"supervillain\",\n                 \"season\", \"handedness\", \"musical\",\n                 \"campus_safety\", \"campus_resources\", \"laundry\",\n                 \"favorite_class\", \"favorite_teacher\", \"uni_applied\",\n                 \"uni_accepted\", \"gap_year\", \"survey_comfort\", \n                 \"additional_questions\"\n                 )\n\ndemo_df &lt;- demo_raw |&gt;\n  # remove student names and Canvas metadeta\n  select(seq(9, 193, 2)) |&gt;\n\n  # apply short column names (i.e. ease programming)\n  setNames(short_names) |&gt;\n  \n  # shuffle all rows (i.e. no longer alphabetical by student name)\n  sample_frac(1.0) |&gt;\n  \n  # mask majors who are underrepresented\n  group_by(major) |&gt;\n  mutate(majorCount = n()) |&gt;\n  ungroup() |&gt;\n  mutate(major = ifelse(majorCount &gt;= 3, major, \"other\")) |&gt;\n  select(-majorCount) |&gt;\n  \n  # remove other possible ID factors\n  select(-c(ethnicity, minor1, minor2, uni_accepted)) |&gt;\n  mutate(age = ifelse(age &lt; 19 | age &gt; 21, NA, age)) |&gt;\n  mutate(class_standing = ifelse(class_standing %in% c(\"Sophomore\", \"Junior\", \"Senior\"), class_standing, \"Senior\")) |&gt;\n  mutate(gender = ifelse(gender %in% c(\"Female\", \"Male\"), gender, NA)) |&gt;\n  mutate(num_courses_past = ifelse(num_courses_past &lt; 10, NA, num_courses_past)) |&gt;\n  \n  # remove outliers\n  mutate(GPA_uni = ifelse(GPA_uni &lt; 2 | GPA_uni &gt; 4, NA, GPA_uni)) |&gt;\n  mutate(GPA_HS = ifelse(GPA_HS &lt; 3 | GPA_HS &gt; 4, NA, GPA_HS)) |&gt;\n  mutate(SAT = ifelse(SAT &lt; 1200 | SAT &gt; 1600, NA, SAT)) |&gt;\n  mutate(politics = ifelse(politics &lt; 0, 0, politics)) |&gt;\n  mutate(politics = ifelse(politics &gt; 100, 100, politics)) |&gt;\n  mutate(siblings = ifelse(siblings &gt; 4, 4, siblings)) |&gt;\n  mutate(spicy_food = ifelse(spicy_food &gt; 7, 7, spicy_food))\n\nreadr::write_csv(demo_df, \"demographics_data..csv\")\n\n\n\n\n\n\n\n\n\n\nWarningDCP1"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#numerical-summary",
    "href": "posts/04_categories/04_categories.html#numerical-summary",
    "title": "4: Categories",
    "section": "Numerical Summary",
    "text": "Numerical Summary\n“On a scale from 0 to 100—with 0 = very anxious and 100 = comfortable—how comfortable were you taking this survey?”\n\nsummary(demo_df$survey_comfort)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00   71.25   90.00   84.62  100.00  100.00      15"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#categorical-summary",
    "href": "posts/04_categories/04_categories.html#categorical-summary",
    "title": "4: Categories",
    "section": "Categorical Summary",
    "text": "Categorical Summary\n“What is your favorite season?”\n\ntable(demo_df$season)\n\n\n  Fall Spring Summer Winter \n    48     43     59      6"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#cross-tabulation",
    "href": "posts/04_categories/04_categories.html#cross-tabulation",
    "title": "4: Categories",
    "section": "Cross-Tabulation",
    "text": "Cross-Tabulation\n\ntable(demo_df$season, demo_df$gap_year)\n\n        \n         No Yes\n  Fall   43   4\n  Spring 38   3\n  Summer 51   6\n  Winter  6   0\n\n\n\n\n\n\n\n\nWarningDCP2"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#numerical-across-a-category",
    "href": "posts/04_categories/04_categories.html#numerical-across-a-category",
    "title": "4: Categories",
    "section": "Numerical Across a Category",
    "text": "Numerical Across a Category\n\ndemo_df |&gt;\n  group_by(gap_year) |&gt;\n  summarize(avg_age = mean(age, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  gap_year avg_age\n  &lt;chr&gt;      &lt;dbl&gt;\n1 No          19.5\n2 Yes         20.8\n3 &lt;NA&gt;        19.8"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#seeking-proportions",
    "href": "posts/04_categories/04_categories.html#seeking-proportions",
    "title": "4: Categories",
    "section": "Seeking Proportions",
    "text": "Seeking Proportions\n“Did you take a gap year?”\n\n# carefully defining denominator without missing values\nn &lt;- sum(!is.na(demo_df$gap_year))\ntable(demo_df$gap_year) / n\n\n\n        No        Yes \n0.90967742 0.09032258 \n\n\n\nmean(demo_df$gap_year == \"Yes\", na.rm = TRUE)\n\n[1] 0.09032258"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#bar-chart-count",
    "href": "posts/04_categories/04_categories.html#bar-chart-count",
    "title": "4: Categories",
    "section": "Bar Chart — Count",
    "text": "Bar Chart — Count\n“Do you play a musical instrument(s)?”\n\nLater: discern difference between stat = \"count\" and stat = \"identity\"\n\n\ndemo_df |&gt;\n  # filter(!is.na(musical)) |&gt;\n  ggplot(aes(x = musical)) +\n  geom_bar(color = \"black\", fill = \"green\", stat = \"count\") +\n  labs(title = \"Do you play a musical instrument(s)?\",\n       subtitle = \"SML 201, Spring 2026\",\n       x = \"\",\n       y = \"count\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#most-represented-major-by-residential-college",
    "href": "posts/04_categories/04_categories.html#most-represented-major-by-residential-college",
    "title": "4: Categories",
    "section": "Most Represented Major by Residential College",
    "text": "Most Represented Major by Residential College\n\ndemo_df |&gt;\n  group_by(residential_college, major) |&gt;\n  mutate(major_count = n()) |&gt;\n  ungroup() |&gt;\n  group_by(residential_college) |&gt;\n  filter(major_count == max(major_count)) |&gt;\n  ungroup() |&gt;\n  select(residential_college, major) |&gt;\n  distinct() |&gt;\n  arrange(residential_college)\n\n# A tibble: 10 × 2\n   residential_college major                                               \n   &lt;chr&gt;               &lt;chr&gt;                                               \n 1 Butler              Neuroscience                                        \n 2 Butler              Molecular Biology                                   \n 3 Forbes              Princeton School of Public and International Affairs\n 4 Forbes              Molecular Biology                                   \n 5 Forbes              Neuroscience                                        \n 6 Mathey              Molecular Biology                                   \n 7 New College West    Neuroscience                                        \n 8 Rockefeller         Molecular Biology                                   \n 9 Whitman             Molecular Biology                                   \n10 Yeh College         Molecular Biology                                   \n\n\n\n\n\n\n\n\nWarningDCP3"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#outcomes",
    "href": "posts/04_categories/04_categories.html#outcomes",
    "title": "4: Categories",
    "section": "Outcomes",
    "text": "Outcomes\n\nLogical AND\n\nbot_df &lt;- bot_df |&gt;\n  mutate(outcome = case_when(\n    essay_cheat & detect_cheat ~ \"true positive\",\n    !essay_cheat & detect_cheat ~ \"false positive\",\n    essay_cheat & !detect_cheat ~ \"false negative\",\n    !essay_cheat & !detect_cheat ~ \"true negative\",\n    TRUE ~ \"unknown classification\"\n  ))"
  },
  {
    "objectID": "posts/04_categories/04_categories.html#counts",
    "href": "posts/04_categories/04_categories.html#counts",
    "title": "4: Categories",
    "section": "Counts",
    "text": "Counts\n\nTP &lt;- sum(bot_df$outcome == \"true positive\")\nTN &lt;- sum(bot_df$outcome == \"true negative\")\nFP &lt;- sum(bot_df$outcome == \"false positive\")\nFN &lt;- sum(bot_df$outcome == \"false negative\")\n\n\ntable(bot_df$essay_cheat, bot_df$detect_cheat)\n\n       \n        FALSE TRUE\n  FALSE    32   87\n  TRUE     25   57\n\n\n\nprint(paste0(\"There were \", TP, \" true positives\"))\n\n[1] \"There were 57 true positives\"\n\nprint(paste0(\"There were \", TN, \" true negatives\"))\n\n[1] \"There were 32 true negatives\"\n\nprint(paste0(\"There were \", FP, \" false positives\"))\n\n[1] \"There were 87 false positives\"\n\nprint(paste0(\"There were \", FN, \" false negatives\"))\n\n[1] \"There were 25 false negatives\""
  },
  {
    "objectID": "posts/04_categories/04_categories.html#metrics",
    "href": "posts/04_categories/04_categories.html#metrics",
    "title": "4: Categories",
    "section": "Metrics",
    "text": "Metrics\n\nLogical OR\n\n\n\n\n\n\nNoteAccuracy\n\n\n\nAccuracy is the ratio of true positives OR true negatives over all outcomes\n\\[\\text{accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}\\]\n\n\n\n# accuracy = (TP + TN) / (TP + FP + FN + TN)\naccuracy &lt;- mean(bot_df$outcome == \"true positive\" |\n               bot_df$outcome == \"true negative\")\nprint(round(accuracy, 2))\n\n[1] 0.44\n\n\n\nThis AI bot made correct classifications 44 percent of the time.\n\n\n\nPrecision\n\\[\\text{precision} = \\frac{TP}{TP + FP}\\]\n\nprecision = TP / (TP + FP)\nprint(round(precision, 2))\n\n[1] 0.4\n\n\n\nOut of all of the essays were participants intentionally cheated, the bot correctly classified those essays as “AI assisted” 40 percent of the time.\n\n\n\nRecall\n\\[\\text{recall} = \\frac{TP}{TP + FN}\\]\n\nrecall = TP / (TP + FN)\nprint(round(recall, 2))\n\n[1] 0.7\n\n\n\nAmong all of the essays that were marked as “AI assisted”, 70 percent of those were created by cheating.\n\n\n\n\n\n\n\nNoteSensitivity and specificity\n\n\n\nAll of these contingency table formulas are collected nicely on the Wikipedia page for sensitivity and specificity."
  },
  {
    "objectID": "posts/05_geospatial/05_geospatial.html",
    "href": "posts/05_geospatial/05_geospatial.html",
    "title": "5: Geospatial",
    "section": "",
    "text": "Goal: Visualize geospatial data\nObjective: Merge shapefiles and data files\n\n\n\n\n\n\n\nNJ at a glance\n\n\n\nimage source: Totally Bamboo\n\n\n\n\n\n\n\nlibrary(\"gt\")\nlibrary(\"sf\") #special features, useful for geography\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\n# nj_health &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/06_geospatial/nj_health.csv\")\n# nj_pop    &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/06_geospatial/nj_pop.csv\")\n# nj_shp    &lt;- readr::read_rds(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/06_geospatial/nj_shp.rds\")\n\nnj_health &lt;- readr::read_csv(\"nj_health.csv\")\nnj_pop    &lt;- readr::read_csv(\"nj_pop.csv\")\nnj_shp    &lt;- readr::read_rds(\"nj_shp.rds\")"
  },
  {
    "objectID": "posts/05_geospatial/05_geospatial.html#start",
    "href": "posts/05_geospatial/05_geospatial.html#start",
    "title": "5: Geospatial",
    "section": "",
    "text": "Goal: Visualize geospatial data\nObjective: Merge shapefiles and data files\n\n\n\n\n\n\n\nNJ at a glance\n\n\n\nimage source: Totally Bamboo"
  },
  {
    "objectID": "posts/05_geospatial/05_geospatial.html#load-data-and-packages",
    "href": "posts/05_geospatial/05_geospatial.html#load-data-and-packages",
    "title": "5: Geospatial",
    "section": "",
    "text": "library(\"gt\")\nlibrary(\"sf\") #special features, useful for geography\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\n# nj_health &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/06_geospatial/nj_health.csv\")\n# nj_pop    &lt;- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/06_geospatial/nj_pop.csv\")\n# nj_shp    &lt;- readr::read_rds(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/posts/06_geospatial/nj_shp.rds\")\n\nnj_health &lt;- readr::read_csv(\"nj_health.csv\")\nnj_pop    &lt;- readr::read_csv(\"nj_pop.csv\")\nnj_shp    &lt;- readr::read_rds(\"nj_shp.rds\")"
  },
  {
    "objectID": "posts/05_geospatial/05_geospatial.html#centroids",
    "href": "posts/05_geospatial/05_geospatial.html#centroids",
    "title": "5: Geospatial",
    "section": "Centroids",
    "text": "Centroids\n\n# Calculate the centroid of each hexagon to add the label\n# https://stackoverflow.com/questions/49343958/do-the-values-returned-by-rgeosgcentroid-and-sfst-centroid-differ\ncenters &lt;- data.frame(\n  st_coordinates(st_centroid(nj_shp$geometry)),\n  id=nj_shp$COUNTY)\n\nnj_counties &lt;- nj_shp |&gt;\n  left_join(centers, by = c(\"COUNTY\" = \"id\"))"
  },
  {
    "objectID": "posts/05_geospatial/05_geospatial.html#text-for-labels",
    "href": "posts/05_geospatial/05_geospatial.html#text-for-labels",
    "title": "5: Geospatial",
    "section": "Text (for labels)",
    "text": "Text (for labels)\n\nnj_counties |&gt;\n  ggplot() +\n  geom_sf(aes(fill = COUNTY)) +\n  geom_text(aes(x = X, y = Y, label = COUNTY)) +\n  labs(title = \"Counties of New Jersey\",\n       subtitle = \"categorical data\",\n       caption = \"SML 201\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/05_geospatial/05_geospatial.html#labels",
    "href": "posts/05_geospatial/05_geospatial.html#labels",
    "title": "5: Geospatial",
    "section": "Labels",
    "text": "Labels\n\nnj_counties |&gt;\n  ggplot() +\n  geom_sf(aes(fill = COUNTY)) +\n  geom_label(aes(x = X, y = Y, label = COUNTY)) +\n  labs(title = \"Counties of New Jersey\",\n       subtitle = \"categorical data\",\n       caption = \"SML 201\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/05_geospatial/05_geospatial.html#subset",
    "href": "posts/05_geospatial/05_geospatial.html#subset",
    "title": "5: Geospatial",
    "section": "Subset",
    "text": "Subset\n\nnj_label_subset &lt;- nj_counties |&gt;\n  filter(COUNTY %in% c(\"MERCER\", \"SUSSEX\", \"SALEM\"))\n\nnj_counties |&gt;\n  ggplot() +\n  geom_sf(aes(fill = COUNTY)) +\n  geom_label(aes(x = X, y = Y, label = COUNTY),\n             data = nj_label_subset,\n             size = 2) +\n  labs(title = \"Counties of New Jersey\",\n       subtitle = \"categorical data\",\n       caption = \"SML 201\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/05_geospatial/05_geospatial.html#example-one-county",
    "href": "posts/05_geospatial/05_geospatial.html#example-one-county",
    "title": "5: Geospatial",
    "section": "Example: One County",
    "text": "Example: One County\n\nnj_df |&gt;\n  ggplot() +\n  geom_sf(fill = \"gray90\") +\n  geom_sf(fill = \"orange\",\n          data = nj_counties |&gt;\n            filter(COUNTY == \"MERCER\")) +\n  labs(title = \"Princeton University\",\n       subtitle = \"is in Mercer County\",\n       caption = \"SML 201\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/05_geospatial/05_geospatial.html#example-cluster-of-counties",
    "href": "posts/05_geospatial/05_geospatial.html#example-cluster-of-counties",
    "title": "5: Geospatial",
    "section": "Example: Cluster of Counties",
    "text": "Example: Cluster of Counties\n\ncoastal_counties &lt;- c(\"MONMOUTH\", \"OCEAN\", \"ATLANTIC\", \"CAPE MAY\")\n\nnj_df |&gt;\n  ggplot() +\n  geom_sf(fill = \"gray90\") +\n  geom_sf(fill = \"bisque\",\n          data = nj_counties |&gt;\n            filter(COUNTY %in% coastal_counties)) +\n  labs(title = \"Atlantic Ocean Beaches\",\n       subtitle = \"among New Jersey counties\",\n       caption = \"SML 201\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/05_geospatial/05_geospatial.html#examples-of-results",
    "href": "posts/05_geospatial/05_geospatial.html#examples-of-results",
    "title": "5: Geospatial",
    "section": "Examples of Results",
    "text": "Examples of Results\nThese images were from year 2023 data (but you will work on year 2025 data).\n\nExercise 5Exercise 8Exercise 10\n\n\n\n\n\nmap\n\n\n\n\n\n\n\nbar graph\n\n\n\n\n\n\n\nbar graph"
  },
  {
    "objectID": "posts/06_correlation/06_correlation.html",
    "href": "posts/06_correlation/06_correlation.html",
    "title": "6: Correlation and Paradoxes",
    "section": "",
    "text": "Goal: Explore covariance\nObjective: Compute interquartile ranges and correlations\n\n\n\n\n\n\n\ncorrelation\n\n\n\nimage source: XKCD\n\n\n\n\nlibrary(\"corrplot\")  # plots correlation matrices\nlibrary(\"ggsignif\")  # significance levels in boxplots\nlibrary(\"gt\")        # great tables\nlibrary(\"patchwork\") # arrange plots side-by-side\nlibrary(\"tidyverse\") # data wrangling and visualization\n\ncorrelatedValues = function(x, r = 0.9){\n  r2 = r**2\n  ve = 1-r2\n  SD = sqrt(ve)\n  e  = rnorm(length(x), mean=0, sd=SD)\n  y  = r*x + e\n  return(y)\n}\n\n\n\n\n\n\n\nCoffee Ratings\nsource: Coffee Quality Database\nhost: TidyTuesday — July 7, 2020\n\n\n\n\n\n# coffee_df &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-07/coffee_ratings.csv')\ncoffee_df &lt;- readr::read_csv(\"coffee_ratings.csv\")\n\n\n\n\n\n\n\nheadstructurecolnames\n\n\n\nhead(coffee_df)\n\n# A tibble: 6 × 43\n  total_cup_points species owner    country_of_origin farm_name lot_number mill \n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;\n1             90.6 Arabica metad p… Ethiopia          \"metad p… &lt;NA&gt;       meta…\n2             89.9 Arabica metad p… Ethiopia          \"metad p… &lt;NA&gt;       meta…\n3             89.8 Arabica grounds… Guatemala         \"san mar… &lt;NA&gt;       &lt;NA&gt; \n4             89   Arabica yidneka… Ethiopia          \"yidneka… &lt;NA&gt;       wole…\n5             88.8 Arabica metad p… Ethiopia          \"metad p… &lt;NA&gt;       meta…\n6             88.8 Arabica ji-ae a… Brazil             &lt;NA&gt;     &lt;NA&gt;       &lt;NA&gt; \n# ℹ 36 more variables: ico_number &lt;chr&gt;, company &lt;chr&gt;, altitude &lt;chr&gt;,\n#   region &lt;chr&gt;, producer &lt;chr&gt;, number_of_bags &lt;dbl&gt;, bag_weight &lt;chr&gt;,\n#   in_country_partner &lt;chr&gt;, harvest_year &lt;chr&gt;, grading_date &lt;chr&gt;,\n#   owner_1 &lt;chr&gt;, variety &lt;chr&gt;, processing_method &lt;chr&gt;, aroma &lt;dbl&gt;,\n#   flavor &lt;dbl&gt;, aftertaste &lt;dbl&gt;, acidity &lt;dbl&gt;, body &lt;dbl&gt;, balance &lt;dbl&gt;,\n#   uniformity &lt;dbl&gt;, clean_cup &lt;dbl&gt;, sweetness &lt;dbl&gt;, cupper_points &lt;dbl&gt;,\n#   moisture &lt;dbl&gt;, category_one_defects &lt;dbl&gt;, quakers &lt;dbl&gt;, color &lt;chr&gt;, …\n\n\n\n\n\nstr(coffee_df, give.attr = FALSE)\n\nspc_tbl_ [1,339 × 43] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ total_cup_points     : num [1:1339] 90.6 89.9 89.8 89 88.8 ...\n $ species              : chr [1:1339] \"Arabica\" \"Arabica\" \"Arabica\" \"Arabica\" ...\n $ owner                : chr [1:1339] \"metad plc\" \"metad plc\" \"grounds for health admin\" \"yidnekachew dabessa\" ...\n $ country_of_origin    : chr [1:1339] \"Ethiopia\" \"Ethiopia\" \"Guatemala\" \"Ethiopia\" ...\n $ farm_name            : chr [1:1339] \"metad plc\" \"metad plc\" \"san marcos barrancas \\\"san cristobal cuch\" \"yidnekachew dabessa coffee plantation\" ...\n $ lot_number           : chr [1:1339] NA NA NA NA ...\n $ mill                 : chr [1:1339] \"metad plc\" \"metad plc\" NA \"wolensu\" ...\n $ ico_number           : chr [1:1339] \"2014/2015\" \"2014/2015\" NA NA ...\n $ company              : chr [1:1339] \"metad agricultural developmet plc\" \"metad agricultural developmet plc\" NA \"yidnekachew debessa coffee plantation\" ...\n $ altitude             : chr [1:1339] \"1950-2200\" \"1950-2200\" \"1600 - 1800 m\" \"1800-2200\" ...\n $ region               : chr [1:1339] \"guji-hambela\" \"guji-hambela\" NA \"oromia\" ...\n $ producer             : chr [1:1339] \"METAD PLC\" \"METAD PLC\" NA \"Yidnekachew Dabessa Coffee Plantation\" ...\n $ number_of_bags       : num [1:1339] 300 300 5 320 300 100 100 300 300 50 ...\n $ bag_weight           : chr [1:1339] \"60 kg\" \"60 kg\" \"1\" \"60 kg\" ...\n $ in_country_partner   : chr [1:1339] \"METAD Agricultural Development plc\" \"METAD Agricultural Development plc\" \"Specialty Coffee Association\" \"METAD Agricultural Development plc\" ...\n $ harvest_year         : chr [1:1339] \"2014\" \"2014\" NA \"2014\" ...\n $ grading_date         : chr [1:1339] \"April 4th, 2015\" \"April 4th, 2015\" \"May 31st, 2010\" \"March 26th, 2015\" ...\n $ owner_1              : chr [1:1339] \"metad plc\" \"metad plc\" \"Grounds for Health Admin\" \"Yidnekachew Dabessa\" ...\n $ variety              : chr [1:1339] NA \"Other\" \"Bourbon\" NA ...\n $ processing_method    : chr [1:1339] \"Washed / Wet\" \"Washed / Wet\" NA \"Natural / Dry\" ...\n $ aroma                : num [1:1339] 8.67 8.75 8.42 8.17 8.25 8.58 8.42 8.25 8.67 8.08 ...\n $ flavor               : num [1:1339] 8.83 8.67 8.5 8.58 8.5 8.42 8.5 8.33 8.67 8.58 ...\n $ aftertaste           : num [1:1339] 8.67 8.5 8.42 8.42 8.25 8.42 8.33 8.5 8.58 8.5 ...\n $ acidity              : num [1:1339] 8.75 8.58 8.42 8.42 8.5 8.5 8.5 8.42 8.42 8.5 ...\n $ body                 : num [1:1339] 8.5 8.42 8.33 8.5 8.42 8.25 8.25 8.33 8.33 7.67 ...\n $ balance              : num [1:1339] 8.42 8.42 8.42 8.25 8.33 8.33 8.25 8.5 8.42 8.42 ...\n $ uniformity           : num [1:1339] 10 10 10 10 10 10 10 10 9.33 10 ...\n $ clean_cup            : num [1:1339] 10 10 10 10 10 10 10 10 10 10 ...\n $ sweetness            : num [1:1339] 10 10 10 10 10 10 10 9.33 9.33 10 ...\n $ cupper_points        : num [1:1339] 8.75 8.58 9.25 8.67 8.58 8.33 8.5 9 8.67 8.5 ...\n $ moisture             : num [1:1339] 0.12 0.12 0 0.11 0.12 0.11 0.11 0.03 0.03 0.1 ...\n $ category_one_defects : num [1:1339] 0 0 0 0 0 0 0 0 0 0 ...\n $ quakers              : num [1:1339] 0 0 0 0 0 0 0 0 0 0 ...\n $ color                : chr [1:1339] \"Green\" \"Green\" NA \"Green\" ...\n $ category_two_defects : num [1:1339] 0 1 0 2 2 1 0 0 0 4 ...\n $ expiration           : chr [1:1339] \"April 3rd, 2016\" \"April 3rd, 2016\" \"May 31st, 2011\" \"March 25th, 2016\" ...\n $ certification_body   : chr [1:1339] \"METAD Agricultural Development plc\" \"METAD Agricultural Development plc\" \"Specialty Coffee Association\" \"METAD Agricultural Development plc\" ...\n $ certification_address: chr [1:1339] \"309fcf77415a3661ae83e027f7e5f05dad786e44\" \"309fcf77415a3661ae83e027f7e5f05dad786e44\" \"36d0d00a3724338ba7937c52a378d085f2172daa\" \"309fcf77415a3661ae83e027f7e5f05dad786e44\" ...\n $ certification_contact: chr [1:1339] \"19fef5a731de2db57d16da10287413f5f99bc2dd\" \"19fef5a731de2db57d16da10287413f5f99bc2dd\" \"0878a7d4b9d35ddbf0fe2ce69a2062cceb45a660\" \"19fef5a731de2db57d16da10287413f5f99bc2dd\" ...\n $ unit_of_measurement  : chr [1:1339] \"m\" \"m\" \"m\" \"m\" ...\n $ altitude_low_meters  : num [1:1339] 1950 1950 1600 1800 1950 ...\n $ altitude_high_meters : num [1:1339] 2200 2200 1800 2200 2200 NA NA 1700 1700 1850 ...\n $ altitude_mean_meters : num [1:1339] 2075 2075 1700 2000 2075 ...\n\n\n\n\n\ncolnames(coffee_df)\n\n [1] \"total_cup_points\"      \"species\"               \"owner\"                \n [4] \"country_of_origin\"     \"farm_name\"             \"lot_number\"           \n [7] \"mill\"                  \"ico_number\"            \"company\"              \n[10] \"altitude\"              \"region\"                \"producer\"             \n[13] \"number_of_bags\"        \"bag_weight\"            \"in_country_partner\"   \n[16] \"harvest_year\"          \"grading_date\"          \"owner_1\"              \n[19] \"variety\"               \"processing_method\"     \"aroma\"                \n[22] \"flavor\"                \"aftertaste\"            \"acidity\"              \n[25] \"body\"                  \"balance\"               \"uniformity\"           \n[28] \"clean_cup\"             \"sweetness\"             \"cupper_points\"        \n[31] \"moisture\"              \"category_one_defects\"  \"quakers\"              \n[34] \"color\"                 \"category_two_defects\"  \"expiration\"           \n[37] \"certification_body\"    \"certification_address\" \"certification_contact\"\n[40] \"unit_of_measurement\"   \"altitude_low_meters\"   \"altitude_high_meters\" \n[43] \"altitude_mean_meters\""
  },
  {
    "objectID": "posts/06_correlation/06_correlation.html#start",
    "href": "posts/06_correlation/06_correlation.html#start",
    "title": "6: Correlation and Paradoxes",
    "section": "",
    "text": "Goal: Explore covariance\nObjective: Compute interquartile ranges and correlations\n\n\n\n\n\n\n\ncorrelation\n\n\n\nimage source: XKCD\n\n\n\n\nlibrary(\"corrplot\")  # plots correlation matrices\nlibrary(\"ggsignif\")  # significance levels in boxplots\nlibrary(\"gt\")        # great tables\nlibrary(\"patchwork\") # arrange plots side-by-side\nlibrary(\"tidyverse\") # data wrangling and visualization\n\ncorrelatedValues = function(x, r = 0.9){\n  r2 = r**2\n  ve = 1-r2\n  SD = sqrt(ve)\n  e  = rnorm(length(x), mean=0, sd=SD)\n  y  = r*x + e\n  return(y)\n}"
  },
  {
    "objectID": "posts/06_correlation/06_correlation.html#data",
    "href": "posts/06_correlation/06_correlation.html#data",
    "title": "6: Correlation and Paradoxes",
    "section": "",
    "text": "Coffee Ratings\nsource: Coffee Quality Database\nhost: TidyTuesday — July 7, 2020\n\n\n\n\n\n# coffee_df &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-07/coffee_ratings.csv')\ncoffee_df &lt;- readr::read_csv(\"coffee_ratings.csv\")"
  },
  {
    "objectID": "posts/06_correlation/06_correlation.html#glance",
    "href": "posts/06_correlation/06_correlation.html#glance",
    "title": "6: Correlation and Paradoxes",
    "section": "",
    "text": "headstructurecolnames\n\n\n\nhead(coffee_df)\n\n# A tibble: 6 × 43\n  total_cup_points species owner    country_of_origin farm_name lot_number mill \n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;\n1             90.6 Arabica metad p… Ethiopia          \"metad p… &lt;NA&gt;       meta…\n2             89.9 Arabica metad p… Ethiopia          \"metad p… &lt;NA&gt;       meta…\n3             89.8 Arabica grounds… Guatemala         \"san mar… &lt;NA&gt;       &lt;NA&gt; \n4             89   Arabica yidneka… Ethiopia          \"yidneka… &lt;NA&gt;       wole…\n5             88.8 Arabica metad p… Ethiopia          \"metad p… &lt;NA&gt;       meta…\n6             88.8 Arabica ji-ae a… Brazil             &lt;NA&gt;     &lt;NA&gt;       &lt;NA&gt; \n# ℹ 36 more variables: ico_number &lt;chr&gt;, company &lt;chr&gt;, altitude &lt;chr&gt;,\n#   region &lt;chr&gt;, producer &lt;chr&gt;, number_of_bags &lt;dbl&gt;, bag_weight &lt;chr&gt;,\n#   in_country_partner &lt;chr&gt;, harvest_year &lt;chr&gt;, grading_date &lt;chr&gt;,\n#   owner_1 &lt;chr&gt;, variety &lt;chr&gt;, processing_method &lt;chr&gt;, aroma &lt;dbl&gt;,\n#   flavor &lt;dbl&gt;, aftertaste &lt;dbl&gt;, acidity &lt;dbl&gt;, body &lt;dbl&gt;, balance &lt;dbl&gt;,\n#   uniformity &lt;dbl&gt;, clean_cup &lt;dbl&gt;, sweetness &lt;dbl&gt;, cupper_points &lt;dbl&gt;,\n#   moisture &lt;dbl&gt;, category_one_defects &lt;dbl&gt;, quakers &lt;dbl&gt;, color &lt;chr&gt;, …\n\n\n\n\n\nstr(coffee_df, give.attr = FALSE)\n\nspc_tbl_ [1,339 × 43] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ total_cup_points     : num [1:1339] 90.6 89.9 89.8 89 88.8 ...\n $ species              : chr [1:1339] \"Arabica\" \"Arabica\" \"Arabica\" \"Arabica\" ...\n $ owner                : chr [1:1339] \"metad plc\" \"metad plc\" \"grounds for health admin\" \"yidnekachew dabessa\" ...\n $ country_of_origin    : chr [1:1339] \"Ethiopia\" \"Ethiopia\" \"Guatemala\" \"Ethiopia\" ...\n $ farm_name            : chr [1:1339] \"metad plc\" \"metad plc\" \"san marcos barrancas \\\"san cristobal cuch\" \"yidnekachew dabessa coffee plantation\" ...\n $ lot_number           : chr [1:1339] NA NA NA NA ...\n $ mill                 : chr [1:1339] \"metad plc\" \"metad plc\" NA \"wolensu\" ...\n $ ico_number           : chr [1:1339] \"2014/2015\" \"2014/2015\" NA NA ...\n $ company              : chr [1:1339] \"metad agricultural developmet plc\" \"metad agricultural developmet plc\" NA \"yidnekachew debessa coffee plantation\" ...\n $ altitude             : chr [1:1339] \"1950-2200\" \"1950-2200\" \"1600 - 1800 m\" \"1800-2200\" ...\n $ region               : chr [1:1339] \"guji-hambela\" \"guji-hambela\" NA \"oromia\" ...\n $ producer             : chr [1:1339] \"METAD PLC\" \"METAD PLC\" NA \"Yidnekachew Dabessa Coffee Plantation\" ...\n $ number_of_bags       : num [1:1339] 300 300 5 320 300 100 100 300 300 50 ...\n $ bag_weight           : chr [1:1339] \"60 kg\" \"60 kg\" \"1\" \"60 kg\" ...\n $ in_country_partner   : chr [1:1339] \"METAD Agricultural Development plc\" \"METAD Agricultural Development plc\" \"Specialty Coffee Association\" \"METAD Agricultural Development plc\" ...\n $ harvest_year         : chr [1:1339] \"2014\" \"2014\" NA \"2014\" ...\n $ grading_date         : chr [1:1339] \"April 4th, 2015\" \"April 4th, 2015\" \"May 31st, 2010\" \"March 26th, 2015\" ...\n $ owner_1              : chr [1:1339] \"metad plc\" \"metad plc\" \"Grounds for Health Admin\" \"Yidnekachew Dabessa\" ...\n $ variety              : chr [1:1339] NA \"Other\" \"Bourbon\" NA ...\n $ processing_method    : chr [1:1339] \"Washed / Wet\" \"Washed / Wet\" NA \"Natural / Dry\" ...\n $ aroma                : num [1:1339] 8.67 8.75 8.42 8.17 8.25 8.58 8.42 8.25 8.67 8.08 ...\n $ flavor               : num [1:1339] 8.83 8.67 8.5 8.58 8.5 8.42 8.5 8.33 8.67 8.58 ...\n $ aftertaste           : num [1:1339] 8.67 8.5 8.42 8.42 8.25 8.42 8.33 8.5 8.58 8.5 ...\n $ acidity              : num [1:1339] 8.75 8.58 8.42 8.42 8.5 8.5 8.5 8.42 8.42 8.5 ...\n $ body                 : num [1:1339] 8.5 8.42 8.33 8.5 8.42 8.25 8.25 8.33 8.33 7.67 ...\n $ balance              : num [1:1339] 8.42 8.42 8.42 8.25 8.33 8.33 8.25 8.5 8.42 8.42 ...\n $ uniformity           : num [1:1339] 10 10 10 10 10 10 10 10 9.33 10 ...\n $ clean_cup            : num [1:1339] 10 10 10 10 10 10 10 10 10 10 ...\n $ sweetness            : num [1:1339] 10 10 10 10 10 10 10 9.33 9.33 10 ...\n $ cupper_points        : num [1:1339] 8.75 8.58 9.25 8.67 8.58 8.33 8.5 9 8.67 8.5 ...\n $ moisture             : num [1:1339] 0.12 0.12 0 0.11 0.12 0.11 0.11 0.03 0.03 0.1 ...\n $ category_one_defects : num [1:1339] 0 0 0 0 0 0 0 0 0 0 ...\n $ quakers              : num [1:1339] 0 0 0 0 0 0 0 0 0 0 ...\n $ color                : chr [1:1339] \"Green\" \"Green\" NA \"Green\" ...\n $ category_two_defects : num [1:1339] 0 1 0 2 2 1 0 0 0 4 ...\n $ expiration           : chr [1:1339] \"April 3rd, 2016\" \"April 3rd, 2016\" \"May 31st, 2011\" \"March 25th, 2016\" ...\n $ certification_body   : chr [1:1339] \"METAD Agricultural Development plc\" \"METAD Agricultural Development plc\" \"Specialty Coffee Association\" \"METAD Agricultural Development plc\" ...\n $ certification_address: chr [1:1339] \"309fcf77415a3661ae83e027f7e5f05dad786e44\" \"309fcf77415a3661ae83e027f7e5f05dad786e44\" \"36d0d00a3724338ba7937c52a378d085f2172daa\" \"309fcf77415a3661ae83e027f7e5f05dad786e44\" ...\n $ certification_contact: chr [1:1339] \"19fef5a731de2db57d16da10287413f5f99bc2dd\" \"19fef5a731de2db57d16da10287413f5f99bc2dd\" \"0878a7d4b9d35ddbf0fe2ce69a2062cceb45a660\" \"19fef5a731de2db57d16da10287413f5f99bc2dd\" ...\n $ unit_of_measurement  : chr [1:1339] \"m\" \"m\" \"m\" \"m\" ...\n $ altitude_low_meters  : num [1:1339] 1950 1950 1600 1800 1950 ...\n $ altitude_high_meters : num [1:1339] 2200 2200 1800 2200 2200 NA NA 1700 1700 1850 ...\n $ altitude_mean_meters : num [1:1339] 2075 2075 1700 2000 2075 ...\n\n\n\n\n\ncolnames(coffee_df)\n\n [1] \"total_cup_points\"      \"species\"               \"owner\"                \n [4] \"country_of_origin\"     \"farm_name\"             \"lot_number\"           \n [7] \"mill\"                  \"ico_number\"            \"company\"              \n[10] \"altitude\"              \"region\"                \"producer\"             \n[13] \"number_of_bags\"        \"bag_weight\"            \"in_country_partner\"   \n[16] \"harvest_year\"          \"grading_date\"          \"owner_1\"              \n[19] \"variety\"               \"processing_method\"     \"aroma\"                \n[22] \"flavor\"                \"aftertaste\"            \"acidity\"              \n[25] \"body\"                  \"balance\"               \"uniformity\"           \n[28] \"clean_cup\"             \"sweetness\"             \"cupper_points\"        \n[31] \"moisture\"              \"category_one_defects\"  \"quakers\"              \n[34] \"color\"                 \"category_two_defects\"  \"expiration\"           \n[37] \"certification_body\"    \"certification_address\" \"certification_contact\"\n[40] \"unit_of_measurement\"   \"altitude_low_meters\"   \"altitude_high_meters\" \n[43] \"altitude_mean_meters\""
  },
  {
    "objectID": "posts/06_correlation/06_correlation.html#covariance",
    "href": "posts/06_correlation/06_correlation.html#covariance",
    "title": "6: Correlation and Paradoxes",
    "section": "Covariance",
    "text": "Covariance\n\nFormulaIntuitionCommentary\n\n\nFor data \\((X,Y)\\) listed as \\(n\\) data points \\((x_{i}, y_{i})\\), the covariance is defined as\n\\[\\begin{array}{rcl}\n  \\text{Cov}(X,Y) & = & \\frac{1}{2n^{2}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}(x_{i} - x_{j})(y_{i} - y_{j}) \\\\\n  ~ & = & \\text{E}[X] \\cdot \\text{E}[Y] - \\text{E}[XY] \\\\\n  \\end{array}\\]\n\n\n\n\n\nconstructive or destructive waves\n\n\n\nimage source: Fissics\n\n\n\n\nAre resultant numbers large or small?\nUnits? (e.g. “hot-dog-fries”)"
  },
  {
    "objectID": "posts/06_correlation/06_correlation.html#standardization",
    "href": "posts/06_correlation/06_correlation.html#standardization",
    "title": "6: Correlation and Paradoxes",
    "section": "Standardization",
    "text": "Standardization\n\n\n\nz-score\n\\[\\begin{array}{ccc}\nz & = & \\frac{x - \\bar{x}}{s} \\\\\n~ & = & \\frac{\\text{deviation}}{\\text{standard deviation}} \\\\\n\\end{array}\\]\n\n\n\n\n\nCorrelation\n\\[\\begin{array}{ccc}\n  r & = & \\frac{\\sum_{i=1}^{n} (x_{i} - \\bar{x})(y_{i} - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_{i} - \\bar{x})}\\sqrt{\\sum_{i=1}^{n} (y_{i} - \\bar{y})}} \\\\\n  ~ & = & \\frac{\\text{Cov}(X,Y)}{\\text{SD}(X) \\cdot \\text{SD}(Y)} \\\\\n  ~ & = & \\frac{1}{n-1}\\sum_{i=1}^{n}\\left(\\frac{x_{i}-\\bar{x}}{s_{x}}\\right)\\left(\\frac{y_{i}-\\bar{y}}{s_{y}}\\right) \\\\\n\\end{array}\\]\n\n\n\n\n\n\n\n\n\nTipClaim: Range of Correlation\n\n\n\nThe correlation coefficient \\(r\\) has a mathematical range in \\([-1,1]\\):\n\\[-1 \\leq r \\leq 1\\]\n\n\n\n\n\n\n\n\nNote(optional) Outline of Proof\n\n\n\n\n\n\nShow that \\(\\text{Corr}(X,Y) \\geq -1\\) by starting with\n\n\\[0 \\leq \\text{Var}\\left( \\frac{X}{\\sqrt{\\text{Var}(X)}} + \\frac{Y}{\\sqrt{\\text{Var}(Y)}} \\right)\\]\n\nReplace \\(X\\) with \\(-X\\) in the previous part to show that \\(\\text{Corr}(X,Y) \\leq 1\\)\nCombine the above results"
  },
  {
    "objectID": "posts/06_correlation/06_correlation.html#demonstration",
    "href": "posts/06_correlation/06_correlation.html#demonstration",
    "title": "6: Correlation and Paradoxes",
    "section": "Demonstration",
    "text": "Demonstration\n\n12345\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningDCP 1"
  },
  {
    "objectID": "posts/06_correlation/06_correlation.html#what-is-piping",
    "href": "posts/06_correlation/06_correlation.html#what-is-piping",
    "title": "6: Correlation and Paradoxes",
    "section": "What is piping?",
    "text": "What is piping?\n\n\n\n\n\n\nWarningStep-by-Step\n\n\n\nEarly in learning computer programming, we could create a new data frame after each calculation, but\n\nprone to typos and other errors\nmore needs on computer memory\n\n\n\n\ncoffee_df2 &lt;- select(coffee_df, flavor, aftertaste)\ncoffee_df3 &lt;- mutate(coffee_df2, scale_x = scale(flavor))\ncoffee_df4 &lt;- mutate(coffee_df3, scale_y = scale(aftertaste))\ncovariance_ops &lt;- mutate(coffee_df4, products = scale_x * scale_y)\n\n\ncovariance_ops &lt;- coffee_df |&gt;\n  select(flavor, aftertaste) |&gt;\n  mutate(scale_x = scale(flavor)) |&gt;\n  mutate(scale_y = scale(aftertaste)) |&gt;\n  mutate(products = scale_x * scale_y)\n\n\n\n\n\n\n\nWarningPiping for Efficiency\n\n\n\nPiping in R allows us to code processes naturally\n\nas a chain of thought\nremoves temporary data frames afterward"
  },
  {
    "objectID": "posts/06_correlation/06_correlation.html#long-way",
    "href": "posts/06_correlation/06_correlation.html#long-way",
    "title": "6: Correlation and Paradoxes",
    "section": "Long Way",
    "text": "Long Way\n\n\n\n\n\n\nNoteLong Way Digressions\n\n\n\nThese “Long Way” digressions are here to show more R functions, reveal internal calculations, and/or preview future ideas.\n\n\n\nScatterplots\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- coffee_df |&gt;\n  filter(total_cup_points &gt; 0) |&gt; #avoids outliers\n  ggplot() +\n  geom_point(aes(x = flavor, y = aftertaste),\n             color = \"brown\") +\n  labs(title = \"Aftertaste vs Flavor\",\n       subtitle = \"Coffee Ratings Data\",\n       caption = \"Source: Tidy Tuesday\") +\n  theme_minimal()\n\np2 &lt;- coffee_df |&gt;\n  filter(total_cup_points &gt; 0) |&gt; #avoids outliers\n  mutate(scale_x = scale(flavor)) |&gt;\n  mutate(scale_y = scale(aftertaste)) |&gt;\n  ggplot() +\n  geom_point(aes(x = scale_x, y = scale_y),\n             color = \"brown\") +\n  labs(title = \"Aftertaste vs Flavor\",\n       subtitle = \"Standardized Data\",\n       caption = \"Source: Tidy Tuesday\") +\n  theme_minimal()\n\np1 + p2 #patchwork\n\n\n\n\n\ncovariance_ops &lt;- coffee_df |&gt;\n  select(flavor, aftertaste) |&gt;\n  mutate(scale_x = scale(flavor)) |&gt;\n  mutate(scale_y = scale(aftertaste)) |&gt;\n  mutate(products = scale_x * scale_y)\n\n\n\ngt\n\ntablecode\n\n\n\n\n\n\n\n\n\n\nCorrelation Internal Calculations\n\n\nRescaled Data and Products (first 10 rows shown)\n\n\nflavor\naftertaste\nscale_x\nscale_y\nproducts\n\n\n\n\n8.83\n8.67\n3.29\n3.14\n10.31\n\n\n8.67\n8.50\n2.89\n2.72\n7.84\n\n\n8.50\n8.42\n2.46\n2.52\n6.19\n\n\n8.58\n8.42\n2.66\n2.52\n6.70\n\n\n8.50\n8.25\n2.46\n2.10\n5.16\n\n\n8.42\n8.42\n2.26\n2.52\n5.69\n\n\n8.50\n8.33\n2.46\n2.30\n5.65\n\n\n8.33\n8.50\n2.03\n2.72\n5.52\n\n\n8.67\n8.58\n2.89\n2.91\n8.41\n\n\n8.58\n8.50\n2.66\n2.72\n7.23\n\n\n\nSource: Tidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\ncovariance_ops |&gt;\n  slice(1:10) |&gt; #for visual brevity\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  fmt_number(columns = everything(), decimals = 2) |&gt;\n  tab_footnote(footnote = \"Source: Tidy Tuesday\") |&gt;\n  tab_header(\n    title = \"Correlation Internal Calculations\",\n    subtitle = \"Rescaled Data and Products (first 10 rows shown)\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"lightblue1\")),\n    locations = cells_body(columns = \"scale_x\")\n  ) |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"indianred1\")),\n    locations = cells_body(columns = \"scale_y\")\n  ) |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"mediumorchid1\")),\n    locations = cells_body(columns = \"products\")\n  )\n\n\n\n\n\n\nFinish\n\\[r = \\frac{1}{n-1}\\sum_{i=1}^{n}\\left(\\frac{x_{i}-\\bar{x}}{s_{x}}\\right)\\left(\\frac{y_{i}-\\bar{y}}{s_{y}}\\right)\\]\n\n# sample size\n# (carefully calculated in case of missing values)\nn &lt;- sum(!is.na(coffee_df$flavor))\n\ncorr_val &lt;- sum(covariance_ops$products) / (n - 1)\nprint(round(corr_val, 4))\n\n[1] 0.8957\n\n\n\nThe data for flavor and aftertaste are strongly and positively correlated"
  },
  {
    "objectID": "posts/06_correlation/06_correlation.html#short-way",
    "href": "posts/06_correlation/06_correlation.html#short-way",
    "title": "6: Correlation and Paradoxes",
    "section": "Short Way",
    "text": "Short Way\n\n\n\n\n\n\nNoteShort Way Tools\n\n\n\nYou may use code in the “Short Way” in precepts and projects (unless otherwise instructed)\n\n\n\nCompute the correlation between flavor and aftertaste\n\n\ncor(coffee_df$flavor, coffee_df$aftertaste)\n\n[1] 0.8956718\n\n\n\nCompute the correlation between uniformity and clean_cup\n\n\n\n\n\n\n\nNoteMissing Values\n\n\n\nDerek recommends using use = \"pairwise.complete.obs\" to avoid missing values in the cor() calculation(s).\n\nSee: Precept 4\n\n\n\n\ncor(coffee_df$uniformity, coffee_df$clean_cup,\n    use = \"pairwise.complete.obs\")\n\n[1] 0.5262187\n\n\n\nCompute the correlation between aroma and sweetness\n\n\ncoffee_df |&gt;\n  summarize(r = cor(aroma, sweetness,\n                    use = \"pairwise.complete.obs\"))\n\n# A tibble: 1 × 1\n      r\n  &lt;dbl&gt;\n1 0.253"
  },
  {
    "objectID": "posts/06_correlation/06_correlation.html#simpsons-paradox",
    "href": "posts/06_correlation/06_correlation.html#simpsons-paradox",
    "title": "6: Correlation and Paradoxes",
    "section": "Simpson’s Paradox",
    "text": "Simpson’s Paradox\n\n\n\n\n\n\nNotedemonstration code\n\n\n\n\n\n\nx1 = rnorm(100, mean = -6, sd = 1)\ny1 = correlatedValues(x1, r = 0.75) + 6\nx2 = rnorm(100, mean = -3, sd = 1)\ny2 = correlatedValues(x2, r = 0.75) + 3\nx3 = rnorm(100, mean = 0, sd = 1)\ny3 = correlatedValues(x3, r = 0.75)\nx4 = rnorm(100, mean = 3, sd = 1)\ny4 = correlatedValues(x4, r = 0.75) - 3\nx5 = rnorm(100, mean = 6, sd = 1)\ny5 = correlatedValues(x5, r = 0.75) - 6\n\ndf1 &lt;- data.frame(x1, y1, \"group 1\")\ndf2 &lt;- data.frame(x2, y2, \"group 2\")\ndf3 &lt;- data.frame(x3, y3, \"group 3\")\ndf4 &lt;- data.frame(x4, y4, \"group 4\")\ndf5 &lt;- data.frame(x5, y5, \"group 5\")\nnames(df1) &lt;- c(\"xdata\", \"ydata\", \"group\")\nnames(df2) &lt;- c(\"xdata\", \"ydata\", \"group\")\nnames(df3) &lt;- c(\"xdata\", \"ydata\", \"group\")\nnames(df4) &lt;- c(\"xdata\", \"ydata\", \"group\")\nnames(df5) &lt;- c(\"xdata\", \"ydata\", \"group\")\ndemo_df &lt;- rbind(df1, df2, df3, df4, df5)\n\n\n\n\nNow we will visualize the data in demo_df\n\ndemo_df |&gt;\nggplot(aes(x = xdata, y = ydata)) +\n  geom_point() +\n  labs(title = \"all together\")\n\n\n\n\n\n\n\n# compute correlation\ndemo_df |&gt;\n  summarize(cor = cor(xdata, ydata))\n\n         cor\n1 -0.6190699\n\n\nNow let us treat the groups separately.\n\ndemo_df |&gt;\nggplot(aes(x = xdata, y = ydata, color = group)) +\n  geom_point() +\n  labs(title = \"separate groups\")\n\n\n\n\n\n\n\n# compute correlation\ndemo_df |&gt;\n  group_by(group) |&gt;\n  summarize(cor = cor(xdata, ydata))\n\n# A tibble: 5 × 2\n  group     cor\n  &lt;chr&gt;   &lt;dbl&gt;\n1 group 1 0.783\n2 group 2 0.701\n3 group 3 0.825\n4 group 4 0.673\n5 group 5 0.700"
  },
  {
    "objectID": "posts/06_correlation/06_correlation.html#berksons-paradox",
    "href": "posts/06_correlation/06_correlation.html#berksons-paradox",
    "title": "6: Correlation and Paradoxes",
    "section": "Berkson’s Paradox",
    "text": "Berkson’s Paradox\n\nCollege Admission Stats\n\noverallsubsetbiascode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx1 = rnorm(1000, mean = 0, sd = 1)\ny1 = correlatedValues(x1, r = 0.75)\ngpa_hs &lt;- 0.5*x1 + 3\nsat_hs &lt;- 200*y1 + 1200\ndata_hs &lt;- data.frame(gpa_hs, sat_hs) |&gt;\n  filter(gpa_hs &lt;= 4.0) |&gt;\n  filter(sat_hs &lt;= 1600)\ndata_ivy &lt;- data_hs |&gt;\n  filter(gpa_hs &gt; 3.6) |&gt;\n  filter(sat_hs &gt; 1400)\n\ncorr_val_before &lt;- cor(data_hs$gpa_hs, data_hs$sat_hs)\ncorr_val_after &lt;- cor(data_ivy$gpa_hs, data_ivy$sat_hs)\n\np1 &lt;- data_hs |&gt;\n  ggplot() + \n  geom_point(aes(x = gpa_hs, y = sat_hs),\n             color = \"gray75\") +\n  labs(title = \"High School Statistics\",\n       subtitle = paste0(\"correlation: r = \", round(corr_val_before, 4)),\n       caption = \"SML 201\",\n       x = \"High School GPA\", \n       y = \"SAT Score\") +\n  theme_minimal()\n\np2 &lt;- data_hs |&gt;\n  ggplot() + \n  geom_point(aes(x = gpa_hs, y = sat_hs),\n             color = \"gray75\") +\n  geom_point(aes(x = gpa_hs, y = sat_hs),\n             color = \"#FF671F\",\n             data = data_ivy,\n             size = 3) +\n  geom_vline(xintercept = 3.6, color = \"black\",\n             linewidth = 2, linetype = 2) +\n  geom_hline(yintercept = 1400, color = \"black\", \n             linewidth = 2, linetype = 2) +\n  labs(title = \"Selection Bias\",\n       subtitle = \"(subsetting)\",\n       caption = \"SML 201\",\n       x = \"High School GPA\", \n       y = \"SAT Score\") +\n  theme_minimal()\n\np3 &lt;- data_ivy |&gt;\n  ggplot() + \n  geom_point(aes(x = gpa_hs, y = sat_hs),\n             color = \"#FF671F\",\n             size = 3) +\n  labs(title = \"Princeton Students\",\n       subtitle = paste0(\"correlation: r = \", round(corr_val_after, 4)),\n       caption = \"SML 201\",\n       x = \"High School GPA\", \n       y = \"SAT Score\") +\n  theme_minimal()\n\n\n\n\n\n\nRizz\n\ndating poolsubset 1subset 2\n\n\n\n\n\ndating pool\n\n\n\nimage source: Cristóbal Alcázar\n\n\n\n\n\n\nsubset 1\n\n\n\nimage source: Cristóbal Alcázar\n\n\n\n\n\n\nsubset 2\n\n\n\n\n\n\n\n\nWarningThis is why everyone you date is a jerk!\n\n\n\n\n\n\n\nimage source: Cristóbal Alcázar\nas mentioned in a book by Carl T Bergstrom and Jevin D West"
  },
  {
    "objectID": "posts/06_correlation/06_correlation.html#categorical-group",
    "href": "posts/06_correlation/06_correlation.html#categorical-group",
    "title": "6: Correlation and Paradoxes",
    "section": "Categorical Group",
    "text": "Categorical Group\nThe dplyr code is easily adaptable to grouped data.\n\ncoffee_df |&gt;\n  group_by(species) |&gt;\n  summarize(min = min(total_cup_points, na.rm = TRUE),\n            q25 = quantile(total_cup_points, 0.25, na.rm = TRUE),\n            q50 = quantile(total_cup_points, 0.50, na.rm = TRUE),\n            q75 = quantile(total_cup_points, 0.75, na.rm = TRUE),\n            max = max(total_cup_points, na.rm = TRUE))\n\n# A tibble: 2 × 6\n  species   min   q25   q50   q75   max\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Arabica   0    81.2  82.5  83.7  90.6\n2 Robusta  73.8  80.2  81.5  82.5  83.8"
  },
  {
    "objectID": "posts/06_correlation/06_correlation.html#histograms",
    "href": "posts/06_correlation/06_correlation.html#histograms",
    "title": "6: Correlation and Paradoxes",
    "section": "Histograms",
    "text": "Histograms\n\nplotscode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\narabica &lt;- coffee_df |&gt;\n  filter(total_cup_points &gt; 0) |&gt;\n  filter(species == \"Arabica\")\n\nrobusta &lt;- coffee_df |&gt;\n  filter(total_cup_points &gt; 0) |&gt;\n  filter(species == \"Robusta\")\n\nq25 &lt;- quantile(arabica$total_cup_points, 0.25, na.rm = TRUE)\nq50 &lt;- quantile(arabica$total_cup_points, 0.50, na.rm = TRUE)\nq75 &lt;- quantile(arabica$total_cup_points, 0.75, na.rm = TRUE)\nIQR &lt;- q75 - q25\nbottom &lt;- q25 - 1.5*IQR\ntop    &lt;- q75 + 1.5*IQR\n\n# points of interest\nPOI &lt;- c(bottom, q25, q50, q75, top)\n\np1 &lt;- arabica |&gt;\n  ggplot() +\n  geom_histogram(aes(x = total_cup_points),\n                 bins = 25,\n                 color = \"black\",\n                 fill = \"gray80\") +\n  geom_vline(xintercept = POI, color = \"red\",\n             linewidth = 2) +\n  labs(title = \"Arabica Coffee\",\n       subtitle = \"Distribution of Total Cup Points\",\n       caption = \"Source: Tidy Tuesday\") +\n  theme_minimal() +\n  xlim(55, 95)\n\nq25 &lt;- quantile(robusta$total_cup_points, 0.25, na.rm = TRUE)\nq50 &lt;- quantile(robusta$total_cup_points, 0.50, na.rm = TRUE)\nq75 &lt;- quantile(robusta$total_cup_points, 0.75, na.rm = TRUE)\nIQR &lt;- q75 - q25\nbottom &lt;- q25 - 1.5*IQR\ntop    &lt;- q75 + 1.5*IQR\n\n# points of interest\nPOI &lt;- c(bottom, q25, q50, q75, top)\n\np2 &lt;- robusta |&gt;\n  ggplot() +\n  geom_histogram(aes(x = total_cup_points),\n                 bins = 25,\n                 color = \"black\",\n                 fill = \"gray80\") +\n  geom_vline(xintercept = POI, color = \"blue\",\n             linewidth = 2) +\n  labs(title = \"Robusta Coffee\",\n       subtitle = \"Distribution of Total Cup Points\",\n       caption = \"Source: Tidy Tuesday\") +\n  theme_minimal() +\n  xlim(55, 95)\n\n#| echo: false\n#| eval: true\n\np1 / p2 #patchwork"
  },
  {
    "objectID": "posts/06_correlation/06_correlation.html#more-than-two-groups",
    "href": "posts/06_correlation/06_correlation.html#more-than-two-groups",
    "title": "6: Correlation and Paradoxes",
    "section": "More than Two Groups",
    "text": "More than Two Groups\n\n12Boxplot\n\n\n\ncoffee_df |&gt;\n  filter(!is.na(processing_method)) |&gt;\n  group_by(processing_method) |&gt;\n  summarize(min = min(total_cup_points, na.rm = TRUE),\n            q25 = quantile(total_cup_points, 0.25, na.rm = TRUE),\n            q50 = quantile(total_cup_points, 0.50, na.rm = TRUE),\n            q75 = quantile(total_cup_points, 0.75, na.rm = TRUE),\n            max = max(total_cup_points, na.rm = TRUE))\n\n# A tibble: 5 × 6\n  processing_method           min   q25   q50   q75   max\n  &lt;chr&gt;                     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Natural / Dry              67.9  81.2  82.8  83.8  89  \n2 Other                      63.1  80.8  81.8  83.0  84.7\n3 Pulped natural / honey     80.1  82.0  82.7  83.2  86.6\n4 Semi-washed / Semi-pulped  78.8  81.5  82.5  83.6  86.1\n5 Washed / Wet               59.8  81    82.4  83.5  90.6\n\n\n\n\n\ncoffee_df |&gt;\n  filter(!is.na(processing_method)) |&gt;\n  group_by(processing_method) |&gt;\n  mutate(min_val = min(total_cup_points, na.rm = TRUE),\n         q25 = quantile(total_cup_points, 0.25, na.rm = TRUE),\n         q50 = quantile(total_cup_points, 0.50, na.rm = TRUE),\n         q75 = quantile(total_cup_points, 0.75, na.rm = TRUE),\n         max_val = max(total_cup_points, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(processing_method, min_val, q25, q50, q75, max_val) |&gt;\n  distinct()\n\n# A tibble: 5 × 6\n  processing_method         min_val   q25   q50   q75 max_val\n  &lt;chr&gt;                       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Washed / Wet                 59.8  81    82.4  83.5    90.6\n2 Natural / Dry                67.9  81.2  82.8  83.8    89  \n3 Pulped natural / honey       80.1  82.0  82.7  83.2    86.6\n4 Semi-washed / Semi-pulped    78.8  81.5  82.5  83.6    86.1\n5 Other                        63.1  80.8  81.8  83.0    84.7\n\n\n\n\n\ncoffee_df |&gt;\n  filter(!is.na(processing_method)) |&gt;\n  ggplot() +\n  geom_boxplot(aes(x = processing_method, y = total_cup_points,\n                   color = processing_method)) +\n  labs(title = \"Coffee Ratings\",\n       subtitle = \"Are these quantities different?\",\n       caption = \"Source: Coffee Quality Database\",\n       x = \"\", y = \"total points\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningDCP 3"
  },
  {
    "objectID": "posts/06_correlation/implicit_bias/implicit_wrangler.html",
    "href": "posts/06_correlation/implicit_bias/implicit_wrangler.html",
    "title": "implicit wrangler",
    "section": "",
    "text": "library(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nimplicit_raw &lt;- readr::read_csv(\"DCP0129 Survey Student Analysis Report.csv\")\n\nNew names:\nRows: 154 Columns: 33\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(7): name, section, section_id, section_sis_id, submitted, 165273: Prom... dbl\n(24): id, sis_id, attempt, 0.0...10, 0...12, 165274: Prompt 2-1, 1.0...1... lgl\n(2): 165489: Prompt 2 refers to the 7 graphs respectively at this set o...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `0.0` -&gt; `0.0...10`\n• `0` -&gt; `0...12`\n• `1.0` -&gt; `1.0...14`\n• `1.0` -&gt; `1.0...16`\n• `1.0` -&gt; `1.0...18`\n• `1.0` -&gt; `1.0...20`\n• `1.0` -&gt; `1.0...22`\n• `1.0` -&gt; `1.0...24`\n• `1.0` -&gt; `1.0...26`\n• `0.0` -&gt; `0.0...28`\n• `0` -&gt; `0...30`\n\n\n\nimplicit_df &lt;- implicit_raw |&gt;\n  select(seq(13, 25, 2)) |&gt;\n  set_names(paste0(\"graph\", 1:7)) |&gt;\n  mutate(id = 1:n()) |&gt;\n  mutate(section = ifelse(id &lt;= 73, \"L02\", \"L01\")) |&gt;\n  filter(id != 3)\n\n\nreadr::write_csv(implicit_df, \"implicit_bias_data.csv\")"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html",
    "href": "posts/07_linear_regression/07_linear_regression.html",
    "title": "7: Linear Regression",
    "section": "",
    "text": "library(\"gt\")        #great tables\nlibrary(\"HistData\")  #historical data sets\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\nliquor_df &lt;- data.frame(\n  year = 2014:2023,\n  LLV = c(129, 54, 103, 50, 15, 10, 18, 49, 57, 262)\n)\nMM_df &lt;- data.frame(\n  S = c(0.08, 0.12, 0.54, 1.23, 1.82, 2.72, 4.94, 10.00),\n  v = c(0.15, 0.21, 0.7, 1.1, 1.3, 1.5, 1.7, 1.8)\n)"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#start",
    "href": "posts/07_linear_regression/07_linear_regression.html#start",
    "title": "7: Linear Regression",
    "section": "Start",
    "text": "Start\n\n\n\nGoal: Make predictions\nObjective: Perform linear regression and compute coefficients of determination\n\n\n\n\n\n\n\nlinear regression\n\n\n\nimage source: XKCD"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#linear-regression-in-r",
    "href": "posts/07_linear_regression/07_linear_regression.html#linear-regression-in-r",
    "title": "7: Linear Regression",
    "section": "Linear Regression in R",
    "text": "Linear Regression in R\n\nresponse variable (y): LLV\nexplanatory variable (x): year\n\n\nlin_fit &lt;- lm(LLV ~ year, data = liquor_df)\n\nIn a model equation, the tilde ~ is read as “explained by”. In this model, we can say that the response variable LLV is explained by the year."
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#prediction-in-r",
    "href": "posts/07_linear_regression/07_linear_regression.html#prediction-in-r",
    "title": "7: Linear Regression",
    "section": "Prediction in R",
    "text": "Prediction in R\nWe use the predict function where the input is a data frame. In this example, we are predicting the number of judicial referrals for liquor law violations in the year 2024.\n\nyhat &lt;- predict(lin_fit,\n                newdata = data.frame(year = 2024))"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#validation",
    "href": "posts/07_linear_regression/07_linear_regression.html#validation",
    "title": "7: Linear Regression",
    "section": "Validation",
    "text": "Validation\nIn this simple example, we know the true answer: there were 218 judicial referrals for liquor law violations in the year 2024\n\n# true value\ny &lt;- 218\n\n\n# prediction\nprint(yhat)\n\n\n\n102.9333\n\n\n\n# absolute error\nabs(y - yhat)\n\n\n\n115.0667\n\n\n\n# relative error\nabs(y - yhat) / y\n\n\n\n0.5278287\n\n\n\n\n\n\n\n\nWarningDCP1"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#scatterplot",
    "href": "posts/07_linear_regression/07_linear_regression.html#scatterplot",
    "title": "7: Linear Regression",
    "section": "Scatterplot",
    "text": "Scatterplot\nIt is a good idea to look at the data (when practical).\n\nliquor_df |&gt;\n  ggplot() +\n  geom_point(aes(x = factor(year), y = LLV),\n             size = 4, color = \"black\") +\n  labs(title = \"Liquor Law Violations\",\n       subtitle = \"Princeton University, main campus\",\n       caption = \"SML 201\",\n       x = \"year\", y = \"judicial referrals\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#smooth",
    "href": "posts/07_linear_regression/07_linear_regression.html#smooth",
    "title": "7: Linear Regression",
    "section": "Smooth",
    "text": "Smooth\nOn the visual size, linear regression summarizes a scatterplot.\n\nliquor_df |&gt;\n  ggplot(aes(x = factor(year), y = LLV)) +\n  geom_point(size = 4, color = \"black\") +\n  geom_smooth(formula = \"y ~ x\", method = \"lm\", se = FALSE,\n              color = \"blue\", linewidth = 2) +\n  labs(title = \"Liquor Law Violations\",\n       subtitle = \"Princeton University, main campus\",\n       caption = \"SML 201\",\n       x = \"year\", y = \"judicial referrals\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#corollary-centroid",
    "href": "posts/07_linear_regression/07_linear_regression.html#corollary-centroid",
    "title": "7: Linear Regression",
    "section": "Corollary: Centroid",
    "text": "Corollary: Centroid\nClaim: a linear regression model goes through the centroid\n\\[(\\bar{x}, \\bar{y})\\]\n\nxbar &lt;- mean(liquor_df$year)\nybar &lt;- mean(liquor_df$LLV)\n\nliquor_df |&gt;\n  ggplot(aes(x = year, y = LLV)) +\n  geom_point(size = 4, color = \"black\") +\n  geom_vline(xintercept = xbar, color = \"green\") +\n  geom_hline(yintercept = ybar, color = \"green\") +\n  geom_smooth(formula = \"y ~ x\", method = \"lm\", se = FALSE,\n              color = \"blue\", linewidth = 2) +\n  labs(title = \"Linear Regression\",\n       subtitle = \"A linear regression model goes through the centroid \",\n       caption = \"SML 201\",\n       x = \"year\", y = \"judicial referrals\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#vector-space",
    "href": "posts/07_linear_regression/07_linear_regression.html#vector-space",
    "title": "7: Linear Regression",
    "section": "Vector Space",
    "text": "Vector Space\nHow was the line drawn?\n\n\n\nWhere to draw the line?"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#formulas",
    "href": "posts/07_linear_regression/07_linear_regression.html#formulas",
    "title": "7: Linear Regression",
    "section": "Formulas",
    "text": "Formulas\n\n\n\n\n\n\nNoteSimple linear regression\n\n\n\n\\[\\hat{y} = a + bx\\]\nFor simple linear regression (today’s materials, before more complicated models), the intercept and slope can be written in terms of previously studied sample statistics:\n\\[b = r \\cdot \\frac{s_{y}}{s_{x}}, \\quad a = \\bar{y} - b*\\bar{x}\\]\n\n\n\n\n\n\n\n\nTipConcept Map\n\n\n\nWith slope \\(b\\) defined as\n\\[b = r \\cdot \\frac{s_{y}}{s_{x}}\\]\nin simple linear regression, and recalling that\n\\[s_{x} &gt; 0 \\quad\\text{and}\\quad s_{y} &gt; 0\\]\nfor non-trivial examples (i.e. data is not constant), it follows that\n\nslope of regression line is positive iff correlation is positive\nslope of regression line is negative iff correlation is negative\n\n\n\n\n\n\n\n\n\nTipCorollary: Centroid\n\n\n\n\n\nClaim: a linear regression model goes through the centroid\n\\[(\\bar{x}, \\bar{y})\\]\nProof:\nThis proof is easy if we’re allowed to start with these formulas\n\\[y = a + bx, \\quad a = \\bar{y} - b*\\bar{x}\\]\nWe then verify that \\((x,y) = (\\bar{x}, \\bar{y})\\) is a valid location in this model.\n\\[\\begin{array}{rcl}\n  y & = & a + bx \\\\\n  \\bar{y} & = & a + b\\bar{x} \\\\\n  \\bar{y} & = & \\bar{y} - b*\\bar{x} + b\\bar{x} \\\\\n  \\bar{y} & = & \\bar{y}\n\\end{array}\\]"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#prediction",
    "href": "posts/07_linear_regression/07_linear_regression.html#prediction",
    "title": "7: Linear Regression",
    "section": "Prediction",
    "text": "Prediction\nPredict the number of judicial referrals for liquor law violations in the year 2024.\n\n\n\n\n\n\nTipWrite a math expression …\n\n\n\n\n\n-10286.933 + (5.133)x\n\n\n\n\n\n# mathematically\na &lt;- summary(lin_fit)$coefficients[1]\nb &lt;- summary(lin_fit)$coefficients[2]\n\na + b*(2024)\n\n\n\n102.9333\n\n\n\n# properly programmatically\npredict(lin_fit, newdata = data.frame(year = 2024))\n\n\n\n102.9333"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#inference",
    "href": "posts/07_linear_regression/07_linear_regression.html#inference",
    "title": "7: Linear Regression",
    "section": "Inference",
    "text": "Inference\nWhy is that denoted “\\(R^2\\)”? For linear regression, the coefficient of determination is literally the square of the correlation coefficient (\\(r\\))\n\ncorrelation \\(-1 \\leq r \\leq 1\\) implies coefficient of determination \\[0 \\leq R^{2} \\leq 1\\]\nwant more “explained variation”, thus higher \\(R^{2}\\) means a better model"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#guidelines",
    "href": "posts/07_linear_regression/07_linear_regression.html#guidelines",
    "title": "7: Linear Regression",
    "section": "Guidelines",
    "text": "Guidelines\n\n\nIn this course, we will simply follow the Pearson suggestions for interpreting coefficient of determination values:\n\n\\(0 \\leq R^{2} &lt; 0.4\\): poor model\n\\(0.4 \\leq R^{2} &lt; 0.7\\): good model\n\\(0.7 \\leq R^{2} \\leq 1.0\\): great model\n\n\n\n\n\n\n\nKarl Pearson"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#model-statistics",
    "href": "posts/07_linear_regression/07_linear_regression.html#model-statistics",
    "title": "7: Linear Regression",
    "section": "Model Statistics",
    "text": "Model Statistics\nIn R, we can use summary to access model statistics.\n\nsummary(lin_fit)\n\n\nCall:\nlm(formula = LLV ~ year, data = liquor_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-67.27 -52.48 -26.33  30.17 164.20 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -10286.933  17504.409  -0.588    0.573\nyear             5.133      8.672   0.592    0.570\n\nResidual standard error: 78.77 on 8 degrees of freedom\nMultiple R-squared:  0.04196,   Adjusted R-squared:  -0.07779 \nF-statistic: 0.3504 on 1 and 8 DF,  p-value: 0.5702"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#multiple-r-squared",
    "href": "posts/07_linear_regression/07_linear_regression.html#multiple-r-squared",
    "title": "7: Linear Regression",
    "section": "Multiple R-Squared",
    "text": "Multiple R-Squared\n\n\n\n\n\n\nNoteSum of Squares\n\n\n\nThe coefficient of determination can be a measure of how much better model2 is than model1 (later: model selection).\n\\[R^{2} = \\frac{\\text{SSR}(\\text{model1}) - \\text{SSR}(\\text{model2})}{\\text{SSR}(\\text{model1})}\\]\n\n\n\n\n\n\n\n\nTipBaseline\n\n\n\nIn this example (liquor law violations), we could treat \\(\\bar{y}\\), the average of the output data, as a baseline and see if the linear regression model leads to better predictions.\n\\[R^{2} = \\frac{\\text{SSR}(\\text{mean}) - \\text{SSR}(\\text{line})}{\\text{SSR}(\\text{mean})}\\]\n\n\n\nCalculationcode\n\n\n\\[\\begin{array}{rcccl}\n  \\text{SSR}(\\text{mean}) & = & \\sum_{i=0}^{n}\\left(y - \\bar{y}\\right)^{2} & \\approx & 51808 \\\\\n  \\text{SSR}(\\text{line}) & = & \\sum_{i=0}^{n}\\left(y - \\hat{y}\\right)^{2} & \\approx & 49634 \\\\\n  R^{2} & = & \\frac{\\text{SSR}(\\text{mean}) - \\text{SSR}(\\text{line})}{\\text{SSR}(\\text{mean})} & \\approx & 0.04196\n\\end{array}\\]\n\n\n\nyhat &lt;- mean(liquor_df$LLV, na.rm = TRUE)\ndeviations &lt;- liquor_df$LLV - yhat\n\nlin_fit &lt;- lm(LLV ~ year, data = liquor_df)\npredictions &lt;- predict(lin_fit,\n                      newdata = data.frame(year = liquor_df$year))\nresiduals &lt;- liquor_df$LLV - predictions\n\nSSR_mean &lt;- sum(deviations * deviations)\nSSR_line &lt;- sum(residuals * residuals)\n\nR_squared &lt;- (SSR_mean - SSR_line) / SSR_mean\ncat(round(R_squared, 5))\n\n0.04196\n\n\n\n\n\n\n\n\n\n\n\nWarningInterpreting Multiple R-Squared\n\n\n\nIn this example, with the small value\n\\[R^{2} \\approx 0.04196\\]\nthe linear regression model was barely better than simply reporting the average output value."
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#adjusted-r-squared",
    "href": "posts/07_linear_regression/07_linear_regression.html#adjusted-r-squared",
    "title": "7: Linear Regression",
    "section": "Adjusted R-Squared",
    "text": "Adjusted R-Squared\n\n\n\n\n\n\nNoteAdjusting for Degrees of Freedom\n\n\n\nHere in SML 201, we will use the “Adjusted R-squared” value that accounts for the number of predictor variables (later: degrees of freedom), and then look for the highest adjusted R-squared values.\n\n\n\nsummary(lin_fit)$adj.r.squared #negative result??\n\n[1] -0.07779285\n\n\n\n\n\n\n\n\nWarningInterpreting Adjusted R-Squared\n\n\n\nIn this example, with the negative value\n\\[R^{2} \\approx -0.07779\\]\nthe linear regression model was worse than simply reporting the average output value."
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#francis-galton",
    "href": "posts/07_linear_regression/07_linear_regression.html#francis-galton",
    "title": "7: Linear Regression",
    "section": "Francis Galton",
    "text": "Francis Galton\n\n\n\n\n\nSir Francis Galton\n\n\n\n\n\n\n1822 - 1911\ncousin of Charles Darwin\ncorrelation discovery\n\n1846: August Bravais\n1888: Francis Galton"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#do-tall-parents-have-tall-children",
    "href": "posts/07_linear_regression/07_linear_regression.html#do-tall-parents-have-tall-children",
    "title": "7: Linear Regression",
    "section": "Do tall parents have tall children?",
    "text": "Do tall parents have tall children?\nThe Galton data set in the HistData package has two variables\n\nparents’ height (see documentation for weighted formula)\nchild’s height\n\nfor about 200 families\n\nheredity_df &lt;- HistData::Galton"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#scatterplot-1",
    "href": "posts/07_linear_regression/07_linear_regression.html#scatterplot-1",
    "title": "7: Linear Regression",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nlin_fit &lt;- lm(child ~ parent, data = heredity_df)\na &lt;- summary(lin_fit)$coefficients[1]\nb &lt;- summary(lin_fit)$coefficients[2]\n\nheredity_df |&gt;\n  ggplot(aes(x = parent, y = child)) +\n  geom_point() +\n  geom_smooth(formula = \"y ~ x\", method = \"lm\", se = FALSE) +\n  labs(title = \"Do tall parents have tall children?\",\n       subtitle = paste0(\"y = \", round(a, 4), \" + \", round(b, 4), \"x\"),\n       caption = \"Source: Galton Survey of Heights\",\n       x = \"parent's heights (weighted average)\",\n       y = \"child's height\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#linear-model",
    "href": "posts/07_linear_regression/07_linear_regression.html#linear-model",
    "title": "7: Linear Regression",
    "section": "Linear Model",
    "text": "Linear Model\n\nlin_fit &lt;- lm(child ~ parent, data = heredity_df)\n\nprint(lin_fit)\n\n\nCall:\nlm(formula = child ~ parent, data = heredity_df)\n\nCoefficients:\n(Intercept)       parent  \n    23.9415       0.6463  \n\n\nFor every one inch increase in parents’ height, the child’s height increases by about 0.65 inches."
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#determination-1",
    "href": "posts/07_linear_regression/07_linear_regression.html#determination-1",
    "title": "7: Linear Regression",
    "section": "Determination",
    "text": "Determination\n\nsummary(lin_fit)$adj.r.squared #still a bad model\n\n[1] 0.2096103"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#prediction-1",
    "href": "posts/07_linear_regression/07_linear_regression.html#prediction-1",
    "title": "7: Linear Regression",
    "section": "Prediction",
    "text": "Prediction\nIf the parents are 70 inches in height, what do we predict for the height of the child?\n\npredict(lin_fit, newdata = data.frame(parent = 70))\n\n       1 \n69.18187 \n\n\nIf the parents are 58 inches in height, what do we predict for the height of the child?\n\npredict(lin_fit, newdata = data.frame(parent = 58))\n\n       1 \n61.42638"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#legacy",
    "href": "posts/07_linear_regression/07_linear_regression.html#legacy",
    "title": "7: Linear Regression",
    "section": "Legacy",
    "text": "Legacy\n\n\n\n\n\n\nWarningRegression to the Mean\n\n\n\nIn these early studies of heredity, Galton coined the phrase\n\\[\\text{regression to the mean}\\]\nand similar calculations have been called regression ever since.\n\n\n\n\n\n\n\n\nWarningDCP3"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#michaelis-and-menton",
    "href": "posts/07_linear_regression/07_linear_regression.html#michaelis-and-menton",
    "title": "7: Linear Regression",
    "section": "Michaelis and Menton",
    "text": "Michaelis and Menton\n\n\n\n\n\nLeonor Michaelis and Maud Menton\n\n\n\n\n\nLeonor Michaelis\n\nBerlin University (1897)\n\nMaud Menton\n\nUniversity of Toronto (1911)\n\nDie Kinetik der Invertinwirkung (1913)\n\\[v = \\frac{V_{\\text{max}}[S]}{K_{m} + [S]}\\]\n\n\\(v\\): reaction rate (micromolars per minute)\n\\([S]\\): substrate concentration (micromolars)"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#enzyme-kinetics",
    "href": "posts/07_linear_regression/07_linear_regression.html#enzyme-kinetics",
    "title": "7: Linear Regression",
    "section": "Enzyme Kinetics",
    "text": "Enzyme Kinetics\n\n\n\n\n\nEssential Cell Biology\n\n\n\n\n\nThe reaction rates of the reaction S \\(\\rightarrow\\) P catalyzed by enzyme E were determined under conditions such that only very little product was formed. Compute the maximum reaction velocity asymptote \\(V_{\\text{max}}\\) and the Michaelis-Menton constant \\(K_{m}\\)\n\n\n\n\n\n\n\n\nMichaelis Menton Experiment\n\n\nreaction rate vs substrate concentration\n\n\nS\nv\n\n\n\n\n0.08\n0.15\n\n\n0.12\n0.21\n\n\n0.54\n0.70\n\n\n1.23\n1.10\n\n\n1.82\n1.30\n\n\n2.72\n1.50\n\n\n4.94\n1.70\n\n\n10.00\n1.80\n\n\n\nDie Kinetik der Invertinwirkung"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#scatterplot-2",
    "href": "posts/07_linear_regression/07_linear_regression.html#scatterplot-2",
    "title": "7: Linear Regression",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nreaction rate (lower case v)\nsubstrate concentration (capital S)\n\n\nMM_df |&gt;\n  ggplot(aes(x = S, y = v)) +\n  geom_smooth(formula = \"y ~ x\", method = \"loess\", se = TRUE) +\n  geom_point(color = \"black\", size = 4) +\n  labs(title = \"Michaelis Menton Enzyme Kinetics Experiment\",\n       subtitle = \"reaction rate vs substrate concentration\",\n       caption = \"Essential Cell Biology\",\n       x = \"substrate concentration (micromolars)\",\n       y = \"reaction rate (micromolars per minute)\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#transformation",
    "href": "posts/07_linear_regression/07_linear_regression.html#transformation",
    "title": "7: Linear Regression",
    "section": "Transformation",
    "text": "Transformation\nHans Lineweaver (George Washington Univ., 1934)\n\\[v = \\frac{V_{\\text{max}}[S]}{K_{m} + [S]} \\quad\\rightarrow\\quad \\frac{1}{v} = \\frac{K_{m}}{V_{\\text{max}}} \\cdot \\frac{1}{[S]} + \\frac{1}{V_{\\text{max}}}\\]\nThe reciprocal of the reaction rate is linear with respect to the reciprocal of the substrate concentration."
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#double-reciprocal-plot",
    "href": "posts/07_linear_regression/07_linear_regression.html#double-reciprocal-plot",
    "title": "7: Linear Regression",
    "section": "Double-Reciprocal Plot",
    "text": "Double-Reciprocal Plot\n\nMM_df &lt;- MM_df |&gt;\n  mutate(rS = 1/S,\n         rv = 1/v)\n\n\nMM_df |&gt;\n  ggplot(aes(x = rS, y = rv)) +\n  geom_smooth(formula = \"y ~ x\", method = \"lm\", se = FALSE) +\n  geom_point(color = \"black\", size = 4) +\n  labs(title = \"Michaelis Menton Enzyme Kinetics Experiment\",\n       subtitle = \"Double Reciprocal Plot\",\n       caption = \"Essential Cell Biology\",\n       x = \"1/[S] (1/micromoles)\",\n       y = \"1/v (minutes per micromolar)\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#linear-model-1",
    "href": "posts/07_linear_regression/07_linear_regression.html#linear-model-1",
    "title": "7: Linear Regression",
    "section": "Linear Model",
    "text": "Linear Model\n\nlin_fit &lt;- lm(rv ~ rS, data = MM_df)"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#determination-2",
    "href": "posts/07_linear_regression/07_linear_regression.html#determination-2",
    "title": "7: Linear Regression",
    "section": "Determination",
    "text": "Determination\n\nsummary(lin_fit)$adj.r.squared #great model!\n\n[1] 0.9995051"
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#extracting-the-constants",
    "href": "posts/07_linear_regression/07_linear_regression.html#extracting-the-constants",
    "title": "7: Linear Regression",
    "section": "Extracting the Constants",
    "text": "Extracting the Constants\n\nslope &lt;- lin_fit$coefficients[2]\nintercept &lt;- lin_fit$coefficients[1]\n\nVmax &lt;- 1/intercept\nKm &lt;- slope * Vmax\n\nprint(paste(\"The Vmax asymptote is \", Vmax))\n\n[1] \"The Vmax asymptote is  1.9894653306508\"\n\nprint(paste(\"The Michaelis-Menton constant is \", Km))\n\n[1] \"The Michaelis-Menton constant is  0.991986524114476\""
  },
  {
    "objectID": "posts/07_linear_regression/07_linear_regression.html#mm-experiment-revisited",
    "href": "posts/07_linear_regression/07_linear_regression.html#mm-experiment-revisited",
    "title": "7: Linear Regression",
    "section": "MM Experiment Revisited",
    "text": "MM Experiment Revisited\n\nMM_df |&gt;\n  ggplot(aes(x = S, y = v)) +\n  geom_smooth(formula = \"y ~ x\", method = \"loess\", se = TRUE) +\n  geom_point(color = \"black\", size = 4) +\n  geom_abline(slope = Km, intercept = 0, color = \"red\") +\n  geom_abline(slope = 0, intercept = Vmax, color = \"purple\") +\n  labs(title = \"Michaelis Menton Enzyme Kinetics Experiment\",\n       subtitle = \"maximum reaction velocity asymptote (purple)\\nMichaelis-Menton constant (red slope)\",\n       caption = \"Essential Cell Biology\",\n       x = \"substrate concentration (micromolars)\",\n       y = \"reaction rate (micromolars per minute)\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html",
    "href": "posts/08_multiple_linear/08_multiple_linear.html",
    "title": "8: Multiple Linear Regression",
    "section": "",
    "text": "Goal: Expand to larger regression models\nObjective: Include multiple linear terms and an interaction term\n\n\n\n\n\n\n\nMoneyball (2011)\n\n\n\n\n\n\n\n\n\n\nNoteCode Packages\n\n\n\n\n\n\nlibrary(\"corrplot\")  #visualize correlations simultaneously\nlibrary(\"gt\")        #great tables\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\noakland_green &lt;- \"#003831\"\noakland_yellow &lt;- \"#EFB21E\"\n\n# user-defined function\ncor2text &lt;- function(x,y, num_digits = 4){\n  # This function will compute a correlation, round the result, and describe the results\n  # INPUTS:\n  ## x: numerical vector\n  ## y: numerical vector\n  ## num_digits: number of digits for rounding (default: 4)\n  # OUTPUT: string\n  \n  r = cor(x,y, use = \"pairwise.complete.obs\")\n  \n  cor_des &lt;- case_when(\n    r &gt;= 0.7 ~ \"strongly and positively correlated\",\n    r &gt;= 0.4 & r &lt; 0.7 ~ \"slightly and positively correlated\",\n    r &lt;= -0.4 & r &gt; -0.7 ~ \"slightly and negatively correlated\",\n    r &lt;= -0.7 ~ \"strongly and negatively correlated\",\n    .default = \"virtually uncorrelated\"\n  )\n  \n  #return\n  paste0(\"r = \", round(r, num_digits),\n         \", \", cor_des)\n}\n\n\nbb_df &lt;- readr::read_csv(\"baseball_data_90s.csv\")\n\noffense_cats &lt;- c(\"R\", \"H\", \"X2B\", \"X3B\", \"HR\", \"BB\", \"SO\", \"SB\")\ndefense_cats &lt;- c(\"RA\", \"ER\", \"HA\", \"HRA\", \"BBA\", \"SOA\", \"E\", \"FP\")\n\n\n\n\n\n\n\n\nDeterminationTruncationHeights\n\n\n\\[R^{2} = \\frac{\\text{explained variation}}{\\text{total variation}}\\]\nStatisticians like to define the coefficient of determination as the ratio of explained variation to total variation in the processes to produce the predictions (\\(\\hat{y}\\))\n\nHow much of the response variable’s variance is “explained” by the explanatory variables?\n\nExogenous variables may lead to the unexplained variation.\n\n\n\n\nToday, we are looking at multiple linear regression\n\\[\\hat{y} = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{3} + \\beta_{4}X_{4} + \\cdots\\]\nIf we have a truncated model\n\\[\\hat{y} = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2}\\]\nthen the coefficient of determination (\\(R^{2}\\)) refers to the variance in the response variable from\n\nexplained variation due to explanatory variables \\(X_{1}\\) and \\(X_{2}\\)\nunexplained variation from unexamined variables \\(X_{3}\\), \\(X_{4}\\), …\n\n\n\nLet us briefly return to the Galton heights example.\n\nhereditary_df &lt;- HistData::Galton\nlin_fit &lt;- lm(child ~ parent, data = hereditary_df)\nsummary(lin_fit)$adj.r.squared\n\n[1] 0.2096103\n\n\n\\[R^{2} \\approx 0.21\\]\nAbout 21 percent of the variance in the childrens’ heights are explained by the parents heights.\nThe other 79 percent of the variation perhaps could be explained by\n\nhealth\nfamily income\ndiet\nlocation\netc.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBaseball Incoming\n\n\n\n\n\nYou do not need to understand the baseball terminology (e.g. BA, SLG)\nInstead, focus on the arithmetic\n\nsifting through \\(R^2\\) values\nplugging numbers into models\ninterpreting slopes\n\n\n\n\n\n\n\nIs baseball boring?\n\n\n\n\n\n\n\n\n\n\nBookFinance\n\n\n\n\n\n\n\nMoneyball (2003)\n\n\n\n\n\n\nOakland Athletics fielded a competitive team despite having a payroll size around 1/3 of some other franchises\nTraditional scouting vs modern statistics\nIdea: Can we identify qualities (variables) in baseball players that lead to more wins?\n\n\n\n\n\n\n\n\nMLB Team Salaries, 2002\n\n\n\nimage source: Wikimedia Commons\n\n\n\n\n\n\n\n\nLahmanOffenseDefense\n\n\nToday’s data set comes from the Lahman package, which contains a lot of historical data about Major League Baseball.\n\nLahman package CRAN page\n\n\n\n\nR: runs\nH: hits\nX2B: doubles\nX3B: triples\nHR: home runs\nBB: walks\nSO: strikeouts (by hitters)\nSB: stolen bases\n\n\n\n\nRA: runs allowed\nER: earned runs\nHA: hits allowed\nHRA: home runs allowed\nBBA: walks allowed\nSOA: strikeouts (by pitchers)\nE: errors\nFP: fielding percentage"
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html#start",
    "href": "posts/08_multiple_linear/08_multiple_linear.html#start",
    "title": "8: Multiple Linear Regression",
    "section": "",
    "text": "Goal: Expand to larger regression models\nObjective: Include multiple linear terms and an interaction term\n\n\n\n\n\n\n\nMoneyball (2011)\n\n\n\n\n\n\n\n\n\n\nNoteCode Packages\n\n\n\n\n\n\nlibrary(\"corrplot\")  #visualize correlations simultaneously\nlibrary(\"gt\")        #great tables\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\noakland_green &lt;- \"#003831\"\noakland_yellow &lt;- \"#EFB21E\"\n\n# user-defined function\ncor2text &lt;- function(x,y, num_digits = 4){\n  # This function will compute a correlation, round the result, and describe the results\n  # INPUTS:\n  ## x: numerical vector\n  ## y: numerical vector\n  ## num_digits: number of digits for rounding (default: 4)\n  # OUTPUT: string\n  \n  r = cor(x,y, use = \"pairwise.complete.obs\")\n  \n  cor_des &lt;- case_when(\n    r &gt;= 0.7 ~ \"strongly and positively correlated\",\n    r &gt;= 0.4 & r &lt; 0.7 ~ \"slightly and positively correlated\",\n    r &lt;= -0.4 & r &gt; -0.7 ~ \"slightly and negatively correlated\",\n    r &lt;= -0.7 ~ \"strongly and negatively correlated\",\n    .default = \"virtually uncorrelated\"\n  )\n  \n  #return\n  paste0(\"r = \", round(r, num_digits),\n         \", \", cor_des)\n}\n\n\nbb_df &lt;- readr::read_csv(\"baseball_data_90s.csv\")\n\noffense_cats &lt;- c(\"R\", \"H\", \"X2B\", \"X3B\", \"HR\", \"BB\", \"SO\", \"SB\")\ndefense_cats &lt;- c(\"RA\", \"ER\", \"HA\", \"HRA\", \"BBA\", \"SOA\", \"E\", \"FP\")"
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html#explained-variation",
    "href": "posts/08_multiple_linear/08_multiple_linear.html#explained-variation",
    "title": "8: Multiple Linear Regression",
    "section": "",
    "text": "DeterminationTruncationHeights\n\n\n\\[R^{2} = \\frac{\\text{explained variation}}{\\text{total variation}}\\]\nStatisticians like to define the coefficient of determination as the ratio of explained variation to total variation in the processes to produce the predictions (\\(\\hat{y}\\))\n\nHow much of the response variable’s variance is “explained” by the explanatory variables?\n\nExogenous variables may lead to the unexplained variation.\n\n\n\n\nToday, we are looking at multiple linear regression\n\\[\\hat{y} = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{3} + \\beta_{4}X_{4} + \\cdots\\]\nIf we have a truncated model\n\\[\\hat{y} = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2}\\]\nthen the coefficient of determination (\\(R^{2}\\)) refers to the variance in the response variable from\n\nexplained variation due to explanatory variables \\(X_{1}\\) and \\(X_{2}\\)\nunexplained variation from unexamined variables \\(X_{3}\\), \\(X_{4}\\), …\n\n\n\nLet us briefly return to the Galton heights example.\n\nhereditary_df &lt;- HistData::Galton\nlin_fit &lt;- lm(child ~ parent, data = hereditary_df)\nsummary(lin_fit)$adj.r.squared\n\n[1] 0.2096103\n\n\n\\[R^{2} \\approx 0.21\\]\nAbout 21 percent of the variance in the childrens’ heights are explained by the parents heights.\nThe other 79 percent of the variation perhaps could be explained by\n\nhealth\nfamily income\ndiet\nlocation\netc."
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html#forward",
    "href": "posts/08_multiple_linear/08_multiple_linear.html#forward",
    "title": "8: Multiple Linear Regression",
    "section": "",
    "text": "NoteBaseball Incoming\n\n\n\n\n\nYou do not need to understand the baseball terminology (e.g. BA, SLG)\nInstead, focus on the arithmetic\n\nsifting through \\(R^2\\) values\nplugging numbers into models\ninterpreting slopes\n\n\n\n\n\n\n\nIs baseball boring?"
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html#story",
    "href": "posts/08_multiple_linear/08_multiple_linear.html#story",
    "title": "8: Multiple Linear Regression",
    "section": "",
    "text": "BookFinance\n\n\n\n\n\n\n\nMoneyball (2003)\n\n\n\n\n\n\nOakland Athletics fielded a competitive team despite having a payroll size around 1/3 of some other franchises\nTraditional scouting vs modern statistics\nIdea: Can we identify qualities (variables) in baseball players that lead to more wins?\n\n\n\n\n\n\n\n\nMLB Team Salaries, 2002\n\n\n\nimage source: Wikimedia Commons"
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html#data",
    "href": "posts/08_multiple_linear/08_multiple_linear.html#data",
    "title": "8: Multiple Linear Regression",
    "section": "",
    "text": "LahmanOffenseDefense\n\n\nToday’s data set comes from the Lahman package, which contains a lot of historical data about Major League Baseball.\n\nLahman package CRAN page\n\n\n\n\nR: runs\nH: hits\nX2B: doubles\nX3B: triples\nHR: home runs\nBB: walks\nSO: strikeouts (by hitters)\nSB: stolen bases\n\n\n\n\nRA: runs allowed\nER: earned runs\nHA: hits allowed\nHRA: home runs allowed\nBBA: walks allowed\nSOA: strikeouts (by pitchers)\nE: errors\nFP: fielding percentage"
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html#correlation",
    "href": "posts/08_multiple_linear/08_multiple_linear.html#correlation",
    "title": "8: Multiple Linear Regression",
    "section": "Correlation",
    "text": "Correlation\n\nOffenseCodeDefenseCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbb_df |&gt;\n  ggplot(aes(x = R, y = W)) +\n  geom_point(color = oakland_green) + \n  labs(title = \"Wins vs Runs Scored\",\n       subtitle = paste0(\"r = \", round(cor_value, 4), \n                         \", slightly and positively correlated\"),\n       caption = \"seasons 1990 to 1999\",\n       x = \"runs scored\",\n       y = \"wins\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncor_value &lt;- cor(bb_df$RA, bb_df$W)\n\nbb_df |&gt;\n  ggplot(aes(x = RA, y = W)) +\n  geom_point(color = oakland_green) + \n  labs(title = \"Wins vs Runs Allowed\",\n       subtitle = paste0(\"r = \", round(cor_value, 4), \n                         \", virtually uncorrelated\"),\n       caption = \"seasons 1990 to 1999\",\n       x = \"runs allowed\",\n       y = \"wins\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html#correlation-matrices",
    "href": "posts/08_multiple_linear/08_multiple_linear.html#correlation-matrices",
    "title": "8: Multiple Linear Regression",
    "section": "Correlation Matrices",
    "text": "Correlation Matrices\n\nOffenseCodeDefenseCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbb_df |&gt;\n  select(any_of(offense_cats)) |&gt;\n  cor() |&gt;\n  corrplot.mixed(order = \"FPC\",\n                 upper = \"ellipse\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbb_df |&gt;\n  select(any_of(defense_cats)) |&gt;\n  cor() |&gt;\n  corrplot.mixed(order = \"FPC\",\n                 upper = \"ellipse\")\n\n\n\n\n\n\n\n\n\n\nTipNew Directions\n\n\n\nSo far, a sabermetrician might observe and ask\n\nWins are correlated with runs scored\nWhat correlates well with runs scored?\n\n\n\n\n\n\n\n\n\nWarningDCP1"
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html#model-equation",
    "href": "posts/08_multiple_linear/08_multiple_linear.html#model-equation",
    "title": "8: Multiple Linear Regression",
    "section": "Model Equation",
    "text": "Model Equation\n\nMathCodeInterceptSlopeDetermination\n\n\n\\[\\text{Runs} = \\beta_{0} + \\beta_{1}(\\text{Hits})\\]\n\n\\(\\beta_{0}\\): intercept\n\\(\\beta_{1}\\): change in Runs with respect to Hits\n\n\n\n\nlm(R ~ H, data = bb_df)\n\n\nCall:\nlm(formula = R ~ H, data = bb_df)\n\nCoefficients:\n(Intercept)            H  \n  -108.7903       0.5934  \n\n\n\n\\(\\beta_{0} \\approx -108.7903\\)\n\\(\\beta_{1} \\approx 0.5934\\)\n\n\\[\\text{Runs} = -108.7903 + 0.5934(\\text{Hits})\\]\n\n\nIn a hypothetical scenario where a team has zero hits,\n\\[\\text{Runs} = -108.7903 + 0.5934(0)\\] the model estimates that the baseball team will win about negative 109 games in a season.\n\nsee note about “Removing the intercept” below\n\n\n\nWe continue to intercept the rate of change (or slope)\n\\[\\beta_{1} \\approx 0.5934\\]\nwith language like\n\nFor every additional hit, the number of runs increases by about 0.5934.\n\n\n\nWe can get a sense of how useful this model can be with the coefficient of determination.\n\nmod1 &lt;- lm(R ~ H, data = bb_df) #baseline model\nsummary(mod1)$adj.r.squared\n\n[1] 0.6893222\n\n\n\nAccording to the coefficient of determination, this model (with “Hits” as an explanatory variable) explains about 69 percent of the variance in runs scored.\n\n\n\n\n\n\n\n\n\n\nCautionRemoving the Intercept\n\n\n\n\n\nSometimes an analyst might want to remove the intercept term (here: zero hits should imply zero runs?)\n\\[\\text{Runs} = \\beta_{1}(\\text{Hits})\\]\n\nmod0 &lt;- lm(R ~ H - 1, data = bb_df) #removed intercept\nmod0\n\n\nCall:\nlm(formula = R ~ H - 1, data = bb_df)\n\nCoefficients:\n    H  \n0.517  \n\n\n\nsummary(mod0)$adj.r.squared #removed intercept\n\n[1] 0.993489\n\nsummary(mod1)$adj.r.squared #baseline model\n\n[1] 0.6893222\n\n\nWhile removing the intercept seems great in this simple example, in practice removing the intercept does not tend to generalize to larger models or inclusion of additional data."
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html#different-models-different-coefficients",
    "href": "posts/08_multiple_linear/08_multiple_linear.html#different-models-different-coefficients",
    "title": "8: Multiple Linear Regression",
    "section": "Different Models, Different Coefficients",
    "text": "Different Models, Different Coefficients\n\n\n\n\n\n\nWarningCoefficients are different in different models\n\n\n\n\n\n\n\n\n\n\n\n\n\nEarly Baseball Stats Models\n\n\nComparing the coefficients\n\n\ncoefs\nmod1\nmod2\n\n\n\n\nbeta_0\n-108.7903\n-124.4358\n\n\nbeta_1\n0.5934\n0.4382\n\n\nbeta_2\n-\n0.4395\n\n\n\nSML 201\n\n\n\n\n\n\n\n\n\nmod_stats_df &lt;- data.frame(\n  coefs = c(\"beta_0\", \"beta_1\", \"beta_2\"),\n  mod1 = c(-108.7903, 0.5934, \"-\"),\n  mod2 = c(-124.4358, 0.4382, 0.4395)\n)\n\nmod_stats_df |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"SML 201\") |&gt;\n  tab_header(\n    title = \"Early Baseball Stats Models\",\n    subtitle = \"Comparing the coefficients\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = oakland_green),\n      cell_text(color = oakland_yellow)\n    ),\n    locations = cells_body(columns = mod1)\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = oakland_yellow),\n      cell_text(color = oakland_green)\n    ),\n    locations = cells_body(columns = mod2)\n  )\n\n\n\n\nIn practice, we tend to explore several models through trial-and-error. After choosing a model, we then scrutinize the interpretation of the \\(\\beta\\) coefficients."
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html#augmentation",
    "href": "posts/08_multiple_linear/08_multiple_linear.html#augmentation",
    "title": "8: Multiple Linear Regression",
    "section": "Augmentation",
    "text": "Augmentation\nWe can attach new columns and calculations using mutate.\n\nbb_df &lt;- bb_df |&gt;\n  mutate(BA = H/AB,               #batting average\n         OBP = (H + BB + HBP)/AB, #on-base percentage\n         SLG = (H + X2B + 2*X3B + 3*HR)/AB, #slugging percentage\n         OPS = OBP + SLG)         #on-base plus slugging\n\n\nStats like runs, hits, walks, and strikeouts are called count statistics. Baseball players tend to accumulate count statistics with more playing time.\nStats like BA, OBP, and SLGare called rate statistics. These baseball statistics are adjusted over playing time.\nThese derived statistics may be better to evaluate individual baseball players (rather than whole teams).\nAside: yes, it may be silly to add together OBP and SLG (i.e. two rate statistics), but baseball writers really like this calculation."
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html#using-the-derived-statistics",
    "href": "posts/08_multiple_linear/08_multiple_linear.html#using-the-derived-statistics",
    "title": "8: Multiple Linear Regression",
    "section": "Using the Derived Statistics",
    "text": "Using the Derived Statistics\nWe build a model for different allocations of explanatory variables.\n\nfit_BA  &lt;- lm(R ~ BA,  data = bb_df)\nfit_OBP &lt;- lm(R ~ OBP, data = bb_df)\nfit_SLG &lt;- lm(R ~ SLG, data = bb_df)\nfit_OPS &lt;- lm(R ~ OBP + SLG, data = bb_df)"
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html#measuring-the-models",
    "href": "posts/08_multiple_linear/08_multiple_linear.html#measuring-the-models",
    "title": "8: Multiple Linear Regression",
    "section": "Measuring the Models",
    "text": "Measuring the Models\nWe use the coefficients of determination to help us rank the models.\n\nsummary(fit_BA)$adj.r.squared\n\n[1] 0.3445441\n\nsummary(fit_OBP)$adj.r.squared\n\n[1] 0.4279833\n\nsummary(fit_SLG)$adj.r.squared\n\n[1] 0.433078\n\nsummary(fit_OPS)$adj.r.squared\n\n[1] 0.4902423"
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html#picking-the-best-model",
    "href": "posts/08_multiple_linear/08_multiple_linear.html#picking-the-best-model",
    "title": "8: Multiple Linear Regression",
    "section": "Picking the Best Model",
    "text": "Picking the Best Model\n\ngtCode\n\n\n\n\n\n\n\n\n\n\nDerived Baseball Stats Models\n\n\nComparing the coefficients of determination\n\n\nmodels\nr2_vals\n\n\n\n\nfit_BA\n0.3445\n\n\nfit_OBP\n0.4280\n\n\nfit_SLG\n0.4331\n\n\nfit_OPS\n0.4902\n\n\n\nSML 201\n\n\n\n\n\n\n\n\n\n\n\nmod_stats_df2 &lt;- data.frame(\n  models = paste0(\"fit_\", c(\"BA\", \"OBP\", \"SLG\", \"OPS\")),\n  r2_vals = round(c(summary(fit_BA)$adj.r.squared,\n                    summary(fit_OBP)$adj.r.squared,\n                    summary(fit_SLG)$adj.r.squared,\n                    summary(fit_OPS)$adj.r.squared), 4)\n)\n\nmod_stats_df2 |&gt;\n  gt() |&gt;\n  cols_align(align = \"center\") |&gt;\n  tab_footnote(footnote = \"SML 201\") |&gt;\n  tab_header(\n    title = \"Derived Baseball Stats Models\",\n    subtitle = \"Comparing the coefficients of determination\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = oakland_yellow),\n      cell_text(color = oakland_green,\n                weight = \"bold\")\n    ),\n    locations = cells_body(columns = c(models, r2_vals),\n                           rows = r2_vals == max(r2_vals))\n    # finds maximum value programmatically\n  )\n\n\n\n\n\n\n\n\n\n\nTipUsing the Coefficients of Determination\n\n\n\n\nhigher \\(R^{2}\\) value may indicate a better model\n\nas far as connecting explanatory variables to the data\n\n\n\n\n\n\n\n\n\n\nWarningDCP2"
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html#before-interaction",
    "href": "posts/08_multiple_linear/08_multiple_linear.html#before-interaction",
    "title": "8: Multiple Linear Regression",
    "section": "Before Interaction",
    "text": "Before Interaction\nFirst, let us try another multiple linear regression model\n\nVariablesCodeInterceptSlopesDetermination\n\n\n\\[\\text{Wins} = \\beta_{0} + \\beta_{1}(\\text{Runs Scored}) + \\beta_{2}(\\text{Runs Allowed})\\]\n\nresponse variable: Wins\nexplanatory variables:\n\nRuns Scored (offense)\nRuns Allowed (defense)\n\n\n\n\n\nlm(W ~ R + RA, data = bb_df)\n\n\nCall:\nlm(formula = W ~ R + RA, data = bb_df)\n\nCoefficients:\n(Intercept)            R           RA  \n   42.50059      0.12530     -0.07692  \n\n\n\n\nIn a hypothetical scenario where a team has zero runs and zero runs allowed,\n\\[\\text{Wins} = 42.5006 + 0.1253(0) - 0.0769(0)\\]\nthe model estimates that the baseball team will win about 43 games in a season.\n\n\nIn regression, we say that we control for other variables by treating other variables as constants.\n\\[\\text{Wins} = 42.5006 + 0.1253(\\text{Runs Scored}) - 0.0769(\\text{Runs Allowed})\\]\n\nHolding runs allowed constant, for every additional run scored, the number of wins increases by about 0.1253.\nHolding runs scored constant, for every additional run allowed, the number of wins decreases by about 0.0769.\n\n\n\nWith our usage of the coefficient of determination\n\nwithout_interaction &lt;- lm(W ~ R + RA, data = bb_df)\n\nsummary(without_interaction)$adj.r.squared\n\n[1] 0.700125\n\n\n\nAccording to the coefficient of determination, this model (with 2 explanatory variables) explains about 70 percent of the variance in wins."
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html#with-an-interaction-term",
    "href": "posts/08_multiple_linear/08_multiple_linear.html#with-an-interaction-term",
    "title": "8: Multiple Linear Regression",
    "section": "With an Interaction Term",
    "text": "With an Interaction Term\nNow let us explore an interaction term\n\nVariablesCodeInterceptSlopesDetermination\n\n\n\\[\\begin{array}{rcl}\n\\text{Wins} & = & \\beta_{0} \\\\\n& & + \\beta_{1}(\\text{Runs Scored}) \\\\\n& & + \\beta_{2}(\\text{Runs Allowed}) \\\\\n& & + \\beta_{3}(\\text{Runs Scored})(\\text{Runs Allowed}) \\\\\n\\end{array}\\]\n\nresponse variable: Wins\nexplanatory variables:\n\nRuns Scored\nRuns Allowed\ninteraction between Runs Scored and Runs Allowed\n\n\n\n\n\nlm(W ~ R + RA + R:RA, data = bb_df)\n\n\nCall:\nlm(formula = W ~ R + RA + R:RA, data = bb_df)\n\nCoefficients:\n(Intercept)            R           RA         R:RA  \n -8.896e+01    3.082e-01    1.090e-01   -2.555e-04  \n\n\n\n\nIn a hypothetical scenario where a team has zero runs and zero runs allowed,\n\\[\\text{Wins} = -88.96 + 0.3082(0) + 0.1090(0) - 0.0002(0)(0)\\]\nthe model estimates that the baseball team will win about -89 games in a season.\n\n\nTo get a sense of how many runs a MLB team allows in a season, we can use the summary command.\n\nsummary(bb_df$RA)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  448.0   655.2   721.0   726.5   794.0  1103.0 \n\n\n\nStrong Defense\nSuppose that a MLB team has good defensive skills and allows about 650 runs in a season (i.e. around the 20th percentile).\n\\[\\begin{array}{rcl}\n\\text{Wins} & = & -88.96 + 0.3082(\\text{Runs Scored}) + 0.1090(\\text{Runs Allowed}) - 0.0002(\\text{Runs Scored})(\\text{Runs Allowed}) \\\\\n~ & = & -88.96 + 0.3082(\\text{Runs Scored}) + 0.1090(650) - 0.0002(\\text{Runs Scored})(650) \\\\\n  ~ & = & -18.11 + 0.1782(\\text{Runs Scored})\\\\\n\\end{array}\\]\n\nHolding runs allowed constant at 650, for every additional run scored, the number of wins increases by about 0.1782.\n\n\n\nWeak Defense\nSuppose that a MLB team has weak defensive skills and allows about 800 runs in a season (i.e. around the 80th percentile).\n\\[\\begin{array}{rcl}\n\\text{Wins} & = & -88.96 + 0.3082(\\text{Runs Scored}) + 0.1090(\\text{Runs Allowed}) - 0.0002(\\text{Runs Scored})(\\text{Runs Allowed}) \\\\\n~ & = & -88.96 + 0.3082(\\text{Runs Scored}) + 0.1090(800) - 0.0002(\\text{Runs Scored})(800) \\\\\n  ~ & = & -1.76 + 0.1482(\\text{Runs Scored})\\\\\n\\end{array}\\]\n\nHolding runs allowed constant at 800, for every additional run scored, the number of wins increases by about 0.1482.\n\n\n\n\nWith our usage of the coefficient of determination\n\nwith_interaction &lt;- lm(W ~ R + RA + R:RA, data = bb_df)\n\nsummary(without_interaction)$adj.r.squared\n\n[1] 0.700125\n\nsummary(with_interaction)$adj.r.squared\n\n[1] 0.764043\n\n\n\nAccording to the coefficient of determination, this model (with the interaction term) explains about 76 percent of the variance in wins.\n\n\n\n\n\n\n\n\n\n\nWarningDCP3"
  },
  {
    "objectID": "posts/08_multiple_linear/08_multiple_linear.html#milestone-home-run-totals",
    "href": "posts/08_multiple_linear/08_multiple_linear.html#milestone-home-run-totals",
    "title": "8: Multiple Linear Regression",
    "section": "Milestone Home Run Totals",
    "text": "Milestone Home Run Totals\n\nRecordsCodeZ-ScoresSwitchWhat If\n\n\n\n\n\n\n\nRoger Maris\n\n\n\n\n\n\nRoger Maris hit 61 home runs in 1961\nAaron Judge hit 62 home runs in 2022\n\nAmerican League records\n\n\n\n\n\n\n\n\nAaron Judge\n\n\n\n\n\n\n\nLahman::Batting |&gt;\n  filter(yearID %in% c(\"1961\", \"2022\")) |&gt;\n  filter(AB &gt;= 100) |&gt;\n  group_by(yearID) |&gt;\n  summarize(xbar = mean(HR, na.rm = TRUE),\n            s = sd(HR, na.rm = TRUE))\n\n# A tibble: 2 × 3\n  yearID  xbar     s\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   1961  10.8 10.8 \n2   2022  10.6  8.91\n\n\n\n\n\\[z_{M} = \\frac{61 - \\bar{x}_{2022}}{s_{2022}} = \\frac{61 - 10.7615}{10.7928} \\approx 4.6548\\] Roger Maris’ home run record was about 4.6548 standard deviations above the mean in 1961.\n\\[z_{J} = \\frac{62 - \\bar{x}_{2022}}{s_{2022}} = \\frac{62 - 10.5729}{8.9065} \\approx 5.7741\\]\nAaron Judge’s home run record was about 5.7741 standard deviations above the mean in 2022.\n\n\n\\[4.6548 = \\frac{x_{M} - \\bar{x}_{2022}}{s_{2022}} = \\frac{x_{M} - 10.5729}{8.9065} \\Rightarrow x_{M} \\approx 52.0388\\]\n\\[5.7741 = \\frac{x_{J} - \\bar{x}_{1961}}{s_{1961}} = \\frac{x_{J} - 10.7615}{10.7928} \\Rightarrow x_{J} \\approx 73.0802\\]\n\n\n\n\n\n\n\nRoger Maris\n\n\n\n\n\n\nRoger Maris would have hit 52 home runs is 2022\nAaron Judge would have hit 73 home runs in 1961\n\n\n\n\n\n\n\nAaron Judge"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html",
    "href": "posts/09_explain_cats/09_explain_cats.html",
    "title": "9: Modeling Categorical Variables",
    "section": "",
    "text": "Goal: Utilize categorical variables in regression models\nObjective: Explore one-hot encoding and start classification\n\n\n\n\n\n\n\nmultiple slopes!\n\n\n\n\n\n\n\n\n\n\nNotecode packages\n\n\n\n\n\n\nlibrary(\"janitor\")   #tools for data cleaning\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n\nloan_df &lt;- readr::read_csv(\"loan_data_set.csv\") |&gt;\n  janitor::clean_names()\n\n\n\n\n\n\n\n\nDescriptionScenarioResponse VariableExplanatory Variables\n\n\n\n\n“Dream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.”\n\nSource: Kaggle\n\n\n\n\n\n\n\nDream Home Finance\n\n\n\n\n\n\n“Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set.”\n\n\nWe will try to predict the loan amount (i.e. numerical variable)\n\nLoan amount (in thousands of dollars)\n\n\n\n\nGender (of primary applicant)\nMarital status (of primary applicant)\nDependents\nEducation\nSelf-employed\nApplicant income (monthly, in dollars)\nCo-applicant income (monthly, in dollars)\nloan amount terms (in months)\nCredit history\nProperty area\n\n\n\n\n\n\n\n\nremove rows that have missing values in the response variable (loan_amount)\nconvert dependents to a numerical variable\n\nhere, replace “+” with nothing\n\ncombine “income” columns\n\nensure all dollar amounts are in the same units (thousands of dollars)\n\nconvert credit_history to a factor variable (i.e. categorical)\nretain relevant columns\n\n\nloan_df &lt;- loan_df |&gt;\n  filter(!is.na(loan_amount)) |&gt;\n  mutate(dependents_num = as.numeric(\n    str_replace(dependents, \"\\\\+\", \"\")\n  )) |&gt;\n  mutate(income = applicant_income/1000 + coapplicant_income/1000) |&gt;\n  mutate(credit_history = factor(credit_history)) |&gt;\n  select(loan_amount, income, dependents_num, gender, married, education, self_employed, credit_history, property_area, loan_status)\n\nAfter cleaning the data, we should report the size of the resultant data frame\n\nnrow(loan_df) #number of observations\n\n[1] 592\n\nncol(loan_df) #number of variables\n\n[1] 10\n\n\nand the structure of the data frame.\n\nstr(loan_df, give.attr = FALSE)\n\ntibble [592 × 10] (S3: tbl_df/tbl/data.frame)\n $ loan_amount   : num [1:592] 128 66 120 141 267 95 158 168 349 70 ...\n $ income        : num [1:592] 6.09 3 4.94 6 9.61 ...\n $ dependents_num: num [1:592] 1 0 0 0 2 0 3 2 1 2 ...\n $ gender        : chr [1:592] \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ married       : chr [1:592] \"Yes\" \"Yes\" \"Yes\" \"No\" ...\n $ education     : chr [1:592] \"Graduate\" \"Graduate\" \"Not Graduate\" \"Graduate\" ...\n $ self_employed : chr [1:592] \"No\" \"Yes\" \"No\" \"No\" ...\n $ credit_history: Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 1 2 2 2 ...\n $ property_area : chr [1:592] \"Rural\" \"Urban\" \"Urban\" \"Urban\" ...\n $ loan_status   : chr [1:592] \"N\" \"Y\" \"Y\" \"Y\" ..."
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#start",
    "href": "posts/09_explain_cats/09_explain_cats.html#start",
    "title": "9: Modeling Categorical Variables",
    "section": "",
    "text": "Goal: Utilize categorical variables in regression models\nObjective: Explore one-hot encoding and start classification\n\n\n\n\n\n\n\nmultiple slopes!\n\n\n\n\n\n\n\n\n\n\nNotecode packages\n\n\n\n\n\n\nlibrary(\"janitor\")   #tools for data cleaning\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n\nloan_df &lt;- readr::read_csv(\"loan_data_set.csv\") |&gt;\n  janitor::clean_names()"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#data",
    "href": "posts/09_explain_cats/09_explain_cats.html#data",
    "title": "9: Modeling Categorical Variables",
    "section": "",
    "text": "DescriptionScenarioResponse VariableExplanatory Variables\n\n\n\n\n“Dream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.”\n\nSource: Kaggle\n\n\n\n\n\n\n\nDream Home Finance\n\n\n\n\n\n\n“Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set.”\n\n\nWe will try to predict the loan amount (i.e. numerical variable)\n\nLoan amount (in thousands of dollars)\n\n\n\n\nGender (of primary applicant)\nMarital status (of primary applicant)\nDependents\nEducation\nSelf-employed\nApplicant income (monthly, in dollars)\nCo-applicant income (monthly, in dollars)\nloan amount terms (in months)\nCredit history\nProperty area"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#cleaning",
    "href": "posts/09_explain_cats/09_explain_cats.html#cleaning",
    "title": "9: Modeling Categorical Variables",
    "section": "",
    "text": "remove rows that have missing values in the response variable (loan_amount)\nconvert dependents to a numerical variable\n\nhere, replace “+” with nothing\n\ncombine “income” columns\n\nensure all dollar amounts are in the same units (thousands of dollars)\n\nconvert credit_history to a factor variable (i.e. categorical)\nretain relevant columns\n\n\nloan_df &lt;- loan_df |&gt;\n  filter(!is.na(loan_amount)) |&gt;\n  mutate(dependents_num = as.numeric(\n    str_replace(dependents, \"\\\\+\", \"\")\n  )) |&gt;\n  mutate(income = applicant_income/1000 + coapplicant_income/1000) |&gt;\n  mutate(credit_history = factor(credit_history)) |&gt;\n  select(loan_amount, income, dependents_num, gender, married, education, self_employed, credit_history, property_area, loan_status)\n\nAfter cleaning the data, we should report the size of the resultant data frame\n\nnrow(loan_df) #number of observations\n\n[1] 592\n\nncol(loan_df) #number of variables\n\n[1] 10\n\n\nand the structure of the data frame.\n\nstr(loan_df, give.attr = FALSE)\n\ntibble [592 × 10] (S3: tbl_df/tbl/data.frame)\n $ loan_amount   : num [1:592] 128 66 120 141 267 95 158 168 349 70 ...\n $ income        : num [1:592] 6.09 3 4.94 6 9.61 ...\n $ dependents_num: num [1:592] 1 0 0 0 2 0 3 2 1 2 ...\n $ gender        : chr [1:592] \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ married       : chr [1:592] \"Yes\" \"Yes\" \"Yes\" \"No\" ...\n $ education     : chr [1:592] \"Graduate\" \"Graduate\" \"Not Graduate\" \"Graduate\" ...\n $ self_employed : chr [1:592] \"No\" \"Yes\" \"No\" \"No\" ...\n $ credit_history: Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 1 2 2 2 ...\n $ property_area : chr [1:592] \"Rural\" \"Urban\" \"Urban\" \"Urban\" ...\n $ loan_status   : chr [1:592] \"N\" \"Y\" \"Y\" \"Y\" ..."
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#scenario-1",
    "href": "posts/09_explain_cats/09_explain_cats.html#scenario-1",
    "title": "9: Modeling Categorical Variables",
    "section": "Scenario 1:",
    "text": "Scenario 1:\n\nresponse variable: loan_amount\nexplanatory variables:\n\nincome\ndependents_num"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#build-the-model",
    "href": "posts/09_explain_cats/09_explain_cats.html#build-the-model",
    "title": "9: Modeling Categorical Variables",
    "section": "Build the Model",
    "text": "Build the Model\n\nmod1 &lt;- lm(loan_amount ~ income + dependents_num, data = loan_df)"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#interpretation",
    "href": "posts/09_explain_cats/09_explain_cats.html#interpretation",
    "title": "9: Modeling Categorical Variables",
    "section": "Interpretation",
    "text": "Interpretation\nSometimes we interpret the slopes to see if the found coefficients make sense in context.\n\nmod1\n\n\nCall:\nlm(formula = loan_amount ~ income + dependents_num, data = loan_df)\n\nCoefficients:\n   (Intercept)          income  dependents_num  \n        84.518           8.048           6.943  \n\n\n\nHolding dependents constant, for each $1000 increase in monthly income, the loan amount increases by about $8000\n\n\nHolding income constant, for each additional dependent, the loan amount increases by about $7000\n\n\n\n\n\n\n\nTipIntercept\n\n\n\n\n\nCan you interpret the intercept value too? Does that make sense to you?"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#determination",
    "href": "posts/09_explain_cats/09_explain_cats.html#determination",
    "title": "9: Modeling Categorical Variables",
    "section": "Determination",
    "text": "Determination\n\nsummary(mod1)$adj.r.squared\n\n[1] 0.3955349\n\n\n\nFor this baseline model, the coefficient of determination states that we can explain about 40 percent of the variance in loan amount with these two numerical explanatory variables."
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#scatterplot",
    "href": "posts/09_explain_cats/09_explain_cats.html#scatterplot",
    "title": "9: Modeling Categorical Variables",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nloan_df |&gt;\n  filter(!is.na(credit_history)) |&gt;\n  ggplot(aes(x = income, y = loan_amount, color = credit_history)) +\n  geom_point() +\n  geom_smooth(formula = \"y ~ x\",\n              method = \"lm\",\n              se = FALSE) +\n  labs(title = \"Dream Home Finance\",\n       subtitle = \"Interaction Plot\",\n       caption = \"SML 201\",\n       x = \"combined monthly income (thousands)\",\n       y = \"loan amount (thousands)\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#model-2",
    "href": "posts/09_explain_cats/09_explain_cats.html#model-2",
    "title": "9: Modeling Categorical Variables",
    "section": "Model 2",
    "text": "Model 2\n\nmod2_without_interaction &lt;- lm(loan_amount ~ income + \n                                 dependents_num +\n                                 credit_history,\n                               data = loan_df)"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#model-statistics",
    "href": "posts/09_explain_cats/09_explain_cats.html#model-statistics",
    "title": "9: Modeling Categorical Variables",
    "section": "Model Statistics",
    "text": "Model Statistics\n\nsummary(mod2_without_interaction)\n\n\nCall:\nlm(formula = loan_amount ~ income + dependents_num + credit_history, \n    data = loan_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-372.17  -28.38   -5.71   21.62  356.55 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      82.5113     8.2910   9.952  &lt; 2e-16 ***\nincome            7.6934     0.4386  17.539  &lt; 2e-16 ***\ndependents_num    8.8323     2.8231   3.129  0.00185 ** \ncredit_history1   2.2122     7.9957   0.277  0.78214    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 65.84 on 526 degrees of freedom\n  (62 observations deleted due to missingness)\nMultiple R-squared:  0.3908,    Adjusted R-squared:  0.3873 \nF-statistic: 112.5 on 3 and 526 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#interpretation-1",
    "href": "posts/09_explain_cats/09_explain_cats.html#interpretation-1",
    "title": "9: Modeling Categorical Variables",
    "section": "Interpretation",
    "text": "Interpretation\n\\[\\begin{array}{rcl}\nY & = & \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}X_{3} \\\\\n~ & \\approx & 81.7427 + 6.4019X_{1} + 9.3093X_{2} + 1.9206X_{3}\n\\end{array}\\]\nwhere\n\\[X_{1}: \\text{income}, \\quad X_{2}: \\text{number of dependents}\\]\nand\n\\[X_{3} = \\begin{cases} 1, & \\text{passed credit check} \\\\\n0, & \\text{did not pass credit check} \\\\ \\end{cases}\\]\n\nPassing the credit check increases the home loan by about $2000 (??)\n\n\n\n\n\n\n\nWarningDCP1"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#refined-interpretation",
    "href": "posts/09_explain_cats/09_explain_cats.html#refined-interpretation",
    "title": "9: Modeling Categorical Variables",
    "section": "Refined Interpretation",
    "text": "Refined Interpretation\n\nsummary(mod2_with_interaction)\n\n\nCall:\nlm(formula = loan_amount ~ income + dependents_num + income:credit_history, \n    data = loan_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-327.08  -28.02   -4.70   21.74  346.79 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             81.7427     4.6271  17.666  &lt; 2e-16 ***\nincome                   6.4019     0.6430   9.957  &lt; 2e-16 ***\ndependents_num           9.3093     2.8080   3.315 0.000979 ***\nincome:credit_history1   1.9206     0.7048   2.725 0.006646 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 65.38 on 526 degrees of freedom\n  (62 observations deleted due to missingness)\nMultiple R-squared:  0.3992,    Adjusted R-squared:  0.3957 \nF-statistic: 116.5 on 3 and 526 DF,  p-value: &lt; 2.2e-16\n\n\n\\[\\begin{array}{rcl}\nY & = & \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}X_{3} \\\\\n~ & = & 81.7427 + 6.4019X_{1} + 9.3093X_{2} + 1.9206X_{1}X_{3} \\\\\n\\end{array}\\]\n\nFor a family with \\(X_{2} = 0\\) dependents and bad credit history (\\(X_{3} = 0\\))\n\n\\[Y = 81.7427 + 6.4019X_{1}\\]\n\nFor each $1000 increase in monthly income, the loan amount increases by about $6400.\n\n\nFor a family with \\(X_{2} = 0\\) dependents and good credit history (\\(X_{3} = 1\\))\n\n\\[Y = 81.7427 + 6.4019X_{1} + 1.9206X_{1}\\]\n\nFor each $1000 increase in monthly income, the loan amount increases by about $8300.\n\n\n\n\n\n\n\nWarningDCP2"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#determination-1",
    "href": "posts/09_explain_cats/09_explain_cats.html#determination-1",
    "title": "9: Modeling Categorical Variables",
    "section": "Determination",
    "text": "Determination\n\nsummary(mod2_with_interaction)$adj.r.squared\n\n[1] 0.3957492\n\n\n\nFor this model with an interaction term and the two original explanatory variables, the coefficient of determination shows that we can explain about 40 percent of the variance in the loan amount.\n\n\n\n\n\n\n\nNoteHow Complex Should Models Be?\n\n\n\n\n\nDid you catch that?\n\nmod1 had a coefficient of determination of about 0.3955\nmod2_with_interaction had a coefficient of determination of about 0.3957\n\nWith hardly any gains in explaning variance, an analyst might opt to continue with the simpler model (here: mod1) moving forward in an analysis since it eases interpretability"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#scatterplot-1",
    "href": "posts/09_explain_cats/09_explain_cats.html#scatterplot-1",
    "title": "9: Modeling Categorical Variables",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nloan_df |&gt;\n  filter(!is.na(credit_history)) |&gt;\n  ggplot(aes(x = income, y = loan_amount, color = education)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(formula = \"y ~ x\",\n              method = \"lm\",\n              se = FALSE) +\n  labs(title = \"Dream Home Finance\",\n       subtitle = \"Parallel Slopes?\",\n       caption = \"SML 201\",\n       x = \"combined monthly income (thousands)\",\n       y = \"loan amount (thousands)\") +\n  scale_color_manual(values = c(\"gray\", \"darkgreen\")) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#model-3",
    "href": "posts/09_explain_cats/09_explain_cats.html#model-3",
    "title": "9: Modeling Categorical Variables",
    "section": "Model 3",
    "text": "Model 3\n\nmod3_without_interaction &lt;- lm(loan_amount ~ income + \n                                 dependents_num +\n                                 education,\n                               data = loan_df)"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#model-statistics-1",
    "href": "posts/09_explain_cats/09_explain_cats.html#model-statistics-1",
    "title": "9: Modeling Categorical Variables",
    "section": "Model Statistics",
    "text": "Model Statistics\n\nsummary(mod3_without_interaction)\n\n\nCall:\nlm(formula = loan_amount ~ income + dependents_num + education, \n    data = loan_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-388.68  -28.33   -6.10   20.14  402.21 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            89.1379     4.7793  18.651  &lt; 2e-16 ***\nincome                  7.8672     0.4314  18.238  &lt; 2e-16 ***\ndependents_num          7.4332     2.7706   2.683  0.00751 ** \neducationNot Graduate -17.4739     6.8909  -2.536  0.01148 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 66.82 on 575 degrees of freedom\n  (13 observations deleted due to missingness)\nMultiple R-squared:  0.4043,    Adjusted R-squared:  0.4012 \nF-statistic: 130.1 on 3 and 575 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#interpretation-2",
    "href": "posts/09_explain_cats/09_explain_cats.html#interpretation-2",
    "title": "9: Modeling Categorical Variables",
    "section": "Interpretation",
    "text": "Interpretation\n\\[\\begin{array}{rcl}\nY & = & \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}X_{3} \\\\\n~ & \\approx & 89.1379 + 7.8672X_{1} + 7.4332X_{2} - 17.4739X_{3}\n\\end{array}\\]\nwhere\n\\[X_{1}: \\text{income}, \\quad X_{2}: \\text{number of dependents}\\]\nand\n\\[X_{3} = \\begin{cases} 1, & \\text{did not graduate from college} \\\\\n0, & \\text{graduated college} \\\\ \\end{cases}\\]\n\nThose who did not graduate from college had a home loan value that was lower by about $17500."
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#interaction-term-1",
    "href": "posts/09_explain_cats/09_explain_cats.html#interaction-term-1",
    "title": "9: Modeling Categorical Variables",
    "section": "Interaction Term",
    "text": "Interaction Term\n\\[\\begin{array}{rcl}\nY & = & \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}X_{3} \\\\\n\\end{array}\\]\n\nmod3_with_interaction &lt;- lm(loan_amount ~ income +\n                              dependents_num +\n                              income:education,\n                            data = loan_df)"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#refined-interpretation-1",
    "href": "posts/09_explain_cats/09_explain_cats.html#refined-interpretation-1",
    "title": "9: Modeling Categorical Variables",
    "section": "Refined Interpretation",
    "text": "Refined Interpretation\n\nsummary(mod3_with_interaction)\n\n\nCall:\nlm(formula = loan_amount ~ income + dependents_num + income:education, \n    data = loan_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-395.44  -28.18   -6.58   21.03  401.70 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   87.3028     4.6628  18.723   &lt;2e-16 ***\nincome                         7.9834     0.4278  18.662   &lt;2e-16 ***\ndependents_num                 7.1590     2.7728   2.582   0.0101 *  \nincome:educationNot Graduate  -2.3047     1.2066  -1.910   0.0566 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 66.98 on 575 degrees of freedom\n  (13 observations deleted due to missingness)\nMultiple R-squared:  0.4014,    Adjusted R-squared:  0.3983 \nF-statistic: 128.5 on 3 and 575 DF,  p-value: &lt; 2.2e-16\n\n\n\\[\\begin{array}{rcl}\nY & = & \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}X_{3} \\\\\n~ & = & 87.3028 + 7.9834X_{1} + 7.1590X_{2} - 2.3047X_{1}X_{3} \\\\\n\\end{array}\\]\n\nFor a family with \\(X_{2} = 2\\) dependents and graduated from college (\\(X_{3} = 0\\))\n\n\\[Y = 87.3028 + 7.9834X_{1} + 14.3180\\]\n\nFor each $1000 increase in monthly income, the loan amount increases by about $8000.\n\n\nFor a family with \\(X_{2} = 2\\) dependents and did not graduate from college (\\(X_{3} = 1\\))\n\n\\[Y = 81.7427 + 7.9834X_{1} + 14.3180 - 2.3047X_{1}\\]\n\nFor each $1000 increase in monthly income, the loan amount increases by about $5600."
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#determination-2",
    "href": "posts/09_explain_cats/09_explain_cats.html#determination-2",
    "title": "9: Modeling Categorical Variables",
    "section": "Determination",
    "text": "Determination\n\nsummary(mod3_with_interaction)$adj.r.squared\n\n[1] 0.3983014\n\n\n\nFor this model with an interaction term and the two original explanatory variables, the coefficient of determination shows that we can explain about 40 percent of the variance in the loan amount.\n\n\n\n\n\n\n\nNoteReordering Factors\n\n\n\n\n\nYou may have noticed that R tends to output categorical variables in alphabetical order by default (e.g. the bars in a bar chart). If you want to customize the order presented, you would need to employ a factor variable where you can explicitly set the levels.\n\nloan_df &lt;- loan_df |&gt;\n  mutate(education_fac = \n           factor(education,\n                  levels = c(\"Not Graduate\", \"Graduate\")))\n\nNow, the software will treat “Not Graduate” as the baseline and “Graduate” as the augmentation.\n\nmod4 &lt;- lm(loan_amount ~ income + dependents_num + \n             education_fac,\n           data = loan_df)\nsummary(mod4)\n\n\nCall:\nlm(formula = loan_amount ~ income + dependents_num + education_fac, \n    data = loan_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-388.68  -28.33   -6.10   20.14  402.21 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            71.6640     6.7245  10.657  &lt; 2e-16 ***\nincome                  7.8672     0.4314  18.238  &lt; 2e-16 ***\ndependents_num          7.4332     2.7706   2.683  0.00751 ** \neducation_facGraduate  17.4739     6.8909   2.536  0.01148 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 66.82 on 575 degrees of freedom\n  (13 observations deleted due to missingness)\nMultiple R-squared:  0.4043,    Adjusted R-squared:  0.4012 \nF-statistic: 130.1 on 3 and 575 DF,  p-value: &lt; 2.2e-16\n\n\n\nloan_df |&gt;\n  filter(!is.na(education_fac)) |&gt;\n  ggplot(aes(x = income, y = loan_amount, color = education_fac)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(formula = \"y ~ x\",\n              method = \"lm\",\n              se = FALSE) +\n  labs(title = \"Dream Home Finance\",\n       subtitle = \"Parallel Slopes?\",\n       caption = \"SML 201\",\n       x = \"combined monthly income (thousands)\",\n       y = \"loan amount (thousands)\") +\n  scale_color_manual(values = c(\"gray\", \"#E77500\")) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#label-encoding",
    "href": "posts/09_explain_cats/09_explain_cats.html#label-encoding",
    "title": "9: Modeling Categorical Variables",
    "section": "Label Encoding",
    "text": "Label Encoding\n\n\n\n\n\n\nWarningLabeller in practice\n\n\n\nCaution: Label encoding is usually not recommended in general. I include this section here for posterity.\n\n\n\nset.seed(1337)\nloan_df |&gt;\n  mutate(property_type = case_when(\n    property_area == \"Rural\" ~ 0,\n    property_area == \"Semiurban\" ~ 1,\n    property_area == \"Urban\" ~ 2,\n    TRUE ~ -1\n  )) |&gt;\n  select(property_area, property_type) |&gt;\n  sample_n(size = 10, replace = FALSE)\n\n# A tibble: 10 × 2\n   property_area property_type\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 Semiurban                 1\n 2 Semiurban                 1\n 3 Semiurban                 1\n 4 Rural                     0\n 5 Rural                     0\n 6 Semiurban                 1\n 7 Semiurban                 1\n 8 Semiurban                 1\n 9 Urban                     2\n10 Semiurban                 1\n\n\n\n\n\n\n\n\nTipNumerical?\n\n\n\nCould we then use property_type in a regression model?\n\n\n\n\n\n\n\n\nWarningOrdinal\n\n\n\nCaution: Label encoding here implied ranking\n\norder: “Rural”, then “Semiurban”, then “Urban”\nproportions: Is “Urban” (with a coefficient of 2) twice as valuable as “Semiurban”?"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#one-hot-encoding",
    "href": "posts/09_explain_cats/09_explain_cats.html#one-hot-encoding",
    "title": "9: Modeling Categorical Variables",
    "section": "One-Hot Encoding",
    "text": "One-Hot Encoding\nSimilar to previous calculations, our R software employs dummy variables or one-hot encoding to represent categories as ones and zeroes.\n\nset.seed(1337)\nloan_df |&gt;\n  mutate(rural_bool = ifelse(property_area == \"Rural\", 1, 0),\n         semiu_bool = ifelse(property_area == \"Semiurban\", 1, 0),\n         urban_bool = ifelse(property_area == \"Urban\", 1, 0)) |&gt;\n  select(property_area, rural_bool, semiu_bool, urban_bool) |&gt;\n  sample_n(size = 10, replace = FALSE)\n\n# A tibble: 10 × 4\n   property_area rural_bool semiu_bool urban_bool\n   &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Semiurban              0          1          0\n 2 Semiurban              0          1          0\n 3 Semiurban              0          1          0\n 4 Rural                  1          0          0\n 5 Rural                  1          0          0\n 6 Semiurban              0          1          0\n 7 Semiurban              0          1          0\n 8 Semiurban              0          1          0\n 9 Urban                  0          0          1\n10 Semiurban              0          1          0\n\n\n\\[\\begin{array}{rcl}\nY & = & \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{3} + \\beta_{4}X_{4} \\\\\n\\end{array}\\]\nwhere\n\\[X_{1}: \\text{income}, \\quad X_{2}: \\text{number of dependents}\\]\nand\n\\[\\begin{array}{rcl}\n  X_{3} & = & \\begin{cases} 1, \\text{semiurban area} \\\\ 0, \\text{otherwise} \\\\ \\end{cases} \\\\\n  X_{4} & = & \\begin{cases} 1, \\text{urban area} \\\\ 0, \\text{otherwise} \\\\ \\end{cases} \\\\\n\\end{array}\\]\n\n\n\n\n\n\nWarningDCP3"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#model-5",
    "href": "posts/09_explain_cats/09_explain_cats.html#model-5",
    "title": "9: Modeling Categorical Variables",
    "section": "Model 5",
    "text": "Model 5\n\nmod5 &lt;- lm(loan_amount ~ income + \n             dependents_num +\n             property_area,\n           data = loan_df)"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#model-statistics-2",
    "href": "posts/09_explain_cats/09_explain_cats.html#model-statistics-2",
    "title": "9: Modeling Categorical Variables",
    "section": "Model Statistics",
    "text": "Model Statistics\n\nsummary(mod5)\n\n\nCall:\nlm(formula = loan_amount ~ income + dependents_num + property_area, \n    data = loan_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-402.72  -28.36   -7.05   19.80  409.16 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             89.1639     6.1868  14.412   &lt;2e-16 ***\nincome                   8.0575     0.4275  18.849   &lt;2e-16 ***\ndependents_num           6.9669     2.7760   2.510   0.0124 *  \nproperty_areaSemiurban  -3.3175     6.8229  -0.486   0.6270    \nproperty_areaUrban     -10.8084     7.1188  -1.518   0.1295    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 67.1 on 574 degrees of freedom\n  (13 observations deleted due to missingness)\nMultiple R-squared:  0.4002,    Adjusted R-squared:  0.396 \nF-statistic: 95.75 on 4 and 574 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/09_explain_cats/09_explain_cats.html#interpretation-3",
    "href": "posts/09_explain_cats/09_explain_cats.html#interpretation-3",
    "title": "9: Modeling Categorical Variables",
    "section": "Interpretation",
    "text": "Interpretation\n\\[\\begin{array}{rcl}\nY & = & \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{3} + \\beta_{4}X_{4} \\\\\n~ & = & 89.1639 + 8.0575X_{1} + 6.9669X_{2} - 3.3175X_{3} - 10.8084X_{4} \\\\\n\\end{array}\\]\nFor a family with one dependent (\\(X_{2} = 1\\)) and a combined monthly income of $5000 (\\(X_{1} = 5\\)),\n\nif they are seeking a home in a rural area (\\(X_{3} = 0, X_{4} = 0\\)), the loan amount is predicted to be about 136 thousand\nif they are seeking a home in a semi-urban area (\\(X_{3} = 1, X_{4} = 0\\)), the loan amount is predicted to be about 133 thousand (i.e. 3 thousand less than the baseline rural scenario)\nif they are seeking a home in an urban area (\\(X_{3} = 0, X_{4} = 1\\)), the loan amount is predicted to be about 126 thousand (i.e. 10 thousand less than the baseline rural scenario)"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html",
    "href": "posts/10_logistic_regression/10_logistic_regression.html",
    "title": "10: Logistic Regression",
    "section": "",
    "text": "Goal: Classify Binary Responses\nObjectives:\n\ndefine classification\ncarry out logistic regression\n\n\n\n\n\n\n\n\npoint of inflection\n\n\n\n\n\n\n\n\n\n\nNotecode packages\n\n\n\n\n\n\nlibrary(\"janitor\")   #tools for data cleaning\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n\nloan_df &lt;- readr::read_csv(\"loan_data_set.csv\") |&gt;\n  janitor::clean_names()\n\n\n\n\n\n\n\n\nDescriptionScenarioResponse VariableExplanatory Variables\n\n\n\n\n“Dream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.”\n\nSource: Kaggle\n\n\n\n\n\n\n\nDream Home Finance\n\n\n\n\n\n\n“Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set.”\n\n\nWe will try to predict the loan status (i.e. categorical variable)\n\nLoan status: Yes (Y) or No (N)\n\n\n\n\nGender (of primary applicant)\nMarital status (of primary applicant)\nDependents\nEducation\nSelf-employed\nApplicant income (monthly, in dollars)\nCo-applicant income (monthly, in dollars)\nloan amount terms (in months)\nCredit history\nProperty area\n\n\n\n\n\n\n\n\nremove rows that have missing values in the response variable (loan_amount)\nconvert dependents to a numerical variable\n\nhere, replace “+” with nothing\n\ncombine “income” columns\n\nensure all dollar amounts are in the same units (thousands of dollars)\n\nconvert credit_history to a factor variable (i.e. categorical)\nretain relevant columns\n\n\nloan_df &lt;- loan_df |&gt;\n  filter(!is.na(loan_amount)) |&gt;\n  mutate(dependents_num = as.numeric(\n    str_replace(dependents, \"\\\\+\", \"\")\n  )) |&gt;\n  mutate(income = applicant_income/1000 + coapplicant_income/1000) |&gt;\n  mutate(credit_history = factor(credit_history)) |&gt;\n  select(loan_amount, income, dependents_num, gender, married, education, self_employed, credit_history, property_area, loan_status)\n\nAfter cleaning the data, we should report the size of the resultant data frame\n\nnrow(loan_df) #number of observations\n\n[1] 592\n\nncol(loan_df) #number of variables\n\n[1] 10\n\n\nand the structure of the data frame.\n\nstr(loan_df, give.attr = FALSE)\n\ntibble [592 × 10] (S3: tbl_df/tbl/data.frame)\n $ loan_amount   : num [1:592] 128 66 120 141 267 95 158 168 349 70 ...\n $ income        : num [1:592] 6.09 3 4.94 6 9.61 ...\n $ dependents_num: num [1:592] 1 0 0 0 2 0 3 2 1 2 ...\n $ gender        : chr [1:592] \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ married       : chr [1:592] \"Yes\" \"Yes\" \"Yes\" \"No\" ...\n $ education     : chr [1:592] \"Graduate\" \"Graduate\" \"Not Graduate\" \"Graduate\" ...\n $ self_employed : chr [1:592] \"No\" \"Yes\" \"No\" \"No\" ...\n $ credit_history: Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 1 2 2 2 ...\n $ property_area : chr [1:592] \"Rural\" \"Urban\" \"Urban\" \"Urban\" ...\n $ loan_status   : chr [1:592] \"N\" \"Y\" \"Y\" \"Y\" ..."
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#start",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#start",
    "title": "10: Logistic Regression",
    "section": "",
    "text": "Goal: Classify Binary Responses\nObjectives:\n\ndefine classification\ncarry out logistic regression\n\n\n\n\n\n\n\n\npoint of inflection\n\n\n\n\n\n\n\n\n\n\nNotecode packages\n\n\n\n\n\n\nlibrary(\"janitor\")   #tools for data cleaning\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\nprinceton_orange &lt;- \"#E77500\"\nprinceton_black  &lt;- \"#121212\"\n\n\nloan_df &lt;- readr::read_csv(\"loan_data_set.csv\") |&gt;\n  janitor::clean_names()"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#data",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#data",
    "title": "10: Logistic Regression",
    "section": "",
    "text": "DescriptionScenarioResponse VariableExplanatory Variables\n\n\n\n\n“Dream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.”\n\nSource: Kaggle\n\n\n\n\n\n\n\nDream Home Finance\n\n\n\n\n\n\n“Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set.”\n\n\nWe will try to predict the loan status (i.e. categorical variable)\n\nLoan status: Yes (Y) or No (N)\n\n\n\n\nGender (of primary applicant)\nMarital status (of primary applicant)\nDependents\nEducation\nSelf-employed\nApplicant income (monthly, in dollars)\nCo-applicant income (monthly, in dollars)\nloan amount terms (in months)\nCredit history\nProperty area"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#cleaning",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#cleaning",
    "title": "10: Logistic Regression",
    "section": "",
    "text": "remove rows that have missing values in the response variable (loan_amount)\nconvert dependents to a numerical variable\n\nhere, replace “+” with nothing\n\ncombine “income” columns\n\nensure all dollar amounts are in the same units (thousands of dollars)\n\nconvert credit_history to a factor variable (i.e. categorical)\nretain relevant columns\n\n\nloan_df &lt;- loan_df |&gt;\n  filter(!is.na(loan_amount)) |&gt;\n  mutate(dependents_num = as.numeric(\n    str_replace(dependents, \"\\\\+\", \"\")\n  )) |&gt;\n  mutate(income = applicant_income/1000 + coapplicant_income/1000) |&gt;\n  mutate(credit_history = factor(credit_history)) |&gt;\n  select(loan_amount, income, dependents_num, gender, married, education, self_employed, credit_history, property_area, loan_status)\n\nAfter cleaning the data, we should report the size of the resultant data frame\n\nnrow(loan_df) #number of observations\n\n[1] 592\n\nncol(loan_df) #number of variables\n\n[1] 10\n\n\nand the structure of the data frame.\n\nstr(loan_df, give.attr = FALSE)\n\ntibble [592 × 10] (S3: tbl_df/tbl/data.frame)\n $ loan_amount   : num [1:592] 128 66 120 141 267 95 158 168 349 70 ...\n $ income        : num [1:592] 6.09 3 4.94 6 9.61 ...\n $ dependents_num: num [1:592] 1 0 0 0 2 0 3 2 1 2 ...\n $ gender        : chr [1:592] \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ married       : chr [1:592] \"Yes\" \"Yes\" \"Yes\" \"No\" ...\n $ education     : chr [1:592] \"Graduate\" \"Graduate\" \"Not Graduate\" \"Graduate\" ...\n $ self_employed : chr [1:592] \"No\" \"Yes\" \"No\" \"No\" ...\n $ credit_history: Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 1 2 2 2 ...\n $ property_area : chr [1:592] \"Rural\" \"Urban\" \"Urban\" \"Urban\" ...\n $ loan_status   : chr [1:592] \"N\" \"Y\" \"Y\" \"Y\" ..."
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#one-hot-encoding",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#one-hot-encoding",
    "title": "10: Logistic Regression",
    "section": "One-Hot Encoding",
    "text": "One-Hot Encoding\n\nloan_df &lt;- loan_df |&gt;\n  mutate(approved = ifelse(loan_status == \"Y\", 1, 0),\n         approved_fac = factor(approved,\n                               levels = c(0,1)))"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#scatterplot",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#scatterplot",
    "title": "10: Logistic Regression",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nloan_df |&gt;\n  ggplot(aes(x = income, y = approved)) +\n  geom_point(aes(color = approved_fac)) +\n  geom_smooth(formula = \"y ~ x\",\n              method = \"lm\",\n              se = FALSE) +\n  labs(title = \"Dream Home Finance\",\n       subtitle = \"Linear Regression?\",\n       caption = \"SML 201\",\n       x = \"combined monthly income (thousands)\",\n       y = \"loan status\") +\n  scale_color_manual(values = c(\"gray\", \"darkgreen\")) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#logistic-function",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#logistic-function",
    "title": "10: Logistic Regression",
    "section": "Logistic Function",
    "text": "Logistic Function\n\n\n\n\n\nlogistic function\n\n\n\n\n\n\ndomain: \\((-\\infty, \\infty)\\)\nrange: \\((0,1)\\)\none-to-one and invertible\n\n\n\n\n\n\n\n\n\nWarningDCP1"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#logistic-fit",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#logistic-fit",
    "title": "10: Logistic Regression",
    "section": "Logistic Fit",
    "text": "Logistic Fit\n\nloan_df &lt;- loan_df |&gt;\n  mutate(preds6 = predict(mod6,\n                         data.frame(income = loan_df$income),\n                         type = \"response\"))\n\n\nloan_df |&gt;\n  select(income, approved_fac, preds6) |&gt;\n  head(20)"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#scatterplot-1",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#scatterplot-1",
    "title": "10: Logistic Regression",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nloan_df |&gt;\n  ggplot(aes(x = income, y = approved)) +\n  geom_point(aes(color = approved_fac)) +\n  geom_point(aes(x = income, y = preds6),\n             color = \"blue\") +\n  labs(title = \"Dream Home Finance\",\n       subtitle = \"Logistic Regression?\",\n       caption = \"SML 201\",\n       x = \"combined monthly income (thousands)\",\n       y = \"loan status\") +\n  scale_color_manual(values = c(\"gray\", \"darkgreen\")) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#metric",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#metric",
    "title": "10: Logistic Regression",
    "section": "Metric",
    "text": "Metric\n\ncutoff &lt;- median(loan_df$preds6, na.rm = TRUE)\n\nloan_df &lt;- loan_df |&gt;\n  mutate(pred_class = ifelse(preds6 &gt; cutoff, 1, 0))\n\n\nloan_df |&gt;\n  select(income, approved_fac, preds6) |&gt;\n  head(20)\n\n\nloan_df |&gt;\n  janitor::tabyl(approved_fac, pred_class)\n\n approved_fac   0   1\n            0  88  93\n            1 208 203\n\n\n\\[\\text{accuracy} = \\frac{88 + 203}{88 + 93 + 208 + 203} \\approx 0.4916\\]\nSo far, this automated system would classify the loan applications correctly about 49 percent of the time—not better than flipping a coin!\n\n# accuracy, the R way\nmean(loan_df$approved_fac == loan_df$pred_class, na.rm = TRUE)\n\n[1] 0.4915541"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#bar-plot",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#bar-plot",
    "title": "10: Logistic Regression",
    "section": "Bar Plot",
    "text": "Bar Plot\n\nloan_df |&gt;\n  ggplot(aes(x = approved_fac, )) +\n  geom_bar(aes(color = approved_fac,\n             fill = approved_fac),\n           stat = \"count\") +\n  labs(title = \"Dream Home Financial\",\n       subtitle = \"Class imbalance\",\n       caption = \"SML 201\") +\n  scale_color_manual(values = c(princeton_orange, princeton_black)) +\n  scale_fill_manual(values = c(princeton_black, princeton_orange)) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nloan_df |&gt;\n  tabyl(approved_fac) |&gt;\n  adorn_pct_formatting()\n\n approved_fac   n percent\n            0 181   30.6%\n            1 411   69.4%"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#majority-classifier",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#majority-classifier",
    "title": "10: Logistic Regression",
    "section": "Majority Classifier",
    "text": "Majority Classifier\n\nloan_df &lt;- loan_df |&gt;\n  mutate(always_approve = 1)\n\n\nloan_df |&gt;\n  select(income, approved_fac, always_approve) |&gt;\n  head(20)"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#metric-1",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#metric-1",
    "title": "10: Logistic Regression",
    "section": "Metric",
    "text": "Metric\n\nmean(loan_df$approved_fac == loan_df$always_approve)\n\n[1] 0.6942568\n\n\n\n\n\n\n\n\nNoteImproving from the Baseline\n\n\n\nIf this simple strategy is correct 69.4 percent of the time, any future machine learning model should achieve an accuracy level that is higher than 69.4 percent.\n\n\n\n\n\n\n\n\nWarningDCP2"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#logistic-fit-1",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#logistic-fit-1",
    "title": "10: Logistic Regression",
    "section": "Logistic Fit",
    "text": "Logistic Fit\n\nexplanatory_vars &lt;- c(\"income\", \"dependents_num\", \"credit_history\", \"education\", \"property_area\", \"loan_amount\")\n\nloan_df &lt;- loan_df |&gt;\n  mutate(preds7 = predict(mod7,\n                         loan_df |&gt; select(all_of(explanatory_vars)),\n                         type = \"response\"))\n\n\ncutoff &lt;- median(loan_df$preds7, na.rm = TRUE)\n\nloan_df &lt;- loan_df |&gt;\n  mutate(pred_class = ifelse(preds7 &gt; cutoff, 1, 0))"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#metric-2",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#metric-2",
    "title": "10: Logistic Regression",
    "section": "Metric",
    "text": "Metric\n\nloan_df |&gt;\n  filter(!is.na(pred_class)) |&gt;\n  janitor::tabyl(approved_fac, pred_class)\n\n approved_fac   0   1\n            0 127  36\n            1 138 229\n\n\n\\[\\text{accuracy} = \\frac{127 + 229}{127 + 36 + 138 + 229} \\approx 0.6717\\]\nSo far, this automated system would classify the loan applications correctly about 67 percent of the time\n\nbetter than random guessing\nworse than (baseline) majority classifier"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#confusion-matrics",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#confusion-matrics",
    "title": "10: Logistic Regression",
    "section": "Confusion Matrics",
    "text": "Confusion Matrics\n\ncutoff &lt;- median(loan_df$preds7, na.rm = TRUE)\n\nmodel_findings &lt;- loan_df |&gt;\n  mutate(pred_class = ifelse(preds7 &gt; cutoff, 1, 0)) |&gt;\n  select(approved_fac, pred_class) |&gt;\n  mutate(outcome = case_when(\n    approved_fac == \"1\" & pred_class == 1 ~ \"true positive\",\n    approved_fac == \"0\" & pred_class == 1 ~ \"false positive\",\n    approved_fac == \"1\" & pred_class == 0 ~ \"false negative\",\n    approved_fac == \"0\" & pred_class == 0 ~ \"true negative\",\n    TRUE ~ \"unknown classification\"\n  ))\n\n\nTP &lt;- sum(model_findings$outcome == \"true positive\")\nTN &lt;- sum(model_findings$outcome == \"true negative\")\nFP &lt;- sum(model_findings$outcome == \"false positive\")\nFN &lt;- sum(model_findings$outcome == \"false negative\")\n\n\nprint(paste0(\"There were \", TP, \" true positives\"))\n\n[1] \"There were 229 true positives\"\n\nprint(paste0(\"There were \", TN, \" true negatives\"))\n\n[1] \"There were 127 true negatives\"\n\nprint(paste0(\"There were \", FP, \" false positives\"))\n\n[1] \"There were 36 false positives\"\n\nprint(paste0(\"There were \", FN, \" false negatives\"))\n\n[1] \"There were 138 false negatives\"\n\n\n\nprecision = TP / (TP + FP)\nprint(round(precision, 2))\n\n[1] 0.86\n\n\n\nrecall = TP / (TP + FN)\nprint(round(recall, 2))\n\n[1] 0.62\n\n\n\naccuracy = mean(model_findings$approved_fac == model_findings$pred_class, na.rm = TRUE)\nprint(round(accuracy, 2))\n\n[1] 0.67"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#hyperparmeter-tuning",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#hyperparmeter-tuning",
    "title": "10: Logistic Regression",
    "section": "Hyperparmeter Tuning",
    "text": "Hyperparmeter Tuning\n\nGrid Search\nInstead of how I assumed that the cutoff theshold should be at the median of the preds values, what if we tried out many different candidates for the cutoff threshold? We proceed to compute the precision, recall, and accuracy for each candidate.\n\ncutoff_vals    &lt;- 1:99 / 100\nvec_length     &lt;- length(cutoff_vals)\nprecision_vals &lt;- rep(NA, vec_length)\nrecall_vals    &lt;- rep(NA, vec_length)\naccuracy_vals  &lt;- rep(NA, vec_length)\n\nfor(iter in 1:99){\n  cutoff &lt;- quantile(loan_df$preds7, \n                     cutoff_vals[iter],\n                     na.rm = TRUE)\n\nmodel_findings &lt;- loan_df |&gt;\n  mutate(pred_class = ifelse(preds7 &gt; cutoff, 1, 0)) |&gt;\n  select(approved_fac, pred_class) |&gt;\n  mutate(outcome = case_when(\n    approved_fac == \"1\" & pred_class == 1 ~ \"true positive\",\n    approved_fac == \"0\" & pred_class == 1 ~ \"false positive\",\n    approved_fac == \"1\" & pred_class == 0 ~ \"false negative\",\n    approved_fac == \"0\" & pred_class == 0 ~ \"true negative\",\n    TRUE ~ \"unknown classification\"\n  ))\n\nTP &lt;- sum(model_findings$outcome == \"true positive\")\nTN &lt;- sum(model_findings$outcome == \"true negative\")\nFP &lt;- sum(model_findings$outcome == \"false positive\")\nFN &lt;- sum(model_findings$outcome == \"false negative\")\n\nprecision_vals[iter] = TP / (TP + FP)\nrecall_vals[iter] = TP / (TP + FN)\naccuracy_vals[iter] = mean(\n  model_findings$approved_fac == model_findings$pred_class, \n  na.rm = TRUE)\n}\n\n\n\nDiagnostic Graphs\n\ndata.frame(cutoff_vals, precision_vals, recall_vals) |&gt;\n  ggplot() +\n  geom_line(aes(x = cutoff_vals, precision_vals),\n            color = \"blue\", linewidth = 2) +\n  geom_line(aes(x = cutoff_vals, recall_vals),\n            color = \"red\", linewidth = 2) +\n  labs(title = \"Diagnostic Graphs\",\n       subtitle = \"Precision in blue, recall in red\",\n       caption = \"SML 201\",\n       x = \"cutoff value candidates\",\n       y = \"metric value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndata.frame(cutoff_vals, precision_vals, recall_vals) |&gt;\n  ggplot() +\n  geom_line(aes(x = cutoff_vals, accuracy_vals),\n            color = princeton_orange, linewidth = 2) +\n  labs(title = \"Diagnostic Graphs\",\n       subtitle = \"Seeking highest accuracy\",\n       caption = \"SML 201\",\n       x = \"cutoff value candidates\",\n       y = \"metric value\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#optimum",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#optimum",
    "title": "10: Logistic Regression",
    "section": "Optimum",
    "text": "Optimum\n\noptimal_cutoff &lt;- cutoff_vals[which.max(accuracy_vals)]"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#best-model",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#best-model",
    "title": "10: Logistic Regression",
    "section": "Best Model",
    "text": "Best Model\n\ncutoff &lt;- optimal_cutoff\n\nloan_df &lt;- loan_df |&gt;\n  mutate(pred_class = ifelse(preds7 &gt; cutoff, 1, 0))\n\n\nloan_df |&gt;\n  filter(!is.na(pred_class)) |&gt;\n  janitor::tabyl(approved_fac, pred_class)\n\n approved_fac  0   1\n            0 65  98\n            1  7 360\n\n\n\\[\\text{accuracy} = \\frac{65 + 360}{65 + 98 + 7 + 360} \\approx 0.8019\\]\n\n#accuracy\nmean(loan_df$approved_fac == loan_df$pred_class,\n     na.rm = TRUE)\n\n[1] 0.8018868\n\n\n\nThis optimal model achieved about 80 percent accuracy\n\nbetter than the baseline model\nwith nearly instant calculations\n\n\n\n\n\n\n\n\nWarningDCP3"
  },
  {
    "objectID": "posts/10_logistic_regression/10_logistic_regression.html#beyond-the-binary",
    "href": "posts/10_logistic_regression/10_logistic_regression.html#beyond-the-binary",
    "title": "10: Logistic Regression",
    "section": "Beyond the Binary",
    "text": "Beyond the Binary\n\n\n\n\n\n\nTipWhat if the categorical response has more than two levels?\n\n\n\n\n\nIn later machine learning classes, you will encounter\n\nsupport vector machines\nrandom forests\n\nTake SML 301 next semester!"
  }
]