{
  "hash": "49c4c56213e3e110600e970adf345317",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"23: One More Case Study\"\nauthor: \"Derek Sollberger\"\ndate: \"2024-12-03\"\nformat:\n  html:\n    toc: true\n    theme: cerulean\n---\n\n\n\n# SML 201\n\n## Start\n\n::: {.callout-note collapse=\"true\"}\n## Libraries and Loading the Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"ggsignif\")  #show significance levels on boxplots\nlibrary(\"patchwork\") #arrange plots\nlibrary(\"tidyclust\") #tidymodels clustering\nlibrary(\"tidymodels\")#framework for machine learning in R\nlibrary(\"tidyverse\") #general framework for data wrangling\n\n# school colors\nprinceton_orange <- \"#E77500\"\nprinceton_black  <- \"#121212\"\n\nset.seed(201)\ncontrol_data <- data.frame(\n  age = sample(18:65, size = 100, replace = TRUE),\n  height = round(rnorm(100, 65, 8)),\n  sex = sample(c(\"female\", \"male\"), size = 100, replace = TRUE),\n  group = \"control\",\n  weight_before = round(rnorm(100, 150, 15))\n) |>\n  #coded like Gaussian white noise\n  mutate(weight_after = round(weight_before - rnorm(100, 0, 6)))\n\ntreatment_data <- data.frame(\n  age = sample(18:65, size = 100, replace = TRUE),\n  height = round(rnorm(100, 65, 8)),\n  sex = sample(c(\"female\", \"male\"), size = 100, replace = TRUE),\n  group = \"treatment\",\n  weight_before = round(rnorm(100, 150, 15))\n) |>\n  #coded like Gaussian white noise\n  mutate(weight_after = round(weight_before - rnorm(100, 15, 6)))\n\nGLP_raw <- rbind(control_data, treatment_data) |>\n  sample_frac() |>\n  mutate(patient_id = 1:200, .before = \"age\")\n\n# helper function\nvnorm <- function(x, mu = 0, sigma = 1, section = \"lower\"){\n  \n  # bell curve\n  x_vals <- seq(mu - 4*sigma, mu + 4*sigma, length.out = 201)\n  y_vals <- dnorm(x_vals, mu, sigma)\n  df_for_graph <- data.frame(x_vals, y_vals)\n\n  # outline shaded regions\n  if(length(x) == 1){\n    shade_left <- rbind(c(x[1],0), df_for_graph |>\n                        filter(x_vals < x[1]))\n    shade_right <- rbind(c(x[1],0), df_for_graph |>\n                        filter(x_vals > x[1]))\n  }\n  if(length(x) == 2){\n    shade_between <- rbind(c(x[1],0),\n                       df_for_graph |>\n                         filter(x_vals > x[1] &\n                                  x_vals < x[2]),\n                       c(x[2],0))\n    shade_tails <- rbind(df_for_graph |>\n                        filter(x_vals < x[1]),\n                     c(x[1],0),\n                     c(x[2],0),\n                     df_for_graph |>\n                        filter(x_vals > x[2]))\n  }\n  if(section == \"lower\"){\n    bell_curve <- df_for_graph |>\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_left,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val <- round(pnorm(x,mu,sigma), 4)\n  }\n  if(section == \"upper\"){\n    bell_curve <- df_for_graph |>\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_right,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val <- 1 - round(pnorm(x,mu,sigma), 4)\n  }\n  if(section == \"between\"){\n    bell_curve <- df_for_graph |>\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_between,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val <- round(diff(pnorm(x,mu,sigma)), 4)\n  }\n  if(section == \"tails\"){\n    bell_curve <- df_for_graph |>\n      ggplot(aes(x_vals, y_vals)) +\n      geom_polygon(aes(x = x_vals, y = y_vals),\n                   data = shade_tails,\n                   fill = \"#E77500\",) +\n      geom_line(color = \"gray50\", linewidth = 2)\n    prob_val <- round(1 - diff(pnorm(x,mu,sigma)), 4)\n  }\n  \n  # plot bell curve\n  bell_curve + \n    labs(subtitle = paste0(\"Probability: \", prob_val),\n         caption = \"SML 201\", y = \"\") +\n    theme_minimal()\n}\n```\n:::\n\n\n\n:::\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n* **Goal**: Take on one more case study\n\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"40%\"}\n* **Objectives**: \n\n- probability\n- confidence intervals\n- hypothesis testing\n- machine learning\n\n:::\n\n::::\n\n## Data\n\n> \"GLP [Glucagon-like peptide-1] agonists are medications that help lower blood sugar levels and promote weight loss.\" --- Cleveland Clinic\n\nWe are going to take hypothetical data and pretend that we are analyzing results from clinical trials in hopes of obtaining approval from the Food and Drug Administration.\n\n\n# Data Preparation\n\n* add a column for\n\n$$\\text{weight loss} = \\text{weight before} - \\text{weight after}$$\n\n* remove the `patient_id` column\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGLP_df <- GLP_raw |>\n  mutate(weight_loss = weight_before - weight_after) |>\n  select(-patient_id)\n```\n:::\n\n\n\n# Probability\n\nAssuming a normal distribution, what is the probability that a randomly selected person in the treatment group lost at least 10 pounds?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGLP_df |>\n  filter(group == \"treatment\") |>\n  summarize(xbar = mean(weight_loss),\n            s = sd(weight_loss))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  xbar        s\n1 16.1 5.823151\n```\n\n\n:::\n\n```{.r .cell-code}\npnorm(10, 16.1, 5.8232, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8525733\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvnorm(10, 16.1, 5.8232, section = \"upper\") +\n  labs(title = \"GLP Case Study\",\n       subtitle = \"probability of losing at least 10 pounds\",\n       caption = \"SML 201\\n(hypothetical data)\",\n       x = \"weight loss (pounds)\")\n```\n\n::: {.cell-output-display}\n![](23_case_study_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n# Confidence Intervals\n\nUse `infer::visualize` to construct confidence intervals for `weight_loss` for both the control and treatment groups.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrap_distribution <- GLP_df |>\n  filter(group == \"control\") |>\n  specify(response = weight_loss) |>\n  generate(reps = 1337, type = \"bootstrap\") |>\n  calculate(stat = \"mean\")\n\nbootstrap_distribution |>\n  visualize() +\n  shade_ci(endpoints = bootstrap_distribution |>\n             get_ci(level = 0.95, type = \"percentile\"),\n           color = princeton_black,\n           fill = princeton_orange) +\n  labs(title = \"GLP Clinical Trials\",\n       subtitle = \"control group\",\n       caption = \"(hypothetical data)\",\n       x = \"weight loss\", y = \"number of patients\") +\n  theme_minimal(base_size = 16)\n```\n\n::: {.cell-output-display}\n![](23_case_study_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrap_distribution <- GLP_df |>\n  filter(group == \"treatment\") |>\n  specify(response = weight_loss) |>\n  generate(reps = 1337, type = \"bootstrap\") |>\n  calculate(stat = \"mean\")\n\nbootstrap_distribution |>\n  visualize() +\n  shade_ci(endpoints = bootstrap_distribution |>\n             get_ci(level = 0.95, type = \"percentile\"),\n           color = princeton_black,\n           fill = princeton_orange) +\n  labs(title = \"GLP Clinical Trials\",\n       subtitle = \"treatment group\",\n       caption = \"(hypothetical data)\",\n       x = \"weight loss\", y = \"number of patients\") +\n  theme_minimal(base_size = 16)\n```\n\n::: {.cell-output-display}\n![](23_case_study_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n# Hypothesis Testing\n\n## Null Distribution\n\nUse `infer` package code to perform NHST to test the claim (among those in the treatment group)\n\n* null hypothesis: male patients have at least as much weight loss than female patients\n* alternative hypothesis: male patients have less weight loss than female patients\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# build a null distribution\nnull_distribution <- GLP_df |>\n  filter(group == \"treatment\") |>\n  specify(formula = weight_loss ~ sex) |> \n  hypothesize(null = \"independence\") |> \n  generate(reps = 1000, type = \"permute\") |> \n  calculate(stat = \"diff in means\", order = c(\"male\", \"female\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute the observed difference in means\nobs_diff_means <- GLP_df |>\n  filter(group == \"treatment\") |>\n  specify(formula = weight_loss ~ sex) |> \n  calculate(stat = \"diff in means\", order = c(\"male\", \"female\"))\n  \n# visualize the null distribution and shade in the p-value\nnull_distribution |>\nvisualize(bins = 10) + \n  shade_p_value(obs_stat = obs_diff_means, direction = \"less\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](23_case_study_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_distribution |>\n  get_p_value(obs_stat = obs_diff_means, direction = \"less\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 1\n  p_value\n    <dbl>\n1   0.118\n```\n\n\n:::\n:::\n\n\n\n## Boxplots\n\nCreate a side-by-side boxplot to conduct a two-sided NHST for weight loss between the control and treatment groups.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGLP_df |>\n  ggplot(aes(x = group, y = weight_loss,\n             fill = group)) +\n  geom_boxplot() +\n  geom_signif(\n    comparisons = list(c(\"control\", \"treatment\")),\n    map_signif_level = TRUE\n  ) +\n  labs(title = \"GLP Clinical Trials\",\n       subtitle = \"(toward publisable results)\",\n       caption = \"(hypothetical data)\",\n       x = \"\", y = \"weight loss (pounds)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](23_case_study_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n# Machine Learning\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGLP_ml <- GLP_df |>\n  select(-c(weight_before, weight_after))\n\ndata_split <- initial_split(GLP_ml)\ndata_train <- training(data_split)\ndata_fold <- vfold_cv(data_train)\nlambda_grid <- grid_regular(penalty(range = c(-3, 3)),\n                            levels = 50)\n```\n:::\n\n\n\n## Ridge Regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nml_pipeline_fit <- linear_reg(mixture = 0, penalty = 0) |> \n  set_engine(\"glmnet\") |>\n  fit(weight_loss ~ ., data = GLP_ml)\nml_pipeline_recipe <- recipe(weight_loss ~ ., data = GLP_ml) |>\n  step_novel(all_nominal_predictors()) |>\n  step_dummy(all_nominal_predictors()) |> \n  step_zv(all_predictors()) |>\n  step_normalize(all_numeric_predictors())\nml_pipeline_spec <- linear_reg(mixture = 0, penalty = tune()) |>\n  set_engine(\"glmnet\")\nml_pipeline_workflow <- workflow() |> \n  add_recipe(ml_pipeline_recipe) |>\n  add_model(ml_pipeline_spec)\n\nml_pipeline_grid <- tune_grid(\n  ml_pipeline_workflow,\n  resamples = data_fold, \n  grid = lambda_grid\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_penalty <- select_best(ml_pipeline_grid, \n                            metric = \"rmse\") |>\n  pull(penalty)\npredictions <- predict(ml_pipeline_fit, \n                       new_data = GLP_df |>\n                         select(-weight_loss),\n                       penalty = best_penalty)\ntrue_values <- GLP_df  |> select(weight_loss)\nn <- GLP_df  |> drop_na(weight_loss) |> nrow()\nRMSE <- sqrt((1/n)*sum((true_values - predictions)^2))\nprint(RMSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.090254\n```\n\n\n:::\n:::\n\n\n\n## Lasso Regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nml_pipeline_fit <- linear_reg(mixture = 1, penalty = 0) |> \n  set_engine(\"glmnet\") |>\n  fit(weight_loss ~ ., data = GLP_ml)\nml_pipeline_recipe <- recipe(weight_loss ~ ., data = GLP_ml) |>\n  step_novel(all_nominal_predictors()) |>\n  step_dummy(all_nominal_predictors()) |> \n  step_zv(all_predictors()) |>\n  step_normalize(all_numeric_predictors())\nml_pipeline_spec <- linear_reg(mixture = 1, penalty = tune()) |>\n  set_engine(\"glmnet\")\nml_pipeline_workflow <- workflow() |> \n  add_recipe(ml_pipeline_recipe) |>\n  add_model(ml_pipeline_spec)\n\nml_pipeline_grid <- tune_grid(\n  ml_pipeline_workflow,\n  resamples = data_fold, \n  grid = lambda_grid\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_penalty <- select_best(ml_pipeline_grid, \n                            metric = \"rmse\") |>\n  pull(penalty)\npredictions <- predict(ml_pipeline_fit, \n                       new_data = GLP_df |>\n                         select(-weight_loss),\n                       penalty = best_penalty)\ntrue_values <- GLP_df  |> select(weight_loss)\nn <- GLP_df  |> drop_na(weight_loss) |> nrow()\nRMSE <- sqrt((1/n)*sum((true_values - predictions)^2))\nprint(RMSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.063561\n```\n\n\n:::\n:::\n\n\n\n\n## Clustering\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cross-validation folds\nml_pipeline_cv <- vfold_cv(GLP_ml, v = 5)\n\n# specification\nml_pipeline_spec <- k_means(num_clusters = tune())\n\n# recipe\nml_pipeline_recipe <- recipe(~weight_loss, data = GLP_ml) |>\n  step_zv() |>\n  step_normalize()\n\n# workflow\nml_pipeline_workflow <- workflow(ml_pipeline_recipe, ml_pipeline_spec)\n\n# parameter grid\nkvals_grid <- grid_regular(num_clusters(), levels = 10)\n\n# tuning\ntune_results <- tune_cluster(\n  ml_pipeline_workflow,\n  resamples = ml_pipeline_cv,\n  grid = kvals_grid,\n  metrics = cluster_metric_set(sse_within_total, sse_total, sse_ratio)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_results |>\n  collect_metrics() |>\n  filter(.metric == \"sse_ratio\") |>\n  ggplot(aes(x = num_clusters, y = mean)) +\n  geom_point(aes(x = factor(num_clusters), y = mean),\n             size = 3) +\n  geom_line() +\n  labs(title = \"Scree Plot\",\n       subtitle = \"Aiming to choose the number of clusters\",\n       caption = \"SML 201\",\n       x = \"number of clusters\",\n       y = \"within/total SSE ratio\") +\n  theme_minimal(base_size = 14)\n```\n\n::: {.cell-output-display}\n![](23_case_study_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n\n### Visualization\n\nCreate a side-by-side arrangements of scatterplots\n\n* horizontal: `weight_before`\n* vertical: `weight_after`\n\n1. color coded by `group`\n2. color coded by cluster assignment\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_fit <- k_means(num_clusters = 2) |>\n  set_engine(\"stats\") |>\n  fit(~ weight_loss, data = GLP_df)\nGLP_clust <- GLP_df |>\n  mutate(cluster_num = extract_cluster_assignment(cluster_fit) |>\n           pull())\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- GLP_clust |>\n  ggplot(aes(x = weight_before, y = weight_after)) +\n  geom_point(aes(color = group),\n             alpha = 0.5, size = 3) +\n  labs(title = \"GLP Clinical Trials\",\n       subtitle = \"experiment groups\",\n       x = \"weight before (pounds)\",\n       y = \"weight after (pounds)\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np2 <- GLP_clust |>\n  ggplot(aes(x = weight_before, y = weight_after)) +\n  geom_point(aes(color = cluster_num),\n             alpha = 0.5, size = 3) +\n  labs(title = \"GLP Clinical Trials\",\n       subtitle = \"k = 2 clusters\",\n       x = \"weight before (pounds)\",\n       y = \"weight after (pounds)\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#patchwork\np1 + p2\n```\n\n::: {.cell-output-display}\n![](23_case_study_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\n\n# Project 4\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* ridesharing\n\n    * Chicago Data Portal\n    * September 2024\n\n* areas: business, accounting\n* theme: machine learning\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![](ridesharing.png)\n:::\n\n::::\n\n\n# SML 301\n\n> Data Intelligence: Modern Data Science Methods \n\n1. **Eir**: movement tracking for women athletes\n2. Hospital pricing: predict surgery prices\n3. **AEC**: automating document scanning classification\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n* Exam 2 (December 5)\n* Project 4 (due December 11)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n\n\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n## (optional) Additional Resources\n\n* more information about [GLP medications](https://my.clevelandclinic.org/health/treatments/13901-glp-1-agonists) from Cleveland Clinic\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.1 (2024-06-14 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] glmnet_4.1-8       Matrix_1.7-0       lubridate_1.9.3    forcats_1.0.0     \n [5] stringr_1.5.1      readr_2.1.5        tidyverse_2.0.0    yardstick_1.3.1   \n [9] workflowsets_1.1.0 workflows_1.1.4    tune_1.2.1         tidyr_1.3.1       \n[13] tibble_3.2.1       rsample_1.2.1      recipes_1.0.10     purrr_1.0.2       \n[17] parsnip_1.2.1      modeldata_1.4.0    infer_1.0.7        ggplot2_3.5.1     \n[21] dplyr_1.1.4        dials_1.3.0        scales_1.3.0       broom_1.0.7       \n[25] tidymodels_1.2.0   tidyclust_0.2.3    patchwork_1.3.0    ggsignif_0.6.4    \n\nloaded via a namespace (and not attached):\n [1] rlang_1.1.4         magrittr_2.0.3      furrr_0.3.1        \n [4] flexclust_1.4-2     compiler_4.4.1      vctrs_0.6.5        \n [7] RcppZiggurat_0.1.6  lhs_1.2.0           pkgconfig_2.0.3    \n[10] shape_1.4.6.1       fastmap_1.2.0       backports_1.5.0    \n[13] ellipsis_0.3.2      labeling_0.4.3      utf8_1.2.4         \n[16] rmarkdown_2.29      prodlim_2024.06.25  tzdb_0.4.0         \n[19] xfun_0.48           Rfast_2.1.0         modeltools_0.2-23  \n[22] jsonlite_1.8.8      parallel_4.4.1      R6_2.5.1           \n[25] stringi_1.8.4       parallelly_1.37.1   rpart_4.1.23       \n[28] Rcpp_1.0.12         iterators_1.0.14    knitr_1.49         \n[31] future.apply_1.11.2 splines_4.4.1       nnet_7.3-19        \n[34] timechange_0.3.0    tidyselect_1.2.1    rstudioapi_0.17.1  \n[37] yaml_2.3.8          timeDate_4032.109   codetools_0.2-20   \n[40] listenv_0.9.1       lattice_0.22-6      withr_3.0.2        \n[43] evaluate_1.0.1      future_1.33.2       survival_3.6-4     \n[46] RcppParallel_5.1.9  pillar_1.9.0        foreach_1.5.2      \n[49] stats4_4.4.1        generics_0.1.3      hms_1.1.3          \n[52] munsell_0.5.1       globals_0.16.3      class_7.3-22       \n[55] glue_1.8.0          tools_4.4.1         data.table_1.16.2  \n[58] modelenv_0.2.0      gower_1.0.1         grid_4.4.1         \n[61] ipred_0.9-14        colorspace_2.1-1    cli_3.6.3          \n[64] DiceDesign_1.10     fansi_1.0.6         lava_1.8.0         \n[67] gtable_0.3.6        GPfit_1.0-8         digest_0.6.35      \n[70] htmlwidgets_1.6.4   farver_2.1.2        htmltools_0.5.8.1  \n[73] lifecycle_1.0.4     hardhat_1.4.0       MASS_7.3-60.2      \n```\n\n\n:::\n:::\n\n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Example Callout Block\n\n`note`, `tip`, `warning`, `caution`, or `important`\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [
      "23_case_study_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}