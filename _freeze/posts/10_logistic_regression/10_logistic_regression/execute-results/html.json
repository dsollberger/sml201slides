{
  "hash": "1fbab82feace040bcea893a5575ff298",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"10: Logistic Regression\"\nauthor: \"Derek Sollberger\"\ndate: \"2026-02-26\"\nformat:\n  html:\n    toc: true\n    theme: cerulean\n---\n\n\n# SML 201\n\n## Start\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* **Goal**: Classify Binary Responses\n\n* **Objectives**: \n\n  * define classification\n  * carry out logistic regression\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![point of inflection](logistic_function.png)\n:::\n\n::::\n\n::: {.callout-note collapse=\"true\"}\n## code packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"janitor\")   #tools for data cleaning\nlibrary(\"tidyverse\") #tools for data wrangling and visualization\n\nprinceton_orange <- \"#E77500\"\nprinceton_black  <- \"#121212\"\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloan_df <- readr::read_csv(\"loan_data_set.csv\") |>\n  janitor::clean_names()\n```\n:::\n\n:::\n\n## Data\n\n::::: {.panel-tabset}\n\n## Description\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\"Dream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.\"\t\n\n* Source: [Kaggle](https://www.kaggle.com/datasets/burak3ergun/loan-data-set)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\n![Dream Home Finance](Dream_Home_Finance.png)\n:::\n\n::::\n\n## Scenario\n\n\"Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set.\"\n\n## Response Variable\n\nWe will try to predict the loan status (i.e. *categorical* variable)\n\n* Loan status: Yes (Y) or No (N)\n\n## Explanatory Variables\n\n* Gender (of primary applicant)\n* Marital status (of primary applicant)\n* Dependents \n* Education\n* Self-employed\n* Applicant income (monthly, in dollars)\n* Co-applicant income (monthly, in dollars)\n* loan amount terms (in months)\n* Credit history\n* Property area\n\n:::::\n\n## Cleaning\n\n* remove rows that have missing values in the response variable (`loan_amount`)\n* convert `dependents` to a numerical variable\n\n    * here, replace \"+\" with nothing\n\n* combine \"income\" columns\n\n    * ensure all dollar amounts are in the same units (thousands of dollars)\n\n* convert `credit_history` to a factor variable (i.e. categorical)\n\n* retain relevant columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloan_df <- loan_df |>\n  filter(!is.na(loan_amount)) |>\n  mutate(dependents_num = as.numeric(\n    str_replace(dependents, \"\\\\+\", \"\")\n  )) |>\n  mutate(income = applicant_income/1000 + coapplicant_income/1000) |>\n  mutate(credit_history = factor(credit_history)) |>\n  select(loan_amount, income, dependents_num, gender, married, education, self_employed, credit_history, property_area, loan_status)\n```\n:::\n\n\nAfter cleaning the data, we should report the size of the resultant data frame\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnrow(loan_df) #number of observations\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 592\n```\n\n\n:::\n\n```{.r .cell-code}\nncol(loan_df) #number of variables\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10\n```\n\n\n:::\n:::\n\n\nand the structure of the data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(loan_df, give.attr = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [592 Ã— 10] (S3: tbl_df/tbl/data.frame)\n $ loan_amount   : num [1:592] 128 66 120 141 267 95 158 168 349 70 ...\n $ income        : num [1:592] 6.09 3 4.94 6 9.61 ...\n $ dependents_num: num [1:592] 1 0 0 0 2 0 3 2 1 2 ...\n $ gender        : chr [1:592] \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ married       : chr [1:592] \"Yes\" \"Yes\" \"Yes\" \"No\" ...\n $ education     : chr [1:592] \"Graduate\" \"Graduate\" \"Not Graduate\" \"Graduate\" ...\n $ self_employed : chr [1:592] \"No\" \"Yes\" \"No\" \"No\" ...\n $ credit_history: Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 1 2 2 2 ...\n $ property_area : chr [1:592] \"Rural\" \"Urban\" \"Urban\" \"Urban\" ...\n $ loan_status   : chr [1:592] \"N\" \"Y\" \"Y\" \"Y\" ...\n```\n\n\n:::\n:::\n\n\n\n# Categorical Response\n\nAs implied at the beginning, we want to now shift our goal to *classifying* whether or not a loan application was approved.\n\n::: {.callout-tip}\n## Supervised Machine Learning\n\nIf the response variable is ...\n\n* numerical $\\rightarrow$ **regression** task\n* categorical $\\rightarrow$ **classification** task\n:::\n\n## One-Hot Encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloan_df <- loan_df |>\n  mutate(approved = ifelse(loan_status == \"Y\", 1, 0),\n         approved_fac = factor(approved,\n                               levels = c(0,1)))\n```\n:::\n\n\n## Scatterplot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloan_df |>\n  ggplot(aes(x = income, y = approved)) +\n  geom_point(aes(color = approved_fac)) +\n  geom_smooth(formula = \"y ~ x\",\n              method = \"lm\",\n              se = FALSE) +\n  labs(title = \"Dream Home Finance\",\n       subtitle = \"Linear Regression?\",\n       caption = \"SML 201\",\n       x = \"combined monthly income (thousands)\",\n       y = \"loan status\") +\n  scale_color_manual(values = c(\"gray\", \"darkgreen\")) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10_logistic_regression_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Logistic Function\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![logistic function](logistic_function.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* domain: $(-\\infty, \\infty)$\n* range: $(0,1)$\n* one-to-one and invertible\n:::\n\n::::\n\n::: {.callout-warning}\n# DCP1\n:::\n\n# Logistic Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generalized linear model\nmod6 <- glm(approved_fac ~ income,\n            data = loan_df,\n            family = \"binomial\")\n```\n:::\n\n\n## Logistic Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloan_df <- loan_df |>\n  mutate(preds6 = predict(mod6,\n                         data.frame(income = loan_df$income),\n                         type = \"response\"))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloan_df |>\n  select(income, approved_fac, preds6) |>\n  head(20)\n```\n:::\n\n\n\n## Scatterplot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloan_df |>\n  ggplot(aes(x = income, y = approved)) +\n  geom_point(aes(color = approved_fac)) +\n  geom_point(aes(x = income, y = preds6),\n             color = \"blue\") +\n  labs(title = \"Dream Home Finance\",\n       subtitle = \"Logistic Regression?\",\n       caption = \"SML 201\",\n       x = \"combined monthly income (thousands)\",\n       y = \"loan status\") +\n  scale_color_manual(values = c(\"gray\", \"darkgreen\")) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10_logistic_regression_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n## Metric\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutoff <- median(loan_df$preds6, na.rm = TRUE)\n\nloan_df <- loan_df |>\n  mutate(pred_class = ifelse(preds6 > cutoff, 1, 0))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloan_df |>\n  select(income, approved_fac, preds6) |>\n  head(20)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloan_df |>\n  janitor::tabyl(approved_fac, pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n approved_fac   0   1\n            0  88  93\n            1 208 203\n```\n\n\n:::\n:::\n\n\n$$\\text{accuracy} = \\frac{88 + 203}{88 + 93 + 208 + 203} \\approx 0.4916$$\n\nSo far, this automated system would classify the loan applications correctly about 49 percent of the time---not better than flipping a coin!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# accuracy, the R way\nmean(loan_df$approved_fac == loan_df$pred_class, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4915541\n```\n\n\n:::\n:::\n\n\n\n# Baseline Model\n\nWhen forming a classification task, we advise setting a baseline model that simply predicts the most common category.\n\n## Bar Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloan_df |>\n  ggplot(aes(x = approved_fac, )) +\n  geom_bar(aes(color = approved_fac,\n             fill = approved_fac),\n           stat = \"count\") +\n  labs(title = \"Dream Home Financial\",\n       subtitle = \"Class imbalance\",\n       caption = \"SML 201\") +\n  scale_color_manual(values = c(princeton_orange, princeton_black)) +\n  scale_fill_manual(values = c(princeton_black, princeton_orange)) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](10_logistic_regression_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloan_df |>\n  tabyl(approved_fac) |>\n  adorn_pct_formatting()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n approved_fac   n percent\n            0 181   30.6%\n            1 411   69.4%\n```\n\n\n:::\n:::\n\n\n## Majority Classifier\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloan_df <- loan_df |>\n  mutate(always_approve = 1)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloan_df |>\n  select(income, approved_fac, always_approve) |>\n  head(20)\n```\n:::\n\n\n## Metric\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(loan_df$approved_fac == loan_df$always_approve)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6942568\n```\n\n\n:::\n:::\n\n\n::: {.callout-note}\n## Improving from the Baseline\n\nIf this simple strategy is correct 69.4 percent of the time, any future machine learning model should achieve an accuracy level that is higher than 69.4 percent.\n:::\n\n::: {.callout-warning}\n# DCP2\n:::\n\n# Extended Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generalized linear model\nmod7 <- glm(approved_fac ~ income + dependents_num + credit_history +\n              education + property_area + loan_amount,\n            data = loan_df,\n            family = \"binomial\")\n```\n:::\n\n\n## Logistic Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexplanatory_vars <- c(\"income\", \"dependents_num\", \"credit_history\", \"education\", \"property_area\", \"loan_amount\")\n\nloan_df <- loan_df |>\n  mutate(preds7 = predict(mod7,\n                         loan_df |> select(all_of(explanatory_vars)),\n                         type = \"response\"))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutoff <- median(loan_df$preds7, na.rm = TRUE)\n\nloan_df <- loan_df |>\n  mutate(pred_class = ifelse(preds7 > cutoff, 1, 0))\n```\n:::\n\n\n## Metric\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloan_df |>\n  filter(!is.na(pred_class)) |>\n  janitor::tabyl(approved_fac, pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n approved_fac   0   1\n            0 127  36\n            1 138 229\n```\n\n\n:::\n:::\n\n\n$$\\text{accuracy} = \\frac{127 + 229}{127 + 36 + 138 + 229} \\approx 0.6717$$\n\nSo far, this automated system would classify the loan applications correctly about 67 percent of the time\n\n* better than random guessing\n* worse than (baseline) majority classifier\n\n\n# Hyperparameters\n\n::: {.callout-note}\n## Parameter Terminology\n\n* **parameters** are values (such as regression coefficients) that are determined from machine learning training\n* **hyperparameters** are values that are manually set by the machine learning analyst\n:::\n\n## Confusion Matrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutoff <- median(loan_df$preds7, na.rm = TRUE)\n\nmodel_findings <- loan_df |>\n  mutate(pred_class = ifelse(preds7 > cutoff, 1, 0)) |>\n  select(approved_fac, pred_class) |>\n  mutate(outcome = case_when(\n    approved_fac == \"1\" & pred_class == 1 ~ \"true positive\",\n    approved_fac == \"0\" & pred_class == 1 ~ \"false positive\",\n    approved_fac == \"1\" & pred_class == 0 ~ \"false negative\",\n    approved_fac == \"0\" & pred_class == 0 ~ \"true negative\",\n    TRUE ~ \"unknown classification\"\n  ))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTP <- sum(model_findings$outcome == \"true positive\")\nTN <- sum(model_findings$outcome == \"true negative\")\nFP <- sum(model_findings$outcome == \"false positive\")\nFN <- sum(model_findings$outcome == \"false negative\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(paste0(\"There were \", TP, \" true positives\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"There were 229 true positives\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste0(\"There were \", TN, \" true negatives\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"There were 127 true negatives\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste0(\"There were \", FP, \" false positives\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"There were 36 false positives\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste0(\"There were \", FN, \" false negatives\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"There were 138 false negatives\"\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecision = TP / (TP + FP)\nprint(round(precision, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.86\n```\n\n\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecall = TP / (TP + FN)\nprint(round(recall, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.62\n```\n\n\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy = mean(model_findings$approved_fac == model_findings$pred_class, na.rm = TRUE)\nprint(round(accuracy, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.67\n```\n\n\n:::\n:::\n\n\n\n## Hyperparmeter Tuning\n\n### Grid Search\n\nInstead of how I assumed that the `cutoff` theshold should be at the median of the `preds` values, what if we tried out *many* different candidates for the `cutoff` threshold?  We proceed to compute the precision, recall, and accuracy for each candidate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutoff_vals    <- 1:99 / 100\nvec_length     <- length(cutoff_vals)\nprecision_vals <- rep(NA, vec_length)\nrecall_vals    <- rep(NA, vec_length)\naccuracy_vals  <- rep(NA, vec_length)\n\nfor(iter in 1:99){\n  cutoff <- quantile(loan_df$preds7, \n                     cutoff_vals[iter],\n                     na.rm = TRUE)\n\nmodel_findings <- loan_df |>\n  mutate(pred_class = ifelse(preds7 > cutoff, 1, 0)) |>\n  select(approved_fac, pred_class) |>\n  mutate(outcome = case_when(\n    approved_fac == \"1\" & pred_class == 1 ~ \"true positive\",\n    approved_fac == \"0\" & pred_class == 1 ~ \"false positive\",\n    approved_fac == \"1\" & pred_class == 0 ~ \"false negative\",\n    approved_fac == \"0\" & pred_class == 0 ~ \"true negative\",\n    TRUE ~ \"unknown classification\"\n  ))\n\nTP <- sum(model_findings$outcome == \"true positive\")\nTN <- sum(model_findings$outcome == \"true negative\")\nFP <- sum(model_findings$outcome == \"false positive\")\nFN <- sum(model_findings$outcome == \"false negative\")\n\nprecision_vals[iter] = TP / (TP + FP)\nrecall_vals[iter] = TP / (TP + FN)\naccuracy_vals[iter] = mean(\n  model_findings$approved_fac == model_findings$pred_class, \n  na.rm = TRUE)\n}\n```\n:::\n\n\n### Diagnostic Graphs\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(cutoff_vals, precision_vals, recall_vals) |>\n  ggplot() +\n  geom_line(aes(x = cutoff_vals, precision_vals),\n            color = \"blue\", linewidth = 2) +\n  geom_line(aes(x = cutoff_vals, recall_vals),\n            color = \"red\", linewidth = 2) +\n  labs(title = \"Diagnostic Graphs\",\n       subtitle = \"Precision in blue, recall in red\",\n       caption = \"SML 201\",\n       x = \"cutoff value candidates\",\n       y = \"metric value\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10_logistic_regression_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(cutoff_vals, precision_vals, recall_vals) |>\n  ggplot() +\n  geom_line(aes(x = cutoff_vals, accuracy_vals),\n            color = princeton_orange, linewidth = 2) +\n  labs(title = \"Diagnostic Graphs\",\n       subtitle = \"Seeking highest accuracy\",\n       caption = \"SML 201\",\n       x = \"cutoff value candidates\",\n       y = \"metric value\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10_logistic_regression_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n## Optimum\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimal_cutoff <- cutoff_vals[which.max(accuracy_vals)]\n```\n:::\n\n\n## Best Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutoff <- optimal_cutoff\n\nloan_df <- loan_df |>\n  mutate(pred_class = ifelse(preds7 > cutoff, 1, 0))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloan_df |>\n  filter(!is.na(pred_class)) |>\n  janitor::tabyl(approved_fac, pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n approved_fac  0   1\n            0 65  98\n            1  7 360\n```\n\n\n:::\n:::\n\n\n$$\\text{accuracy} = \\frac{65 + 360}{65 + 98 + 7 + 360} \\approx 0.8019$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#accuracy\nmean(loan_df$approved_fac == loan_df$pred_class,\n     na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8018868\n```\n\n\n:::\n:::\n\n\n* This optimal model achieved about 80 percent accuracy\n\n    * better than the baseline model\n    * with nearly instant calculations\n\n::: {.callout-warning}\n# DCP3\n:::\n\n# Predictions\n\n::::: {.panel-tabset}\n\n## Applicant 1\n\nPicture a local resident who makes \\$5k per month (without a college education), has three dependents, has no credit history,  and is seeking a \\$250k loan for a house in a rural area.  Our model says ...\n\n\n::: {.cell}\n\n```{.r .cell-code}\napplicant_1 <- data.frame(income = 5,\n                          dependents_num = 3,\n                          credit_history = \"0\",\n                          education = \"Not Graduate\",\n                          property_area = \"Rural\",\n                          loan_amount = 250)\n\nthis_prediction <- predict(mod7,\n                          applicant_1,\n                          type = \"response\")\n\nprint(paste0(\"The computer model says that we should \",\n             ifelse(this_prediction > optimal_cutoff, \"approve\", \"reject\"),\n             \" this application.\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"The computer model says that we should reject this application.\"\n```\n\n\n:::\n:::\n\n\n## Applicant 2\n\nPicture a Princeton graduate who makes \\$10k per month, has two dependents, has good credit,  and is seeking a \\$500k loan for a house in an urban area.  Our model says ...\n\n\n::: {.cell}\n\n```{.r .cell-code}\napplicant_2 <- data.frame(income = 10,\n                          dependents_num = 2,\n                          credit_history = \"1\",\n                          education = \"Graduate\",\n                          property_area = \"Urban\",\n                          loan_amount = 500)\n\nthis_prediction <- predict(mod7,\n                          applicant_2,\n                          type = \"response\")\n\nprint(paste0(\"The computer model says that we should \",\n             ifelse(this_prediction > optimal_cutoff, \"approve\", \"reject\"),\n             \" this application.\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"The computer model says that we should approve this application.\"\n```\n\n\n:::\n:::\n\n\n:::::\n\n\n## Beyond the Binary\n\n::: {.callout-tip collapse=\"true\"}\n## What if the categorical response has more than two levels?\n\nIn later machine learning classes, you will encounter\n\n* support vector machines\n* random forests\n\nTake SML 301 next semester!\n:::\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* This week: \n\n  * Precept 5\n  * Read Chapter 6\n\n* Exam 1: March 5\n* Refer to weekly announcement for more info\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n![Cloak of Wisdom](cloak_of_wisdom.png)\n\n* image source: [Up and Out Comics](https://www.juliakaye.com/chbeaqv6vde7qjpcppullvaooox6gj)\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n## (optional) Additional Resources\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.5.2 (2025-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [5] purrr_1.1.0     readr_2.1.5     tidyr_1.3.1     tibble_3.3.0   \n [9] ggplot2_4.0.0   tidyverse_2.0.0 janitor_2.2.1  \n\nloaded via a namespace (and not attached):\n [1] generics_0.1.4     lattice_0.22-7     stringi_1.8.7      hms_1.1.3         \n [5] digest_0.6.37      magrittr_2.0.3     evaluate_1.0.4     grid_4.5.2        \n [9] timechange_0.3.0   RColorBrewer_1.1-3 fastmap_1.2.0      Matrix_1.7-4      \n[13] jsonlite_2.0.0     mgcv_1.9-3         scales_1.4.0       cli_3.6.5         \n[17] rlang_1.1.6        crayon_1.5.3       splines_4.5.2      bit64_4.6.0-1     \n[21] withr_3.0.2        yaml_2.3.10        tools_4.5.2        parallel_4.5.2    \n[25] tzdb_0.5.0         vctrs_0.6.5        R6_2.6.1           lifecycle_1.0.4   \n[29] snakecase_0.11.1   htmlwidgets_1.6.4  bit_4.6.0          vroom_1.6.5       \n[33] pkgconfig_2.0.3    pillar_1.11.0      gtable_0.3.6       glue_1.8.0        \n[37] xfun_0.52          tidyselect_1.2.1   rstudioapi_0.17.1  knitr_1.50        \n[41] farver_2.1.2       nlme_3.1-168       htmltools_0.5.8.1  labeling_0.4.3    \n[45] rmarkdown_2.29     compiler_4.5.2     S7_0.2.0          \n```\n\n\n:::\n:::\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [
      "10_logistic_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}