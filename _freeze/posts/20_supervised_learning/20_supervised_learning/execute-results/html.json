{
  "hash": "8cc6a87b5e3d24228bbfb0a1acfe8ed7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"20: Supervised Learning\"\nauthor: \"Derek Sollberger\"\n\ndate: \"2024-11-19\"\nformat:\n  html:\n    toc: true\n    theme: cerulean\n---\n\n\n\n# SML 201\n\n## Start\n\n::: {.callout-note collapse=\"true\"}\n## Libraries and Loading the Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"corrplot\")\nlibrary(\"tidyclust\")\nlibrary(\"tidymodels\")\nlibrary(\"tidyverse\")\n\n# school colors\nprinceton_orange <- \"#E77500\"\nprinceton_black  <- \"#121212\"\n\n# data set: GPT detectors\noffice_raw <- readr::read_csv('office_sentiment.csv')\n# office_raw <- readr::read_csv(\"https://raw.githubusercontent.com/dsollberger/sml201slides/main/data/office_sentiment.csv\")\n```\n:::\n\n\n\n:::\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n* **Goal**: Explore topics in supervised learning\n\n\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"40%\"}\n* **Objectives**: \n\n- `tidymodels`\n- ridge regression\n- LASSO regression\n:::\n\n::::\n\n\n# Case Study: Sentiment in The Office\n\nThe data set this week come from the `schrute` package, and it features the scripts from the popular TV show *The Office*.  I have augmented the data with sentiment analyses scores from three algorithms\n\n1. `syuzhet`\n2. `sentimentr`\n3. `sentimentAnalysis`\n\nEach row of the data frame is a spoken line of dialogue.  From there, the data has the following variables:\n\n* `index`: to easily refer to a particular row in the data\n* `season`: The Office ran for 9 seasons\n* `episode`: the episode number within a season\n* `episode_name`\n* `director`\n* `writer`\n* `character`: the fictional character of the TV show\n* `text`: the spoken text in the line of dialogue\n* `text_w_direction`: ... includes stage direction\n* `imdb_rating`: Internet Movie Database rating for the episode (from 0 to 10, which 10 being the highest)\n* `total_votes`: number of IMdB users that voted on that episode\n* `air_date`\n* `sentimentAnalysis_score`\n* `sentimentr_score`\n* `syuzhet_score`\n\n::: {.callout-note}\n## Research Question\n\nIn this session, we will define **cringe** as the standard deviation of a sentiment analysis score. Are the `imdb_rating` values explained by the *cringe* of the episodes?\n:::\n\n## Filtering\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noffice_df <- office_raw |>\n  select(imdb_rating,\n         season, episode, total_votes,\n         sentimentAnalysis_score, sentimentr_score, syuzhet_score) |>\n  rename(ep_in_season = episode,\n         sent1 = syuzhet_score,\n         sent2 = sentimentr_score,\n         sent3 = sentimentAnalysis_score)\n```\n:::\n\n\n\n## Derived Variables\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noffice_df <- office_df |>\n  group_by(season, ep_in_season) |>\n  mutate(sent1_mean = mean(sent1, na.rm = TRUE),\n         sent2_mean = mean(sent2, na.rm = TRUE),\n         sent3_mean = mean(sent3, na.rm = TRUE),\n         sent1_dev = sd(sent1, na.rm = TRUE),\n         sent2_dev = sd(sent2, na.rm = TRUE),\n         sent3_dev = sd(sent3, na.rm = TRUE)) |>\n  ungroup() |>\n  select(imdb_rating,\n         sent1_mean, sent2_mean, sent3_mean,\n         sent1_dev, sent2_dev, sent3_dev,\n         season, ep_in_season, total_votes) |>\n  distinct() |>\n  mutate(episode_num = 1:n())\n```\n:::\n\n\n\n## Correlation Plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noffice_df |>\n  cor(use = \"pairwise.complete.obs\") |>\n  corrplot(method = \"number\",\n           order = \"FPC\",\n           type = \"upper\")\n```\n\n::: {.cell-output-display}\n![](20_supervised_learning_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n# Linear Regression\n\n* model equation:\n\n$$\\hat{y} = \\beta_{0} + \\sum_{j=1}^{k} \\beta_{j}X_{j}$$\n* loss function:\n\n$$L(\\vec{\\beta}) = \\text{argmin}_{\\vec{\\beta}} \\sum_{i=1}^{N} \\left(y_{i} - \\beta_{0} - \\sum_{j=1}^{k} \\beta_{j}x_{ij}\\right)$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlin_fit <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(imdb_rating ~ ., data = office_df)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(lin_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 × 5\n   term          estimate std.error statistic  p.value\n   <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)   5.92     0.619         9.55  1.14e-17\n 2 sent1_mean   -0.311    0.520        -0.598 5.50e- 1\n 3 sent2_mean   -0.857    1.18         -0.726 4.69e- 1\n 4 sent3_mean    0.167    1.31          0.127 8.99e- 1\n 5 sent1_dev    -0.236    0.401        -0.589 5.57e- 1\n 6 sent2_dev     0.975    1.20          0.814 4.16e- 1\n 7 sent3_dev     0.685    1.06          0.645 5.20e- 1\n 8 season        0.713    0.143         4.97  1.58e- 6\n 9 ep_in_season  0.0400   0.00630       6.35  1.82e- 9\n10 total_votes   0.000428 0.0000388    11.0   8.24e-22\n11 episode_num  -0.0338   0.00644      -5.25  4.34e- 7\n```\n\n\n:::\n:::\n\n\n\n$$\\text{RSME} = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_{i} - \\hat{y})^{2}}{n-k-1}}$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions <- predict(lin_fit, \n                       new_data = office_df |>\n                         select(-imdb_rating))\ntrue_values <- office_df |> select(imdb_rating)\nn <- office_df |> drop_na(imdb_rating) |> nrow()\nk <- ncol(office_df) - 1\nRMSE <- sqrt((1/(n-k-1))*sum((true_values - predictions)^2))\nprint(RMSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3604571\n```\n\n\n:::\n:::\n\n\n\n\n::: {.callout-warning}\n## Size of regression coefficients?\n\nDo the size of the regression coefficients ($\\beta_{i}$) matter?  Are the sizes of the regression coefficients related to the units of the measurements?\n:::\n\n\n# Ridge Regression\n\n* loss function:\n\n$$L(\\vec{\\beta}, \\lambda) = \\text{argmin}_{\\vec{\\beta}} \\left[\\sum_{i=1}^{N} \\left(y_{i} - \\beta_{0} - \\sum_{j=1}^{k} \\beta_{j}x_{ij}\\right) + \\lambda\\sum_{j=i}^{k}\\beta_{j}^{2}\\right]$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge_fit <- linear_reg(mixture = 0, penalty = 0) |>\n  set_engine(\"glmnet\") |>\n  fit(imdb_rating ~ ., data = office_df)\n```\n:::\n\n\n\n## Penalties\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(ridge_fit, penalty = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'Matrix'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoaded glmnet 4.1-8\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 × 3\n   term          estimate penalty\n   <chr>            <dbl>   <dbl>\n 1 (Intercept)   7.48           0\n 2 sent1_mean   -0.336          0\n 3 sent2_mean   -0.911          0\n 4 sent3_mean   -0.247          0\n 5 sent1_dev    -0.684          0\n 6 sent2_dev     1.32           0\n 7 sent3_dev     0.846          0\n 8 season        0.00749        0\n 9 ep_in_season  0.0144         0\n10 total_votes   0.000371       0\n11 episode_num  -0.00222        0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(ridge_fit, penalty = 201)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 × 3\n   term            estimate penalty\n   <chr>              <dbl>   <dbl>\n 1 (Intercept)   8.25           201\n 2 sent1_mean    0.0000141      201\n 3 sent2_mean    0.00537        201\n 4 sent3_mean   -0.000168       201\n 5 sent1_dev    -0.000994       201\n 6 sent2_dev     0.0118         201\n 7 sent3_dev     0.00446        201\n 8 season       -0.000234       201\n 9 ep_in_season  0.0000353      201\n10 total_votes   0.00000117     201\n11 episode_num  -0.0000102      201\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nridge_fit |> autoplot()\n```\n\n::: {.cell-output-display}\n![](20_supervised_learning_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n## Predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(ridge_fit,\n        new_data = office_df |> select(-imdb_rating),\n        penalty = 0) |>\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 1\n  .pred\n  <dbl>\n1  8.75\n2  8.68\n3  8.50\n4  8.36\n5  8.67\n6  8.65\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(ridge_fit,\n        new_data = office_df |> select(-imdb_rating),\n        penalty = 201) |>\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 1\n  .pred\n  <dbl>\n1  8.25\n2  8.25\n3  8.25\n4  8.25\n5  8.25\n6  8.25\n```\n\n\n:::\n:::\n\n\n\n## Lambda Grid\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda_vals <- 10^seq(-3,3, length.out = 10)\nformat(lambda_vals, scientific=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"   0.001000000\" \"   0.004641589\" \"   0.021544347\" \"   0.100000000\"\n [5] \"   0.464158883\" \"   2.154434690\" \"  10.000000000\" \"  46.415888336\"\n [9] \" 215.443469003\" \"1000.000000000\"\n```\n\n\n:::\n\n```{.r .cell-code}\nlambda_grid <- grid_regular(penalty(range = c(-3, 3)),\n                            levels = 50)\n```\n:::\n\n\n\n### Data Split\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noffice_split <- initial_split(office_df, strata = \"imdb_rating\")\noffice_train <- training(office_split)\noffice_test  <- testing(office_split)\noffice_fold  <- vfold_cv(office_train, v = 10)\n```\n:::\n\n\n\n### Preprocessing\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge_recipe <- recipe(formula = imdb_rating ~ ., \n                       data = office_train) |>\n  step_novel(all_nominal_predictors()) |>\n  step_dummy(all_nominal_predictors()) |>\n  step_zv(all_predictors())\n```\n:::\n\n\n\n### Specification\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge_spec <- linear_reg(mixture = 0, penalty = tune()) |>\n  set_engine(\"glmnet\")\n```\n:::\n\n\n\n### Workflow\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge_workflow <- workflow() |>\n  add_recipe(ridge_recipe) |>\n  add_model(ridge_spec)\n```\n:::\n\n\n\n### Tuning\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_results <- tune_grid(\n  ridge_workflow,\n  resamples = office_fold,\n  grid = lambda_grid\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_results |> autoplot()\n```\n\n::: {.cell-output-display}\n![](20_supervised_learning_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_results |> collect_metrics() |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1 0.001   rmse    standard   0.375    10  0.0358 Preprocessor1_Model01\n2 0.001   rsq     standard   0.498    10  0.0956 Preprocessor1_Model01\n3 0.00133 rmse    standard   0.375    10  0.0358 Preprocessor1_Model02\n4 0.00133 rsq     standard   0.498    10  0.0956 Preprocessor1_Model02\n5 0.00176 rmse    standard   0.375    10  0.0358 Preprocessor1_Model03\n6 0.00176 rsq     standard   0.498    10  0.0956 Preprocessor1_Model03\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# tune_results |>\n#   collect_metrics() |>\n#   filter(.metric == \"rmse\") |>\n#   filter(mean == min(mean))\n```\n:::\n\n\n\n## Predictions Refined\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# best_lambda <- tune_results |>\n#   collect_metrics() |>\n#   filter(.metric == \"rmse\") |>\n#   filter(mean == min(mean)) |>\n#   distinct() |>\n#   pull(penalty)\n\nbest_lambda <- select_best(tune_results, metric = \"rmse\") |>\n  pull(penalty)\n\npredict(ridge_fit,\n        new_data = office_df |> select(-imdb_rating),\n        penalty = best_lambda) |>\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 1\n  .pred\n  <dbl>\n1  8.74\n2  8.67\n3  8.52\n4  8.37\n5  8.66\n6  8.65\n```\n\n\n:::\n:::\n\n\n\n\n# LASSO Regression\n\n* **L**east **A**bsolute **S**hrinkage and **S**election **O**perator regression\n* loss function:\n\n$$L(\\vec{\\beta}, \\lambda) = \\text{argmin}_{\\vec{\\beta}} \\left[\\sum_{i=1}^{N} \\left(y_{i} - \\beta_{0} - \\sum_{j=1}^{k} \\beta_{j}x_{ij}\\right) + \\lambda\\sum_{j=i}^{k}|\\beta_{j}|\\right]$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_fit <- linear_reg(mixture = 1, penalty = 0) |>\n  set_engine(\"glmnet\") |>\n  fit(imdb_rating ~ ., data = office_df)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_fit |> autoplot()\n```\n\n::: {.cell-output-display}\n![](20_supervised_learning_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n\n## Coefficients\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(lasso_fit, penalty = 1e-2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 × 3\n   term          estimate penalty\n   <chr>            <dbl>   <dbl>\n 1 (Intercept)   7.62        0.01\n 2 sent1_mean   -0.308       0.01\n 3 sent2_mean   -0.453       0.01\n 4 sent3_mean    0           0.01\n 5 sent1_dev    -0.650       0.01\n 6 sent2_dev     0.852       0.01\n 7 sent3_dev     0.449       0.01\n 8 season        0           0.01\n 9 ep_in_season  0.0135      0.01\n10 total_votes   0.000387    0.01\n11 episode_num  -0.00172     0.01\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(lasso_fit, penalty = 1e-1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 × 3\n   term           estimate penalty\n   <chr>             <dbl>   <dbl>\n 1 (Intercept)   7.59          0.1\n 2 sent1_mean    0             0.1\n 3 sent2_mean    0             0.1\n 4 sent3_mean    0             0.1\n 5 sent1_dev     0             0.1\n 6 sent2_dev     0             0.1\n 7 sent3_dev     0             0.1\n 8 season        0             0.1\n 9 ep_in_season  0             0.1\n10 total_votes   0.000314      0.1\n11 episode_num  -0.0000509     0.1\n```\n\n\n:::\n:::\n\n\n\n## Lambda Grid\n### Preprocessing\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_recipe <- recipe(formula = imdb_rating ~ ., \n                       data = office_train) |>\n  step_novel(all_nominal_predictors()) |>\n  step_dummy(all_nominal_predictors()) |>\n  step_zv(all_predictors()) |>\n  step_normalize()\n```\n:::\n\n\n\n### Specification\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_spec <- linear_reg(mixture = 1, penalty = tune()) |>\n  set_engine(\"glmnet\")\n```\n:::\n\n\n\n### Workflow\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_workflow <- workflow() |>\n  add_recipe(lasso_recipe) |>\n  add_model(lasso_spec)\n```\n:::\n\n\n\n### Tuning\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda_grid <- grid_regular(penalty(range = c(-2, -1)),\n                            levels = 25)\n\ntune_results <- tune_grid(\n  lasso_workflow,\n  resamples = office_fold,\n  grid = lambda_grid\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_results |> autoplot()\n```\n\n::: {.cell-output-display}\n![](20_supervised_learning_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n\n## Explanatory Variables\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_results |>\n  collect_metrics() |>\n  filter(.metric == \"rmse\") |>\n  filter(mean == min(mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1  0.0215 rmse    standard   0.371    10  0.0392 Preprocessor1_Model09\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_lambda <- select_best(tune_results, metric = \"rmse\") |>\n  pull(penalty)\n\ntidy(lasso_fit, penalty = best_lambda)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 × 3\n   term          estimate penalty\n   <chr>            <dbl>   <dbl>\n 1 (Intercept)   7.74      0.0215\n 2 sent1_mean   -0.162     0.0215\n 3 sent2_mean    0         0.0215\n 4 sent3_mean    0         0.0215\n 5 sent1_dev    -0.547     0.0215\n 6 sent2_dev     0.444     0.0215\n 7 sent3_dev     0.0297    0.0215\n 8 season        0         0.0215\n 9 ep_in_season  0.0117    0.0215\n10 total_votes   0.000376  0.0215\n11 episode_num  -0.00149   0.0215\n```\n\n\n:::\n:::\n\n\n\n\n# Principal Components Regression\n\n::: {.callout-tip collapse=\"true\"}\n## PCA\n\n**Principal Component Analysis** (PCA) is a popular algorithm for *dimensionality reduction*.\n:::\n\n## Preprocessing\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_recipe <- \n  recipe(formula = imdb_rating ~ ., data = office_train) |> \n  step_normalize(all_predictors()) |>\n  step_pca(all_predictors(), threshold = tune())\n```\n:::\n\n\n\n## Specification\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_spec <- \n  linear_reg() |>\n  set_engine(\"lm\")\n```\n:::\n\n\n\n## Workflow\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_workflow <- \n  workflow() |>\n  add_recipe(pca_recipe) |>\n  add_model(lm_spec)\n```\n:::\n\n\n\n## Tuning\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda_grid <- grid_regular(threshold(),\n                            levels = 20)\n\ntune_results <- tune_grid(\n  pca_workflow,\n  resamples = office_fold,\n  grid = lambda_grid\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_results |> autoplot()\n```\n\n::: {.cell-output-display}\n![](20_supervised_learning_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_threshold <- select_best(tune_results, metric = \"rmse\")\npca_refined <- finalize_workflow(pca_workflow, best_threshold)\npca_fit <- fit(pca_refined, data = office_train)\n```\n:::\n\n\n\n## Visualization\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflat_data <- pca_recipe <- \n  recipe(formula = imdb_rating ~ ., data = office_train) |> \n  step_normalize(all_predictors()) |>\n  step_pca(all_predictors()) |> \n  prep() |>\n  juice() |>\n  select(imdb_rating, PC1, PC2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkmeans_fit <- k_means(num_clusters = 5) |>\n  set_engine(\"stats\") |>\n  fit(imdb_rating ~ PC1 + PC2, data = flat_data)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nflat_data |>\n  mutate(cluster_num = extract_cluster_assignment(kmeans_fit) |>\n           pull()) |>\n  ggplot(aes(x = PC1, y = PC2, color = cluster_num)) +\n  geom_point() +\n  labs(title = \"The Office\",\n       subtitle = \"inputs projected onto first two principal components\",\n       caption = \"Source: Schrute package\",\n       x = \"principal component 1\", y = \"principal component 2\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](20_supervised_learning_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n\n\n\n# Precept 10\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* MLB players from 1986\n* explanatory variables: offense stats for hitters\n* predict 1987 salaries\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![Meet the Mets!](Mets_1986.png)\n:::\n\n::::\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n* Precept 10\n* Project 3 (Due Nov 20)\n* CLO Assessment\n* Exam 2 (December 5)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n![Ted Lasso](Ted_Lasso.png)\n\n* TODO: watch *Ted Lasso* to generate nerdy jokes\n\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n## (optional) Additional Resources\n\n* [ISLR tidymodels labs](https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/) by Emil Hvitfeldt\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.1 (2024-06-14 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] glmnet_4.1-8       Matrix_1.7-0       lubridate_1.9.3    forcats_1.0.0     \n [5] stringr_1.5.1      readr_2.1.5        tidyverse_2.0.0    yardstick_1.3.1   \n [9] workflowsets_1.1.0 workflows_1.1.4    tune_1.2.1         tidyr_1.3.1       \n[13] tibble_3.2.1       rsample_1.2.1      recipes_1.0.10     purrr_1.0.2       \n[17] parsnip_1.2.1      modeldata_1.4.0    infer_1.0.7        ggplot2_3.5.1     \n[21] dplyr_1.1.4        dials_1.3.0        scales_1.3.0       broom_1.0.7       \n[25] tidymodels_1.2.0   tidyclust_0.2.3    corrplot_0.94     \n\nloaded via a namespace (and not attached):\n [1] rlang_1.1.4         magrittr_2.0.3      furrr_0.3.1        \n [4] compiler_4.4.1      vctrs_0.6.5         lhs_1.2.0          \n [7] pkgconfig_2.0.3     shape_1.4.6.1       crayon_1.5.3       \n[10] fastmap_1.2.0       ellipsis_0.3.2      backports_1.5.0    \n[13] labeling_0.4.3      utf8_1.2.4          rmarkdown_2.28     \n[16] prodlim_2024.06.25  tzdb_0.4.0          bit_4.5.0          \n[19] xfun_0.48           jsonlite_1.8.8      parallel_4.4.1     \n[22] R6_2.5.1            stringi_1.8.4       parallelly_1.37.1  \n[25] rpart_4.1.23        Rcpp_1.0.12         iterators_1.0.14   \n[28] knitr_1.48          future.apply_1.11.2 splines_4.4.1      \n[31] nnet_7.3-19         timechange_0.3.0    tidyselect_1.2.1   \n[34] rstudioapi_0.17.0   yaml_2.3.8          timeDate_4032.109  \n[37] codetools_0.2-20    listenv_0.9.1       lattice_0.22-6     \n[40] withr_3.0.2         evaluate_1.0.1      archive_1.1.8      \n[43] future_1.33.2       survival_3.6-4      pillar_1.9.0       \n[46] foreach_1.5.2       generics_0.1.3      vroom_1.6.5        \n[49] hms_1.1.3           munsell_0.5.1       globals_0.16.3     \n[52] class_7.3-22        glue_1.8.0          tools_4.4.1        \n[55] data.table_1.16.2   modelenv_0.2.0      gower_1.0.1        \n[58] grid_4.4.1          ipred_0.9-14        colorspace_2.1-1   \n[61] cli_3.6.3           DiceDesign_1.10     fansi_1.0.6        \n[64] lava_1.8.0          gtable_0.3.5        GPfit_1.0-8        \n[67] digest_0.6.35       ggrepel_0.9.6       htmlwidgets_1.6.4  \n[70] farver_2.1.2        htmltools_0.5.8.1   lifecycle_1.0.4    \n[73] hardhat_1.4.0       bit64_4.5.2         MASS_7.3-60.2      \n```\n\n\n:::\n:::\n\n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Example Callout Block\n\n`note`, `tip`, `warning`, `caution`, or `important`\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [
      "20_supervised_learning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}