{
  "hash": "cd1dabb8ed66fb43e8a9da751899d461",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"18: Introduction to Machine Learning\"\nauthor: \"Derek Sollberger\"\ndate: \"2024-11-12\"\nformat:\n  html:\n    toc: true\n    theme: cerulean\n---\n\n\n\n# SML 201\n\n## Start\n\n::: {.callout-note collapse=\"true\"}\n## Libraries and Helper Functions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"ggtext\")\nlibrary(\"tidymodels\")\nlibrary(\"tidyverse\")\n\n# school colors\nprinceton_orange <- \"#E77500\"\nprinceton_black  <- \"#121212\"\n\n# data set: Tour de France\ntdf_winners <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-04-07/tdf_winners.csv')\n```\n:::\n\n\n\n:::\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n* **Goal**: Introduce machine learning (ideas and terminology)\n\n\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"40%\"}\n* **Objectives**: \n\n- introduce `tidymodels` package\n- practice with a `TidyTuesday` data set\n:::\n\n::::\n\n\n## Exploratory Data Analyses\n\n::::: {.panel-tabset}\n\n## structure\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(tdf_winners, give.attr = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nspc_tbl_ [106 × 19] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ edition      : num [1:106] 1 2 3 4 5 6 7 8 9 10 ...\n $ start_date   : Date[1:106], format: \"1903-07-01\" \"1904-07-02\" ...\n $ winner_name  : chr [1:106] \"Maurice Garin\" \"Henri Cornet\" \"Louis Trousselier\" \"René Pottier\" ...\n $ winner_team  : chr [1:106] \"La Française\" \"Conte\" \"Peugeot–Wolber\" \"Peugeot–Wolber\" ...\n $ distance     : num [1:106] 2428 2428 2994 4637 4488 ...\n $ time_overall : num [1:106] 94.6 96.1 NA NA NA ...\n $ time_margin  : num [1:106] 2.99 2.27 NA NA NA ...\n $ stage_wins   : num [1:106] 3 1 5 5 2 5 6 4 2 3 ...\n $ stages_led   : num [1:106] 6 3 10 12 5 13 13 3 13 13 ...\n $ height       : num [1:106] 1.62 NA NA NA NA NA 1.78 NA NA NA ...\n $ weight       : num [1:106] 60 NA NA NA NA NA 88 NA NA NA ...\n $ age          : num [1:106] 32 19 24 27 24 25 22 22 26 23 ...\n $ born         : Date[1:106], format: \"1871-03-03\" \"1884-08-04\" ...\n $ died         : Date[1:106], format: \"1957-02-19\" \"1941-03-18\" ...\n $ full_name    : chr [1:106] NA NA NA NA ...\n $ nickname     : chr [1:106] \"The Little Chimney-sweep\" \"Le rigolo (The joker)\" \"Levaloy / Trou-trou\" NA ...\n $ birth_town   : chr [1:106] \"Arvier\" \"Desvres\" \"Paris\" \"Moret-sur-Loing\" ...\n $ birth_country: chr [1:106] \"Italy\" \"France\" \"France\" \"France\" ...\n $ nationality  : chr [1:106] \" France\" \" France\" \" France\" \" France\" ...\n```\n\n\n:::\n:::\n\n\n\n## colnames\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(tdf_winners)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"edition\"       \"start_date\"    \"winner_name\"   \"winner_team\"  \n [5] \"distance\"      \"time_overall\"  \"time_margin\"   \"stage_wins\"   \n [9] \"stages_led\"    \"height\"        \"weight\"        \"age\"          \n[13] \"born\"          \"died\"          \"full_name\"     \"nickname\"     \n[17] \"birth_town\"    \"birth_country\" \"nationality\"  \n```\n\n\n:::\n:::\n\n\n\n:::::\n\n::::: {.panel-tabset}\n\n## plot\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 41 rows containing missing values or values outside the scale range\n(`geom_text()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](18_intro_ml_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n## code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntdf_winners |>\n  mutate(year = edition + 1904) |>\n  ggplot() +\n  # geom_point(aes(x = height, y = time_overall),\n  #            color = \"blue\") +\n  geom_text(aes(x = height, y = time_overall,\n                label = year), color = \"blue\") +\n  labs(title = \"Are taller bicyclists faster?\",\n       subtitle = \"featuring Tour de France winners\",\n       caption = \"Source: TidyTuesday\",\n       x = \"height (meters)\",\n       y = \"time (hours)\") +\n  theme_minimal()\n```\n:::\n\n\n\n:::::\n\n# Cleaning Data\n\nSometimes we like to perform some *preprocessing* of the data.  In this example, we will \n\n* focus on the champions that were more athletic than in the early years.\n* focus on biker `pace` (response variable) as the route changes from year to year\n\n::::: {.panel-tabset}\n\n## filters\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_df <- tdf_winners |>\n  select(distance, time_overall, \n           height, weight, age) |>\n  drop_na() |>\n  filter(height >= 1.7) |>\n  mutate(pace = distance / time_overall) |>\n  select(pace, height, weight, age)\n```\n:::\n\n\n\n## dimensions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(bike_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 62  4\n```\n\n\n:::\n:::\n\n\n\n\n## glimpse\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(bike_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n   pace height weight   age\n  <dbl>  <dbl>  <dbl> <dbl>\n1  31.6   1.72     66    23\n2  33.4   1.72     66    33\n3  32.1   1.77     68    29\n4  32.2   1.77     68    30\n5  34.6   1.79     75    26\n6  33.2   1.79     75    29\n```\n\n\n:::\n:::\n\n\n\n:::::\n\n# Multiple Predictor Variables\n\n::::: {.panel-tabset}\n\n### Height\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18_intro_ml_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n### Age\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18_intro_ml_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n### Weight\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18_intro_ml_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n### R code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_df |>\n  ggplot(aes(x = height, y = pace)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_smooth(method = \"lm\", linewidth = 3,\n              se = FALSE, color = \"red\") +\n  labs(title = \"Are taller bicyclists faster?\",\n       subtitle = \"featuring Tour de France winners\",\n       caption = \"Source: TidyTuesday\",\n       x = \"height (meters)\",\n       y = \"pace (km/hr)\") +\n  theme_minimal()\n\nbike_df |>\n  ggplot(aes(x = age, y = pace)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_smooth(method = \"lm\", linewidth = 3,\n              se = FALSE, color = \"red\") +\n  labs(title = \"Are older bicyclists faster?\",\n       subtitle = \"featuring Tour de France winners\",\n       caption = \"Source: TidyTuesday\",\n       x = \"age\",\n       y = \"pace (km/hr)\") +\n  theme_minimal()\n\nbike_df |>\n  ggplot(aes(x = weight, y = pace)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_smooth(method = \"lm\", linewidth = 3,\n              se = FALSE, color = \"red\") +\n  labs(title = \"Are heavier bicyclists faster?\",\n       subtitle = \"featuring Tour de France winners\",\n       caption = \"Source: TidyTuesday\",\n       x = \"weight (kg)\",\n       y = \"pace (km/hr)\") +\n  theme_minimal()\n```\n:::\n\n\n\n:::::\n\n# Regression via TidyModels\n\n::::: {.panel-tabset}\n\n### Start\n\n“With tidymodels, we start by specifying the functional form of the model that we want using the parsnip package.”\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\n\n### Model Engine\n\n“However, now that the type of model has been specified, a method for fitting or training the model can be stated using the engine. The engine value is often a mash-up of the software that can be used to fit or train the model as well as the estimation method.”\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |> \n  set_engine(\"lm\") #linear model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\n\n### Fitting a Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit <- linear_reg() |> \n  set_engine(\"lm\") |>\n  fit(pace ~ height + weight + age, data = bike_df)\nlm_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = pace ~ height + weight + age, data = data)\n\nCoefficients:\n(Intercept)       height       weight          age  \n     3.8455      21.0987      -0.1387       0.2113  \n```\n\n\n:::\n:::\n\n\n\n### Examining a Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(lm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    3.85    12.3        0.313  0.755 \n2 height        21.1      8.06       2.62   0.0112\n3 weight        -0.139    0.0685    -2.03   0.0474\n4 age            0.211    0.0979     2.16   0.0350\n```\n\n\n:::\n:::\n\n\n\nObserve where we have p-values < 0.05\n\n:::::\n\n\n# Interaction Terms\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit_with_interaction <- linear_reg() |> \n  set_engine(\"lm\") |>\n  fit(pace ~ height + weight + age + height:weight + height:age + weight:age +\n        height:weight:age,\n      data = bike_df)\nlm_fit_with_interaction\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = pace ~ height + weight + age + height:weight + \n    height:age + weight:age + height:weight:age, data = data)\n\nCoefficients:\n      (Intercept)             height             weight                age  \n         924.8499          -444.1560           -15.6339           -27.8628  \n    height:weight         height:age         weight:age  height:weight:age  \n           7.9297            13.9352             0.4802            -0.2425  \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(lm_fit_with_interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 × 5\n  term              estimate std.error statistic p.value\n  <chr>                <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        925.     2272.        0.407   0.686\n2 height            -444.     1287.       -0.345   0.731\n3 weight             -15.6      32.8      -0.477   0.635\n4 age                -27.9      80.3      -0.347   0.730\n5 height:weight        7.93     18.5       0.428   0.670\n6 height:age          13.9      45.5       0.306   0.761\n7 weight:age           0.480     1.16      0.414   0.680\n8 height:weight:age   -0.243     0.656    -0.370   0.713\n```\n\n\n:::\n:::\n\n\n\n::: {.callout-warning collapse=\"true\"}\n## Overfitting\n\nDid you notice that even though we added more terms to the model, we did not reach significance in any of the variables? This may be foreshadowing of *overfitting*.\n:::\n\n\n# Prediction\n\n::::: {.panel-tabset}\n\n### New Data\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n* SpongeBob is a 26-year-old, 1.77 m tall bicyclist who weighs 55 kg\n* Patrick is a 25-year-old, 1.81 m tall bicyclist who weighs 75 kg\n* Squidward is a 31-year-old, 1.89 m tall bicyclist who weighs 65 kg\n\t\n:::\n\n::: {.column width=\"50%\"}\n![](spongebob_patrick_squidward.png)\n:::\n\n::::\n\n### Predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_contestants <- data.frame(name = c(\"SpongeBob\", \"Patrick\", \"Squidward\"),\n                              age = c(26, 25, 31),\n                              height = c(1.77, 1.81, 1.89),\n                              weight = c(55, 75, 65))\nmean_predictions <- predict(lm_fit, new_data = new_contestants)\nmean_predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 1\n  .pred\n  <dbl>\n1  39.1\n2  36.9\n3  41.3\n```\n\n\n:::\n:::\n\n\n\n### Confidence Intervals\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCI_predictions <- predict(lm_fit,\n                          new_data = new_contestants,\n                          type = \"conf_int\")\nCI_predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  .pred_lower .pred_upper\n        <dbl>       <dbl>\n1        37.1        41.0\n2        35.9        38.0\n3        39.0        43.5\n```\n\n\n:::\n:::\n\n\n\n### Error Bars\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_for_plot <- new_contestants |>\n  bind_cols(mean_predictions) |>\n  bind_cols(CI_predictions)\ndf_for_plot\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       name age height weight    .pred .pred_lower .pred_upper\n1 SpongeBob  26   1.77     55 39.05386    37.07966    41.02807\n2   Patrick  25   1.81     75 36.91179    35.85758    37.96601\n3 Squidward  31   1.89     65 41.25491    38.97189    43.53794\n```\n\n\n:::\n:::\n\n\n\n### Plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_for_plot |>\n  ggplot(aes(x = name)) +\n  geom_errorbar(aes(ymin = .pred_lower,\n                    ymax = .pred_upper),\n                color = \"red\",\n                width = 0.32) +\n  geom_point(aes(y = .pred), color = \"blue\", size = 5) +\n  labs(title = \"Tour de Under the Sea\",\n       subtitle = \"Welcome the new contestants!\",\n       caption = \"Math 32\",\n       x = \"\",\n       y = \"pace (km/hr)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](18_intro_ml_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n:::::\n\n# Data Splitting\n\n::: {.callout-important}\n## Data Split\n\nHow do we *generalize*?  What if we want to predict for more riders or into the future?  One strategy is to go back into the provided data and *split* the data:\n\n* **training set**: about 75 percent of the data is allocated toward building the model\n* Then, the model is applied to the remaining data---the **testing set**---to see if the generalization is working well.\n:::\n\n## Split\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_split <- initial_split(bike_df)\ntrain_df <- training(data_split)\ntest_df <- testing(data_split)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(paste(\"The number of observations in the training set is:\", \n            nrow(train_df)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"The number of observations in the training set is: 46\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"The number of observations in the testing set is:\", \n            nrow(test_df)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"The number of observations in the testing set is: 16\"\n```\n\n\n:::\n:::\n\n\n\n## One Split\n\n::::: {.panel-tabset}\n\n## plot\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18_intro_ml_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n\n## code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitle_string <- \"<span style='color:#000000'><b>Training Sets</b></span> <span style='color:#0000FF'>and</span> \n<span style='color:#FF0000'><b>Testing Sets</b></span>\"\n\ntrain_df |>\n  ggplot(aes(x = height, y = pace)) +\n  geom_point(aes(color = \"training set\"), \n             # color = \"black\"\n             ) +\n  geom_smooth(method = \"lm\",\n              aes(x = height, y = pace),\n              color = \"black\",\n              data = train_df,\n              formula = \"y ~ x\",\n              se = FALSE) +\n  geom_point(aes(x = height, y = pace, color = \"testing set\"),\n             # color = \"red\",\n             data = test_df,\n             size = 3) +\n  labs(title = title_string,\n       subtitle = \"approx 75-25 percent split\",\n       caption = \"Math 32\",\n       x = \"height (meters)\",\n       y = \"pace (km/hr)\") +\n  scale_color_manual(name = \"Data Split\",\n                     breaks = c(\"training set\", \"testing set\"),\n                     values = c(\"training set\" = \"black\",\n                                \"testing set\" = \"red\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_markdown(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n```\n:::\n\n\n\n:::::\n\n## Many Splits\n\n::::: {.panel-tabset}\n\n## plot\n\n![](images/many_splits.gif)\n\n## code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitle_string <- \"<span style='color:#000000'><b>Training Sets</b></span> <span style='color:#0000FF'>and</span> \n<span style='color:#FF0000'><b>Testing Sets</b></span>\"\n\nfor(i in 1:10){\n  \n  data_split <- initial_split(df)\n  train_df <- training(data_split)\n  test_df <- testing(data_split)\n  \n  this_plot <- train_df |>\n    ggplot(aes(x = height, y = pace)) +\n    geom_point(aes(color = \"training set\"), \n               # color = \"black\"\n    ) +\n    geom_smooth(method = \"lm\",\n                aes(x = height, y = pace),\n                color = \"black\",\n                data = train_df,\n                formula = \"y ~ x\",\n              se = FALSE) +\n  geom_point(aes(x = height, y = pace, color = \"testing set\"),\n             # color = \"red\",\n             data = test_df,\n             size = 3) +\n  labs(title = title_string,\n       subtitle = \"approx 75-25 percent split\",\n       caption = \"Math 32\",\n       x = \"height (meters)\",\n       y = \"pace (km/hr)\") +\n  scale_color_manual(name = \"Data Split\",\n                     breaks = c(\"training set\", \"testing set\"),\n                     values = c(\"training set\" = \"black\",\n                                \"testing set\" = \"red\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_markdown(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n  \n  ggsave(paste0(\"images/plot\", i, \".png\"),\n         plot = this_plot,\n         device = \"png\")\n}\n```\n:::\n\n\n\n:::::\n\n# Metrics\n\nWe then should get a sense of the validity of a model.  \n\n## MSE\n\nOne metric for evaluating a regression model (i.e. numerical predictions for the response variable) is *mean square error* of the test set.\n\n$$\\text{MSE} = \\ds\\frac{1}{n_{\\text{test}}}\\sum_{j = 1}^{n_{\\text{test}}} (y_{j} - \\hat{y}_{j})^{2}$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_split <- initial_split(bike_df)\ntrain_df <- training(data_split)\ntest_df <- testing(data_split)\n\nlm_train <- linear_reg() |> \n  set_engine(\"lm\") |>\n  fit(pace ~ height + weight + age, data = train_df)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nn_test <- nrow(test_df)\ntrue_values <- test_df$pace\npredictions <- predict(lm_train, new_data = test_df |> select(-pace))\n\nMSE <- (1/n_test)*sum((true_values - predictions)^2)\nprint(MSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.342595\n```\n\n\n:::\n:::\n\n\n\n# Cross-Validation\n\n::: {.callout-note collapse=\"true\"}\n## Cross-Validation\n\nTo help generalize to a variety of testing sets, we can perform *cross-validation* by utilizing several training/testing set splits.\n\nWe can then compute the *cross-validation error* by computing the mean of the test metric.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20241112)\nN <- 10 #number of replicates\nMSE_vec <- rep(NA, N)\n\nfor(i in 1:N){\n  data_split <- initial_split(bike_df)\n  train_df <- training(data_split)\n  test_df <- testing(data_split)\n  \n  lm_train <- linear_reg() |> \n    set_engine(\"lm\") |>\n    fit(pace ~ height + weight + age, data = train_df)\n  \n  n_test <- nrow(test_df)\n  true_values <- test_df$pace\n  predictions <- predict(lm_train, new_data = test_df |> select(-pace))\n  \n  MSE_vec[i] <- (1/n_test)*sum((true_values - predictions)^2)\n}\n\n# vector of MSE\nMSE_vec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 5.340079 4.625713 5.210729 6.940068 5.022087 8.129130 7.934785 4.774170\n [9] 8.477074 4.633587\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# cross-validation error\ncv_error <- mean(MSE_vec)\ncv_error\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.108742\n```\n\n\n:::\n:::\n\n\n\n## Second Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(201)\nN <- 25 #number of replicates\nMSE_vec <- rep(NA, N)\n\nfor(i in 1:N){\n  data_split <- initial_split(bike_df)\n  train_df <- training(data_split)\n  test_df <- testing(data_split)\n  \n  lm_train <- linear_reg() |> \n    set_engine(\"lm\") |>\n    # fit(pace ~ height + weight + age, data = train_df)\n    fit(pace ~ height + weight + age + height:weight + height:age + weight:age +\n        height:weight:age,\n      data = train_df)\n  \n  n_test <- nrow(test_df)\n  true_values <- test_df$pace\n  predictions <- predict(lm_train, new_data = test_df |> select(-pace))\n  \n  MSE_vec[i] <- (1/n_test)*sum((true_values - predictions)^2)\n}\n\n# vector of MSE\nMSE_vec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  3.381904  6.009615  2.841596  9.032613  5.648347  5.244057 14.174292\n [8]  7.890369  6.835132  7.788564 13.704889  5.324071  6.282300  2.811976\n[15]  5.136457  5.586255  5.446546  4.333529 12.666760  7.113682  8.068847\n[22]  6.562516 10.239050  6.873517  6.590273\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# cross-validation error\ncv_error <- mean(MSE_vec)\ncv_error\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.023486\n```\n\n\n:::\n:::\n\n\n\n\n# Project 3\n\n## MisbehaviorX\n\n* VehiGAN AI algorithm to instantaneouly detect malicious signal attacks\n    \n    * Md Hasan Shahriar, Mohammad Raashid Ansari, Jean-Philippe Monteuuis, Cong Chen, Jonathan Petit, Y. Thomas Hou, Wenjing Lou\n    * presented their work at the [ICDCS 2024 conference](https://icdcs2024.icdcs.org/) in Jersey City in the summer of 2024\n\n* realms: machine learning, artificial intelligence, cybersecurity\n\n![VehiGAN diagrams](VehiGAN_diagrams.png)\n\n* provided 0.1% stratified sample of the MisbehaviorX data set\n      \n    * very new data set (originally 8 GB)\n\n* conduct over 100 hypothesis tests!\n\n\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n* Precept 9\n* Research Consent\n* Project 3 (Due Nov 20)\n* Exam 2 (December 5)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n![comic](ml_mask.png)\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n## (optional) Additional Resources\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.1 (2024-06-14 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 22631)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1      readr_2.1.5       \n [5] tidyverse_2.0.0    yardstick_1.3.1    workflowsets_1.1.0 workflows_1.1.4   \n [9] tune_1.2.1         tidyr_1.3.1        tibble_3.2.1       rsample_1.2.1     \n[13] recipes_1.1.0      purrr_1.0.2        parsnip_1.2.1      modeldata_1.4.0   \n[17] infer_1.0.7        ggplot2_3.5.1      dplyr_1.1.4        dials_1.3.0       \n[21] scales_1.3.0       broom_1.0.6        tidymodels_1.2.0   ggtext_0.1.2      \n\nloaded via a namespace (and not attached):\n [1] rlang_1.1.4         magrittr_2.0.3      furrr_0.3.1        \n [4] compiler_4.4.1      mgcv_1.9-1          vctrs_0.6.5        \n [7] lhs_1.2.0           pkgconfig_2.0.3     crayon_1.5.3       \n[10] fastmap_1.2.0       backports_1.5.0     labeling_0.4.3     \n[13] utf8_1.2.4          rmarkdown_2.28      prodlim_2024.06.25 \n[16] markdown_1.13       tzdb_0.4.0          bit_4.0.5          \n[19] xfun_0.46           jsonlite_1.8.8      parallel_4.4.1     \n[22] R6_2.5.1            stringi_1.8.4       parallelly_1.38.0  \n[25] rpart_4.1.23        Rcpp_1.0.13         iterators_1.0.14   \n[28] knitr_1.48          future.apply_1.11.3 Matrix_1.7-0       \n[31] splines_4.4.1       nnet_7.3-19         timechange_0.3.0   \n[34] tidyselect_1.2.1    rstudioapi_0.16.0   yaml_2.3.10        \n[37] timeDate_4041.110   codetools_0.2-20    curl_5.2.1         \n[40] listenv_0.9.1       lattice_0.22-6      withr_3.0.1        \n[43] evaluate_0.24.0     future_1.34.0       survival_3.6-4     \n[46] xml2_1.3.6          pillar_1.9.0        foreach_1.5.2      \n[49] generics_0.1.3      vroom_1.6.5         hms_1.1.3          \n[52] munsell_0.5.1       commonmark_1.9.1    globals_0.16.3     \n[55] class_7.3-22        glue_1.7.0          tools_4.4.1        \n[58] data.table_1.15.4   gower_1.0.1         grid_4.4.1         \n[61] ipred_0.9-15        colorspace_2.1-1    nlme_3.1-164       \n[64] cli_3.6.3           DiceDesign_1.10     fansi_1.0.6        \n[67] lava_1.8.0          gtable_0.3.5        GPfit_1.0-8        \n[70] digest_0.6.36       htmlwidgets_1.6.4   farver_2.1.2       \n[73] htmltools_0.5.8.1   lifecycle_1.0.4     hardhat_1.4.0      \n[76] gridtext_0.1.5      bit64_4.0.5         MASS_7.3-60.2      \n```\n\n\n:::\n:::\n\n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Example Callout Block\n\n`note`, `tip`, `warning`, `caution`, or `important`\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [
      "18_intro_ml_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}