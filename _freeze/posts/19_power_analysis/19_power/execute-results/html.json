{
  "hash": "99aa3bf7c6c09c23d1d65edd120ea46b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"19: Power Analysis\"\nauthor: \"Derek Sollberger\"\ndate: \"2024-11-14\"\nformat:\n  html:\n    toc: true\n    theme: cerulean\n---\n\n\n\n# SML 201\n\n## Start\n\n::: {.callout-note collapse=\"true\"}\n## Libraries and Helper Functions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"infer\")\nlibrary(\"janitor\")\nlibrary(\"pwr\")\nlibrary(\"tidyverse\")\n\n# school colors\nprinceton_orange <- \"#E77500\"\nprinceton_black  <- \"#121212\"\n\n# data set: GPT detectors\nGPT_detectors <- readr::read_csv('detectors.csv')\n# GPT_detectors <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-07-18/detectors.csv')\n```\n:::\n\n\n\n:::\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n* **Goal**: Introduce power analysis\n\n\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"40%\"}\n* **Objectives**: \n\n- contingency tables\n- p-hacking\n- effect size (Cohen's h)\n- power analyses\n:::\n\n::::\n\n\n## Data: GPT Detectors\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* GPT Detectors for academic dishonesty\n* data hosted at [TidyTuesday](https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-07-18/readme.md)\n* Paper: [GPT Detectors Are Biased Against Non-Native English Writers](https://arxiv.org/abs/2304.02819)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![Mr Belding](Belding_GPT.png)\n:::\n\n::::\n\n\n## Exploratory Data Analysis\n\n::::: {.panel-tabset}\n\n## Chatbots\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](19_power_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n## Detectors\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](19_power_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n## Counts\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  kind   AI Human Total\n    AI 1158  2559  3717\n Human  449  2019  2468\n Total 1607  4578  6185\n```\n\n\n:::\n:::\n\n\n\n## code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGPT_detectors |>\n  ggplot(aes(x = model)) +\n  geom_bar(aes(fill = .pred_class), \n           position = \"fill\", stat = \"count\") +\n  labs(title = \"GPT Chatbots\",\n       subtitle = \"Proportions of essays that were deemed AI or human\",\n       caption = \"SML 201\",\n       x = \"chatbot\",\n       y = \"proportions\") +\n  theme_minimal(base_size = 14)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nGPT_detectors |>\n  ggplot(aes(x = detector)) +\n  geom_bar(aes(fill = .pred_class), \n           position = \"fill\", stat = \"count\") +\n  labs(title = \"GPT Detectors\",\n       subtitle = \"Proportions of essays that were deemed AI or human\",\n       caption = \"SML 201\",\n       x = \"detector\",\n       y = \"proportions\") +\n  theme_minimal(base_size = 14) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nGPT_detectors |>\n  tabyl(kind, .pred_class) |>\n  adorn_totals(c(\"row\", \"col\"))\n```\n:::\n\n\n\n:::::\n\n\n# Confusion Matrix\n\nA **confusion matrix** (aka **contingency table**) tallies the classifications versus their ground truth.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncross_tab <- GPT_detectors |> tabyl(kind, .pred_class)\nprint(cross_tab)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  kind   AI Human\n    AI 1158  2559\n Human  449  2019\n```\n\n\n:::\n:::\n\n\n\n## Quartet\n\nAddressing: Are the detectors correctly identifying AI-generated work as works of AI?\n\n* positive: AI\n* negative: human\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# True positive: AI classification of AI work\nTP <- cross_tab[1,2]\n# False negative: Human classification of AI work\nFN <- cross_tab[1,3]\n# False positive: AI classification of human work\nFP <- cross_tab[2,2]\n# True negative: Human classification of human work\nTN <- cross_tab[2,3]\n```\n:::\n\n\n\n## Type I and Type II\n\n* **Type I errors**: false positive\n* **Type II errors**: false negative\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(paste0(\"The number of Type I errors is: \", FP))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"The number of Type I errors is: 449\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste0(\"The number of Type II errors is: \", FN))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"The number of Type II errors is: 2559\"\n```\n\n\n:::\n:::\n\n\n\n## Metrics\n\nThere are many [metrics for confusion matrices](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n\n### False Positive Rate (Fallout)\n\n$$FPR = \\frac{FP}{FP + TN}$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nFPR <- FP / (FP + TN)\nprint(paste0(\"The false positive rate is \", round(100*FPR, 2), \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"The false positive rate is 18.19 percent\"\n```\n\n\n:::\n:::\n\n\n\n### Accuracy\n\n$$\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nacc <- (TP + TN) / (TP + FP + FN + TN)\nprint(paste0(\"The accuracy is \", round(100*acc, 2), \" percent\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"The accuracy is 51.37 percent\"\n```\n\n\n:::\n:::\n\n\n\n\n# NHST\n\n* Null hypothesis: the AI classification rate is the *same* for both AI and human generated essays.\n* Alternative hypothesis: the AI classification rate is *greater* for AI generated essays than human generated essays.\n\n$$H_{o}: \\rho_{A} \\leq \\rho_{H}$$\n$$H_{a}: \\rho_{A} > \\rho_{H}$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_distribution <- GPT_detectors |>\n  specify(formula = .pred_class ~ kind, success = \"AI\") |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 1000, type = \"permute\") |>\n  calculate(stat = \"diff in props\", order = c(\"AI\", \"Human\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_diff <- GPT_detectors |>\n  specify(formula = .pred_class ~ kind, success = \"AI\") |>\n  # hypothesize(null = \"independence\") |>\n  # generate(reps = 1000, type = \"permute\") |>\n  calculate(stat = \"diff in props\", order = c(\"AI\", \"Human\"))\n\nnull_distribution |>\n  get_p_value(obs_diff, direction = \"greater\") |> \n  pull()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\nSince the p-value < 0.05, we *reject* the null hypothesis that the AI classification rate is the *same* for both AI and human generated essays (at the $\\alpha = 0.05$ significance level).\n\n\n# Replication Crisis\n\n## Catalyst\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n* [Professor Daryl Bem](https://psychology.cornell.edu/daryl-j-bem), Cornell University\n* 2011 peer-reviewed and publised [paper](https://prevention.ucsf.edu/sites/prevention.ucsf.edu/files/uploads/2011/02/bem2011.pdf)\n* demonstrated that college students have ESP (extra sensory perception)\n\n    * achieved p-value < 0.05\n    * achieved some effect size\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\n![(not a picture of Daryl Bem)](Peter_Venkman.png)\n:::\n\n::::\n\n\n## p-hacking\n\n:::: {.columns}\n\n::: {.column width=\"70%\"}\n> The more statistical analyses a researcher runs—the more hypotheses they “test”—the more likely that at least one of these analyses will produce a result that appears to be “significant”, simply by chance. --- Sean Trott\n\n* XKCD: [Significant](https://xkcd.com/882/)\n* [p-hacking in R](https://seantrott.github.io/p-hacking/) by Sean Trott\n\n::: {.callout-caution}\n## Project 3\n\nDisclosure: the requested tasks in Project 3 may constitute \"p-hacking\"\n:::\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"20%\"}\n![significant beans!](significant.png)\n:::\n\n::::\n\n## Gaussian White Noise\n\nIn this exploration, we are going to try to extract \"significance\" (p-value < 0.05) from Gaussian white noise\n\n$$X, Y \\sim N(\\mu = 201, \\sigma^{2} = 25^{2})$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_vals <- rnorm(100, 201, 25)\ny_vals <- rnorm(100, 201, 25) #that is, same mean\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncor_val <- cor(x_vals, y_vals)\n\ndata.frame(x_vals, y_vals) |>\n  ggplot(aes(x = x_vals, y = y_vals)) +\n  coord_equal() +\n  geom_point(color = \"gray50\") +\n  geom_smooth(formula = \"y ~ x\",\n              method = \"lm\") +\n  labs(title = \"Gaussian White Noise\",\n       subtitle = paste0(\"Two normally distributed vectors\\nwith the same mean\\nr: \", round(cor_val, 4)),\n       caption = \"SML 201\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](19_power_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 1000 #number of replicates\nslopes <- rep(NA, N)\np_vals <- rep(NA, N)\n\nfor(i in 1:N){\n  this_x <- sample(x_vals) #permutation\n  this_lm <- summary(lm(y_vals ~ this_x))\n  slopes[i] <- this_lm$coefficients[2]\n  p_vals[i] <- this_lm$coefficients[8]\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_for_graph <- data.frame(slopes, p_vals) |>\n  mutate(result = ifelse(p_vals < 0.05, \"significant\", \"not significant\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_for_graph |>\n  ggplot(aes(x = slopes, y = p_vals, color = result)) +\n  geom_point() +\n  labs(title = \"The Search for Significance\",\n       subtitle = \"After resampling the noise, we may have found significance\",\n       caption = \"Source: Sean Trott\",\n       y = \"p-value\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](19_power_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndetection_rate <- mean(p_vals < 0.05)\n```\n:::\n\n\n\nWe have found \"significance\" in 5 percent of our replicates.\n\n::: {.callout-caution}\n## p-hacking\n\nWhat if a research team reported findings of \"significance\" from the 5 percent and *discarded* results from the other 95 percent?\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Mitigating p-Hacking\n\n* Researchers can **pre-register** their studies (usually around the same time as the ethics approval) to show what they are trying to measure first.\n* Report *confidence intervals* too\n* Report *effect sizes* too\n:::\n\n\n# Effect Size\n\n## Cohen\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n* [Professor Jacob Cohen](https://en.wikipedia.org/wiki/Jacob_Cohen_(statistician)), NYU\n* advocate of power analysis and effect size\n* critic of NHST\n* known for\n\n    * Cohen's kappa\n    * Cohen's h\n    * Cohen's d\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\n![(not a picture of Jacob Cohen)](Egon_Spengler.png)\n:::\n\n::::\n\n\n## Cohens h\n\n[Cohen's h](https://en.wikipedia.org/wiki/Cohen%27s_h) is a measure of distance between two proportions\n\n$$h = |2\\text{arcsin}\\sqrt{p_{1}} - 2\\text{arcsin}\\sqrt{p_{2}}|$$\n\n* suggested interpretation:\n\n    * $h = 0.20$: \"small effect size\"\n    * $h = 0.50$: \"medium effect size\"\n    * $h = 0.80$: \"large effect size\"\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sensitivity: probability of detection\np1 <- TP / (TP + FN)\n# false positive rate\np2 <- FP / (TN + FP)\npwr::ES.h(p1, p2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3030231\n```\n\n\n:::\n:::\n\n\n\nWe have found a *small effect size* in GPT detection rate moving from human-generated essays to AI-generated essays.\n\n\n# Case Study: GPT Detection for non-native English Writing\n\n## Counts\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGPT_detectors |>\n  filter(!is.na(native)) |>\n  tabyl(native, .pred_class) |>\n  adorn_totals(c(\"row\", \"col\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n native  AI Human Total\n     No 390   247   637\n    Yes  59  1772  1831\n  Total 449  2019  2468\n```\n\n\n:::\n:::\n\n\n## Confusion Matrix\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncross_tab_2 <- GPT_detectors |>\n  filter(!is.na(native)) |>\n  tabyl(native, .pred_class)\nTP_2 <- cross_tab_2[1,2]\nFN_2 <- cross_tab_2[1,3]\nFP_2 <- cross_tab_2[2,2]\nTN_2 <- cross_tab_2[2,3]\n```\n:::\n\n\n\n## NHST\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_distribution <- GPT_detectors |>\n  filter(!is.na(native)) |>\n  specify(formula = .pred_class ~ native, success = \"AI\") |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 1000, type = \"permute\") |>\n  calculate(stat = \"diff in props\", order = c(\"No\", \"Yes\"))\n\nobs_diff <- GPT_detectors |>\n  filter(!is.na(native)) |>\n  specify(formula = .pred_class ~ native, success = \"AI\") |>\n  # hypothesize(null = \"independence\") |>\n  # generate(reps = 1000, type = \"permute\") |>\n  calculate(stat = \"diff in props\", order = c(\"No\", \"Yes\"))\n\nnull_distribution |>\n  get_p_value(obs_diff, direction = \"greater\") |> \n  pull()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\nSince the p-value < 0.05, we *reject* the null hypothesis that the AI classification rate is the *same* for essays written by both native and non-native English speakers (at the $\\alpha = 0.05$ significance level).\n\n## Effect Size\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sensitivity: probability of detection\np1_2 <- TP_2 / (TP_2 + FN_2)\n# false positive rate\np2_2 <- FP_2 / (TN_2 + FP_2)\npwr::ES.h(p1_2, p2_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.436245\n```\n\n\n:::\n:::\n\n\n\nWe have found a *large effect size* in GPT detection rate moving from native-speaker writers to non-native writers.\n\n\n# Power Analysis\n\n::: {.callout-note}\n## Power (TPR)\n\nThe *power* of a significance test is the probability of correctly rejecting a null hypothesis when it is actually false.\n\n$$\\text{power} = \\text{TPR} = \\frac{TP}{TP + FN}$$\n* also called true positive rate, recall, sensitivity, probability of detection, hit rate\n\n:::\n\n## Seeking Power\n\nFor the first comparison between the AI-generated essays and the human-generated essays, what was the power of the NHST for GPT detection?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.p.test(alternative = \"greater\",\n           h = ES.h(p1,p2),\n           n = sum(!is.na(GPT_detectors$kind)),\n           # power = ? #i.e. this is what we are trying to find\n           sig.level = 0.05\n           )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.3030231\n              n = 6185\n      sig.level = 0.05\n          power = 1\n    alternative = greater\n```\n\n\n:::\n:::\n\n\n\n\nFor the second comparison between the native speaker written essays and the non-native written essays, what was the power of the NHST for GPT detection?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.p.test(alternative = \"greater\",\n           h = ES.h(p1_2,p2_2),\n           n = sum(!is.na(GPT_detectors$native)),\n           # power = ? #i.e. this is what we are trying to find\n           sig.level = 0.05\n           )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 1.436245\n              n = 2468\n      sig.level = 0.05\n          power = 1\n    alternative = greater\n```\n\n\n:::\n:::\n\n\n\n\n## Seeking Sample Size\n\nFor the first comparison between the AI-generated essays and the human-generated essays, how large should the sample size be (at least) to achieve (at least) 0.80 power?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.p.test(alternative = \"greater\",\n           h = ES.h(p1,p2),\n           #n = ? #i.e. this is what we are trying to find\n           power = 0.80, \n           sig.level = 0.05\n           )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.3030231\n              n = 67.33124\n      sig.level = 0.05\n          power = 0.8\n    alternative = greater\n```\n\n\n:::\n:::\n\n\n\n> This experiment needed at least 68 essays.\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n* Precept 9\n* Research Consent\n* Project 3 (Due Nov 20)\n* Exam 2 (December 5)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n![obligatory Invincible meme](power_invincible_meme.png)\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n## (optional) Additional Resources\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.1 (2024-06-14 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [5] purrr_1.0.2     readr_2.1.5     tidyr_1.3.1     tibble_3.2.1   \n [9] ggplot2_3.5.1   tidyverse_2.0.0 pwr_1.3-0       janitor_2.2.0  \n[13] infer_1.0.7    \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4        generics_0.1.3    lattice_0.22-6    stringi_1.8.4    \n [5] hms_1.1.3         digest_0.6.35     magrittr_2.0.3    evaluate_1.0.1   \n [9] grid_4.4.1        timechange_0.3.0  fastmap_1.2.0     Matrix_1.7-0     \n[13] jsonlite_1.8.8    mgcv_1.9-1        fansi_1.0.6       scales_1.3.0     \n[17] cli_3.6.3         crayon_1.5.3      rlang_1.1.4       splines_4.4.1    \n[21] bit64_4.5.2       munsell_0.5.1     withr_3.0.2       yaml_2.3.8       \n[25] parallel_4.4.1    tools_4.4.1       tzdb_0.4.0        colorspace_2.1-1 \n[29] vctrs_0.6.5       R6_2.5.1          lifecycle_1.0.4   snakecase_0.11.1 \n[33] htmlwidgets_1.6.4 bit_4.5.0         vroom_1.6.5       archive_1.1.8    \n[37] pkgconfig_2.0.3   pillar_1.9.0      gtable_0.3.5      glue_1.8.0       \n[41] xfun_0.48         tidyselect_1.2.1  rstudioapi_0.17.0 knitr_1.48       \n[45] farver_2.1.2      nlme_3.1-164      htmltools_0.5.8.1 labeling_0.4.3   \n[49] rmarkdown_2.28    compiler_4.4.1   \n```\n\n\n:::\n:::\n\n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Example Callout Block\n\n`note`, `tip`, `warning`, `caution`, or `important`\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [
      "19_power_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}