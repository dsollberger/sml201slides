---
title: "23: One More Case Study"
author: "Derek Sollberger"
date: "2024-12-03"
format:
  html:
    toc: true
    theme: cerulean
---

# SML 201

## Start

::: {.callout-note collapse="true"}
## Libraries and Loading the Data

```{r}
#| message: false
#| warning: false

library("ggsignif")  #show significance levels on boxplots
library("patchwork") #arrange plots
library("tidyclust") #tidymodels clustering
library("tidymodels")#framework for machine learning in R
library("tidyverse") #general framework for data wrangling

# school colors
princeton_orange <- "#E77500"
princeton_black  <- "#121212"

set.seed(201)
control_data <- data.frame(
  age = sample(18:65, size = 100, replace = TRUE),
  height = round(rnorm(100, 65, 8)),
  sex = sample(c("female", "male"), size = 100, replace = TRUE),
  group = "control",
  weight_before = round(rnorm(100, 150, 15))
) |>
  #coded like Gaussian white noise
  mutate(weight_after = round(weight_before - rnorm(100, 0, 6)))

treatment_data <- data.frame(
  age = sample(18:65, size = 100, replace = TRUE),
  height = round(rnorm(100, 65, 8)),
  sex = sample(c("female", "male"), size = 100, replace = TRUE),
  group = "treatment",
  weight_before = round(rnorm(100, 150, 15))
) |>
  #coded like Gaussian white noise
  mutate(weight_after = round(weight_before - rnorm(100, 15, 6)))

GLP_raw <- rbind(control_data, treatment_data) |>
  sample_frac() |>
  mutate(patient_id = 1:200, .before = "age")

# helper function
vnorm <- function(x, mu = 0, sigma = 1, section = "lower"){
  
  # bell curve
  x_vals <- seq(mu - 4*sigma, mu + 4*sigma, length.out = 201)
  y_vals <- dnorm(x_vals, mu, sigma)
  df_for_graph <- data.frame(x_vals, y_vals)

  # outline shaded regions
  if(length(x) == 1){
    shade_left <- rbind(c(x[1],0), df_for_graph |>
                        filter(x_vals < x[1]))
    shade_right <- rbind(c(x[1],0), df_for_graph |>
                        filter(x_vals > x[1]))
  }
  if(length(x) == 2){
    shade_between <- rbind(c(x[1],0),
                       df_for_graph |>
                         filter(x_vals > x[1] &
                                  x_vals < x[2]),
                       c(x[2],0))
    shade_tails <- rbind(df_for_graph |>
                        filter(x_vals < x[1]),
                     c(x[1],0),
                     c(x[2],0),
                     df_for_graph |>
                        filter(x_vals > x[2]))
  }
  if(section == "lower"){
    bell_curve <- df_for_graph |>
      ggplot(aes(x_vals, y_vals)) +
      geom_polygon(aes(x = x_vals, y = y_vals),
                   data = shade_left,
                   fill = "#E77500",) +
      geom_line(color = "gray50", linewidth = 2)
    prob_val <- round(pnorm(x,mu,sigma), 4)
  }
  if(section == "upper"){
    bell_curve <- df_for_graph |>
      ggplot(aes(x_vals, y_vals)) +
      geom_polygon(aes(x = x_vals, y = y_vals),
                   data = shade_right,
                   fill = "#E77500",) +
      geom_line(color = "gray50", linewidth = 2)
    prob_val <- 1 - round(pnorm(x,mu,sigma), 4)
  }
  if(section == "between"){
    bell_curve <- df_for_graph |>
      ggplot(aes(x_vals, y_vals)) +
      geom_polygon(aes(x = x_vals, y = y_vals),
                   data = shade_between,
                   fill = "#E77500",) +
      geom_line(color = "gray50", linewidth = 2)
    prob_val <- round(diff(pnorm(x,mu,sigma)), 4)
  }
  if(section == "tails"){
    bell_curve <- df_for_graph |>
      ggplot(aes(x_vals, y_vals)) +
      geom_polygon(aes(x = x_vals, y = y_vals),
                   data = shade_tails,
                   fill = "#E77500",) +
      geom_line(color = "gray50", linewidth = 2)
    prob_val <- round(1 - diff(pnorm(x,mu,sigma)), 4)
  }
  
  # plot bell curve
  bell_curve + 
    labs(subtitle = paste0("Probability: ", prob_val),
         caption = "SML 201", y = "") +
    theme_minimal()
}
```

:::

:::: {.columns}

::: {.column width="50%"}
* **Goal**: Take on one more case study

:::

::: {.column width="10%"}

:::

::: {.column width="40%"}
* **Objectives**: 

- probability
- confidence intervals
- hypothesis testing
- machine learning

:::

::::

## Data

> "GLP [Glucagon-like peptide-1] agonists are medications that help lower blood sugar levels and promote weight loss." --- Cleveland Clinic

We are going to take hypothetical data and pretend that we are analyzing results from clinical trials in hopes of obtaining approval from the Food and Drug Administration.


# Data Preparation

* add a column for

$$\text{weight loss} = \text{weight before} - \text{weight after}$$

* remove the `patient_id` column

```{r}
GLP_df <- GLP_raw |>
  mutate(weight_loss = weight_before - weight_after) |>
  select(-patient_id)
```

# Probability

Assuming a normal distribution, what is the probability that a randomly selected person in the treatment group lost at least 10 pounds?

```{r}
GLP_df |>
  filter(group == "treatment") |>
  summarize(xbar = mean(weight_loss),
            s = sd(weight_loss))

pnorm(10, 16.1, 5.8232, lower.tail = FALSE)
```

```{r}
vnorm(10, 16.1, 5.8232, section = "upper") +
  labs(title = "GLP Case Study",
       subtitle = "probability of losing at least 10 pounds",
       caption = "SML 201\n(hypothetical data)",
       x = "weight loss (pounds)")
```


# Confidence Intervals

Use `infer::visualize` to construct confidence intervals for `weight_loss` for both the control and treatment groups.

```{r}
bootstrap_distribution <- GLP_df |>
  filter(group == "control") |>
  specify(response = weight_loss) |>
  generate(reps = 1337, type = "bootstrap") |>
  calculate(stat = "mean")

bootstrap_distribution |>
  visualize() +
  shade_ci(endpoints = bootstrap_distribution |>
             get_ci(level = 0.95, type = "percentile"),
           color = princeton_black,
           fill = princeton_orange) +
  labs(title = "GLP Clinical Trials",
       subtitle = "control group",
       caption = "(hypothetical data)",
       x = "weight loss", y = "number of patients") +
  theme_minimal(base_size = 16)
```

```{r}
bootstrap_distribution <- GLP_df |>
  filter(group == "treatment") |>
  specify(response = weight_loss) |>
  generate(reps = 1337, type = "bootstrap") |>
  calculate(stat = "mean")

bootstrap_distribution |>
  visualize() +
  shade_ci(endpoints = bootstrap_distribution |>
             get_ci(level = 0.95, type = "percentile"),
           color = princeton_black,
           fill = princeton_orange) +
  labs(title = "GLP Clinical Trials",
       subtitle = "treatment group",
       caption = "(hypothetical data)",
       x = "weight loss", y = "number of patients") +
  theme_minimal(base_size = 16)
```


# Hypothesis Testing

## Null Distribution

Use `infer` package code to perform NHST to test the claim (among those in the treatment group)

* null hypothesis: male patients have at least as much weight loss than female patients
* alternative hypothesis: male patients have less weight loss than female patients

```{r}
# build a null distribution
null_distribution <- GLP_df |>
  filter(group == "treatment") |>
  specify(formula = weight_loss ~ sex) |> 
  hypothesize(null = "independence") |> 
  generate(reps = 1000, type = "permute") |> 
  calculate(stat = "diff in means", order = c("male", "female"))
```

```{r}
# compute the observed difference in means
obs_diff_means <- GLP_df |>
  filter(group == "treatment") |>
  specify(formula = weight_loss ~ sex) |> 
  calculate(stat = "diff in means", order = c("male", "female"))
  
# visualize the null distribution and shade in the p-value
null_distribution |>
visualize(bins = 10) + 
  shade_p_value(obs_stat = obs_diff_means, direction = "less") +
  theme_minimal()
```

```{r}
null_distribution |>
  get_p_value(obs_stat = obs_diff_means, direction = "less")
```

## Boxplots

Create a side-by-side boxplot to conduct a two-sided NHST for weight loss between the control and treatment groups.

```{r}
GLP_df |>
  ggplot(aes(x = group, y = weight_loss,
             fill = group)) +
  geom_boxplot() +
  geom_signif(
    comparisons = list(c("control", "treatment")),
    map_signif_level = TRUE
  ) +
  labs(title = "GLP Clinical Trials",
       subtitle = "(toward publisable results)",
       caption = "(hypothetical data)",
       x = "", y = "weight loss (pounds)") +
  theme_minimal()
```


# Machine Learning

```{r}
GLP_ml <- GLP_df |>
  select(-c(weight_before, weight_after))

data_split <- initial_split(GLP_ml)
data_train <- training(data_split)
data_fold <- vfold_cv(data_train)
lambda_grid <- grid_regular(penalty(range = c(-3, 3)),
                            levels = 50)
```

## Ridge Regression

```{r}
#| message: false
#| warning: false

ml_pipeline_fit <- linear_reg(mixture = 0, penalty = 0) |> 
  set_engine("glmnet") |>
  fit(weight_loss ~ ., data = GLP_ml)
ml_pipeline_recipe <- recipe(weight_loss ~ ., data = GLP_ml) |>
  step_novel(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |> 
  step_zv(all_predictors()) |>
  step_normalize(all_numeric_predictors())
ml_pipeline_spec <- linear_reg(mixture = 0, penalty = tune()) |>
  set_engine("glmnet")
ml_pipeline_workflow <- workflow() |> 
  add_recipe(ml_pipeline_recipe) |>
  add_model(ml_pipeline_spec)

ml_pipeline_grid <- tune_grid(
  ml_pipeline_workflow,
  resamples = data_fold, 
  grid = lambda_grid
)
```

```{r}
best_penalty <- select_best(ml_pipeline_grid, 
                            metric = "rmse") |>
  pull(penalty)
predictions <- predict(ml_pipeline_fit, 
                       new_data = GLP_df |>
                         select(-weight_loss),
                       penalty = best_penalty)
true_values <- GLP_df  |> select(weight_loss)
n <- GLP_df  |> drop_na(weight_loss) |> nrow()
RMSE <- sqrt((1/n)*sum((true_values - predictions)^2))
print(RMSE)
```

## Lasso Regression

```{r}
#| message: false
#| warning: false

ml_pipeline_fit <- linear_reg(mixture = 1, penalty = 0) |> 
  set_engine("glmnet") |>
  fit(weight_loss ~ ., data = GLP_ml)
ml_pipeline_recipe <- recipe(weight_loss ~ ., data = GLP_ml) |>
  step_novel(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |> 
  step_zv(all_predictors()) |>
  step_normalize(all_numeric_predictors())
ml_pipeline_spec <- linear_reg(mixture = 1, penalty = tune()) |>
  set_engine("glmnet")
ml_pipeline_workflow <- workflow() |> 
  add_recipe(ml_pipeline_recipe) |>
  add_model(ml_pipeline_spec)

ml_pipeline_grid <- tune_grid(
  ml_pipeline_workflow,
  resamples = data_fold, 
  grid = lambda_grid
)
```

```{r}
best_penalty <- select_best(ml_pipeline_grid, 
                            metric = "rmse") |>
  pull(penalty)
predictions <- predict(ml_pipeline_fit, 
                       new_data = GLP_df |>
                         select(-weight_loss),
                       penalty = best_penalty)
true_values <- GLP_df  |> select(weight_loss)
n <- GLP_df  |> drop_na(weight_loss) |> nrow()
RMSE <- sqrt((1/n)*sum((true_values - predictions)^2))
print(RMSE)
```


## Clustering

```{r}
# cross-validation folds
ml_pipeline_cv <- vfold_cv(GLP_ml, v = 5)

# specification
ml_pipeline_spec <- k_means(num_clusters = tune())

# recipe
ml_pipeline_recipe <- recipe(~weight_loss, data = GLP_ml) |>
  step_zv() |>
  step_normalize()

# workflow
ml_pipeline_workflow <- workflow(ml_pipeline_recipe, ml_pipeline_spec)

# parameter grid
kvals_grid <- grid_regular(num_clusters(), levels = 10)

# tuning
tune_results <- tune_cluster(
  ml_pipeline_workflow,
  resamples = ml_pipeline_cv,
  grid = kvals_grid,
  metrics = cluster_metric_set(sse_within_total, sse_total, sse_ratio)
)
```

```{r}
tune_results |>
  collect_metrics() |>
  filter(.metric == "sse_ratio") |>
  ggplot(aes(x = num_clusters, y = mean)) +
  geom_point(aes(x = factor(num_clusters), y = mean),
             size = 3) +
  geom_line() +
  labs(title = "Scree Plot",
       subtitle = "Aiming to choose the number of clusters",
       caption = "SML 201",
       x = "number of clusters",
       y = "within/total SSE ratio") +
  theme_minimal(base_size = 14)
```


### Visualization

Create a side-by-side arrangements of scatterplots

* horizontal: `weight_before`
* vertical: `weight_after`

1. color coded by `group`
2. color coded by cluster assignment

```{r}
cluster_fit <- k_means(num_clusters = 2) |>
  set_engine("stats") |>
  fit(~ weight_loss, data = GLP_df)
GLP_clust <- GLP_df |>
  mutate(cluster_num = extract_cluster_assignment(cluster_fit) |>
           pull())
```

```{r}
p1 <- GLP_clust |>
  ggplot(aes(x = weight_before, y = weight_after)) +
  geom_point(aes(color = group),
             alpha = 0.5, size = 3) +
  labs(title = "GLP Clinical Trials",
       subtitle = "experiment groups",
       x = "weight before (pounds)",
       y = "weight after (pounds)") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

```{r}
p2 <- GLP_clust |>
  ggplot(aes(x = weight_before, y = weight_after)) +
  geom_point(aes(color = cluster_num),
             alpha = 0.5, size = 3) +
  labs(title = "GLP Clinical Trials",
       subtitle = "k = 2 clusters",
       x = "weight before (pounds)",
       y = "weight after (pounds)") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

```{r}
#patchwork
p1 + p2
```


# Project 4

:::: {.columns}

::: {.column width="45%"}
* ridesharing

    * Chicago Data Portal
    * September 2024

* areas: business, accounting
* theme: machine learning
:::

::: {.column width="10%"}
	
:::

::: {.column width="45%"}
![](ridesharing.png)
:::

::::


# SML 301

> Data Intelligence: Modern Data Science Methods 

1. **Eir**: movement tracking for women athletes
2. Hospital pricing: predict surgery prices
3. **AEC**: automating document scanning classification

# Quo Vadimus?

:::: {.columns}

::: {.column width="40%"}
* Exam 2 (December 5)
* Project 4 (due December 11)
:::

::: {.column width="10%"}
	
:::

::: {.column width="50%"}


:::

::::


# Footnotes

::: {.callout-note collapse="true"}
## (optional) Additional Resources

* more information about [GLP medications](https://my.clevelandclinic.org/health/treatments/13901-glp-1-agonists) from Cleveland Clinic

:::

::: {.callout-note collapse="true"}
## Session Info

```{r}
sessionInfo()
```
:::


::: {.callout-note collapse="true"}
## Example Callout Block

`note`, `tip`, `warning`, `caution`, or `important`
:::


:::: {.columns}

::: {.column width="45%"}
	
:::

::: {.column width="10%"}
	
:::

::: {.column width="45%"}

:::

::::

::::: {.panel-tabset}



:::::